{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4faa2cf9",
   "metadata": {},
   "source": [
    "## QuestGen-LLM: Training\n",
    "\n",
    "This notebook covers the training of various pre-trained _large language models_ (LLMs) on the prepared [\"quest\"](../data/quests_train.json) dataset. Each language model applied is trained and validated on the dataset (with frozen parameters) and the results of these evaluations are compared. The LLMs employed for this application are listed in the following table with their respective parameter count.\n",
    "\n",
    "| S. No. | Large Language Model                | Parameters | Developed By | Notes                                              |\n",
    "| :----: | :---------------------------------- | :--------: | :----------: | :------------------------------------------------- |\n",
    "|   1.   | GPT-2[^1]                           |    124M    |    OpenAI    | Base model from the GPT-2 family                   |\n",
    "|   2.   | GPT-2 Medium[^2]                    |    355M    |    OpenAI    | Larger variant with improved language modeling     |\n",
    "|   3.   | GPT-2 Large[^3]                     |    774M    |    OpenAI    | Capable of generating more coherent longer text    |\n",
    "|   4.   | Llama-2-7B-Chat[^4] \\*†             |     7B     |     Meta     | Chat-optimized version of LLaMA-2                  |\n",
    "|   5.   | Llama-3.1-8B-Instruct[^5] \\*†       |     8B     |     Meta     | Instruction-tuned variant for LLaMA-3.1            |\n",
    "|   6.   | Mistral-7B-Instruct-v0.2[^6]        |     7B     |  Mistral AI  | Instruct fine-tuned version of the Mistral-7B-v0.2 |\n",
    "|   7.   | DeepSeek-R1-Distill-Qwen-1.5B[^7] † |    1.5B    | DeepSeek AI  | Distilled model based on the Qwen architecture     |\n",
    "|   8.   | DeepSeek-R1-Distill-Llama-8B[^8] †  |     8B     | DeepSeek AI  | Distilled model based on the LLaMA architecture    |\n",
    "\n",
    "> Fine-tuning uses _supervised fine-tuning_\\* (SHF) and _reinforcement learning with human feedback_† (RLHF).\n",
    "\n",
    "<!-- References -->\n",
    "\n",
    "[^1]: https://huggingface.co/openai-community/gpt2\n",
    "[^2]: https://huggingface.co/openai-community/gpt2-medium\n",
    "[^3]: https://huggingface.co/openai-community/gpt2-large\n",
    "[^4]: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "[^5]: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "[^6]: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "[^7]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "[^8]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b286473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
