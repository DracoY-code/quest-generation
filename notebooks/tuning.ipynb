{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4faa2cf9",
   "metadata": {},
   "source": [
    "## QuestGen-LLM: Fine-Tuning\n",
    "\n",
    "This notebook covers the fine-tuning of various pre-trained _large language models_ (LLMs) on the prepared [\"quest\"](../data/quests_train.json) dataset. Each language model applied is trained and validated on the dataset (with frozen parameters) and the results of these evaluations are compared. The LLMs employed for this application are listed in the following table with their respective parameter count.\n",
    "\n",
    "| S. No. | Large Language Model                | Parameters | Developed By | Notes                                              |\n",
    "| :----: | :---------------------------------- | :--------: | :----------: | :------------------------------------------------- |\n",
    "|   1.   | GPT-2[^1]                           |    124M    |    OpenAI    | Base model from the GPT-2 family                   |\n",
    "|   2.   | GPT-2 Medium[^2]                    |    355M    |    OpenAI    | Larger variant with improved language modeling     |\n",
    "|   3.   | GPT-2 Large[^3]                     |    774M    |    OpenAI    | Capable of generating more coherent longer text    |\n",
    "|   4.   | Llama-2-7B-Chat[^4] \\*†             |     7B     |     Meta     | Chat-optimized version of LLaMA-2                  |\n",
    "|   5.   | Llama-3.1-8B-Instruct[^5] \\*†       |     8B     |     Meta     | Instruction-tuned variant for LLaMA-3.1            |\n",
    "|   6.   | Mistral-7B-Instruct-v0.2[^6]        |     7B     |  Mistral AI  | Instruct fine-tuned version of the Mistral-7B-v0.2 |\n",
    "|   7.   | DeepSeek-R1-Distill-Qwen-1.5B[^7] † |    1.5B    | DeepSeek AI  | Distilled model based on the Qwen architecture     |\n",
    "|   8.   | DeepSeek-R1-Distill-Llama-8B[^8] †  |     8B     | DeepSeek AI  | Distilled model based on the LLaMA architecture    |\n",
    "\n",
    "> Fine-tuning uses _supervised fine-tuning_\\* (SHF) and _reinforcement learning with human feedback_† (RLHF).\n",
    "\n",
    "<!-- References -->\n",
    "\n",
    "[^1]: https://huggingface.co/openai-community/gpt2\n",
    "[^2]: https://huggingface.co/openai-community/gpt2-medium\n",
    "[^3]: https://huggingface.co/openai-community/gpt2-large\n",
    "[^4]: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "[^5]: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "[^6]: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "[^7]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "[^8]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b286473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Literal, TypeAlias\n",
    "\n",
    "\n",
    "# Map for the model identifiers: (model_key -> model_id)\n",
    "model_ids: dict[str, str] = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large\": \"openai-community/gpt2-large\",\n",
    "    \"llama-2-7b-chat\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama-3.1-8b-instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"mistral-7b-instruct-v0.2\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"deepseek-r1-distill-qwen-1.5b\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"deepseek-r1-distill-llama-8b\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "}\n",
    "\n",
    "# Set the default device\n",
    "DEVICE_TYPE: TypeAlias = Literal[\"cuda\", \"cpu\"]\n",
    "device: DEVICE_TYPE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set the default cache directory\n",
    "cache_dir: Path = Path(\"../models/.cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b10fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from os import PathLike\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerFast,\n",
    ")\n",
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "def download_model(\n",
    "    model_key: str,\n",
    "    device: DEVICE_TYPE = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    cache_dir: PathLike = Path(\"../models/.cache/\"),\n",
    ") -> dict[str, Any]:\n",
    "    start_time: float = time.time()\n",
    "    model_id: Optional[str] = model_ids.get(model_key, None)\n",
    "    print(f\"Downloading {model_key} ({model_id if model_id else 'N/A'})...\")\n",
    "\n",
    "    if model_id is None:\n",
    "        return {\n",
    "            \"model_key\": model_key,\n",
    "            \"model_id\": \"N/A\",\n",
    "            \"tokenizer\": None,\n",
    "            \"model\": None,\n",
    "        }\n",
    "\n",
    "    # Download the tokenizer using the model id\n",
    "    tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=(cache_dir / model_key),\n",
    "        use_fast=True,\n",
    "    )\n",
    "\n",
    "    # Download the model using the model id\n",
    "    model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=(cache_dir / model_key),\n",
    "    )\n",
    "\n",
    "    end_time: float = time.time()\n",
    "    print(f\"{model_key} ready in {(end_time - start_time):.2f}s.\")\n",
    "\n",
    "    return {\n",
    "        \"model_key\": model_key,\n",
    "        \"model_id\": model_id,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"model\": model,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afcbd455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuestGenLLM:\n",
    "    tokenizer: Optional[PreTrainedTokenizerFast]\n",
    "    model: Optional[PreTrainedModel]\n",
    "    model_key: str  # Alias for the model, e.g, \"gpt2\"\n",
    "    model_id: str  # Hugging Face model name, e.g., \"openai-community/gpt2\"\n",
    "    device: str = field(init=False)\n",
    "    dtype: str = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Automatically determine the device used by the model\n",
    "        self.device = str(getattr(self.model, \"device\", \"N/A\"))\n",
    "\n",
    "        # Automatically determine the dtype used by the model\n",
    "        self.dtype = str(getattr(self.model, \"dtype\", \"N/A\")).replace(\"torch.\", \"\")\n",
    "\n",
    "    def to_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"model_key\": self.model_key,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"device\": self.device,\n",
    "            \"dtype\": self.dtype,\n",
    "            \"vocab_size\": getattr(self.tokenizer, \"vocab_size\", \"unknown\"),\n",
    "            \"max_length\": getattr(self.tokenizer, \"model_max_length\", \"unknown\"),\n",
    "            \"model_type\": getattr(\n",
    "                getattr(self.model, \"config\", \"\"), \"model_type\", \"unknown\"\n",
    "            ),\n",
    "            \"num_parameters\": self.model.num_parameters()\n",
    "            if hasattr(self.model, \"num_parameters\")\n",
    "            else \"N/A\",\n",
    "        }\n",
    "\n",
    "    def clear_cache(self, cache_dir: PathLike = Path(\"../models/.cache/\")) -> None:\n",
    "        if os.path.exists(cache_dir / self.model_key):\n",
    "            shutil.rmtree(cache_dir / self.model_key)\n",
    "            print(f\"Cache directory '{cache_dir / self.model_key}' removed.\")\n",
    "        else:\n",
    "            print(f\"No cache directory found at '{cache_dir / self.model_key}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e08769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gpt2 (openai-community/gpt2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b945df76bd4c948a5792369a6662a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cae06096c847b7a7e1b65db0694a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845fc186331f47e18b376f61487beee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72c49bfe7534ec28026d48c6b6f5e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa47064191e84311b7477b587a45939c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7adc829a554460b60056d86774dd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17676a4e19224633b897c5e4fe4c4f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 ready in 32.19s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_key': 'gpt2',\n",
       " 'model_id': 'openai-community/gpt2',\n",
       " 'device': 'mps:0',\n",
       " 'dtype': 'float32',\n",
       " 'vocab_size': 50257,\n",
       " 'max_length': 1024,\n",
       " 'model_type': 'gpt2',\n",
       " 'num_parameters': 124439808}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the pre-trained GPT-2 model\n",
    "gpt2_llm: QuestGenLLM = QuestGenLLM(**download_model(\"gpt2\"))\n",
    "gpt2_llm.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81982640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory '../models/.cache/gpt2' removed.\n"
     ]
    }
   ],
   "source": [
    "gpt2_llm.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c3dcfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gpt2-medium (openai-community/gpt2-medium)...\n",
      "gpt2-medium ready in 3.09s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_key': 'gpt2-medium',\n",
       " 'model_id': 'openai-community/gpt2-medium',\n",
       " 'device': 'mps:0',\n",
       " 'dtype': 'float32',\n",
       " 'vocab_size': 50257,\n",
       " 'max_length': 1024,\n",
       " 'model_type': 'gpt2',\n",
       " 'num_parameters': 354823168}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the pre-trained GPT-2 Medium model\n",
    "gpt2_medium_llm: QuestGenLLM = QuestGenLLM(**download_model(\"gpt2-medium\"))\n",
    "gpt2_medium_llm.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "227ba5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory '../models/.cache/gpt2-medium' removed.\n"
     ]
    }
   ],
   "source": [
    "gpt2_medium_llm.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bccfabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gpt2-large (openai-community/gpt2-large)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16460582ab3c40d68420f58c11c05250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1504ac170f7e4b18a9f42fd8fccc61df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39e74ba3b2342e08e438c8a8a6aabfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fbb5216daa4f0da5672f9bf96e0f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3ae6ccd5e34b42a0518d37dbccab62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2365ea44fcd946eeb0ce6cb0cb4645cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88800f03f7c7404a9a8864078401eda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-large ready in 144.39s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_key': 'gpt2-large',\n",
       " 'model_id': 'openai-community/gpt2-large',\n",
       " 'device': 'mps:0',\n",
       " 'dtype': 'float32',\n",
       " 'vocab_size': 50257,\n",
       " 'max_length': 1024,\n",
       " 'model_type': 'gpt2',\n",
       " 'num_parameters': 774030080}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the pre-trained GPT-2 Large model\n",
    "gpt2_large_llm: QuestGenLLM = QuestGenLLM(**download_model(\"gpt2-large\"))\n",
    "gpt2_large_llm.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a2674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory '../models/.cache/gpt2-large' removed.\n"
     ]
    }
   ],
   "source": [
    "gpt2_large_llm.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f3521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
