{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aab5fc",
   "metadata": {},
   "source": [
    "## QuestGen-LLM: Fine-Tuning\n",
    "\n",
    "This notebook covers the fine-tuning of various pre-trained _large language models_ (LLMs) on the prepared [\"quest\"](../data/quests_train.json) dataset. Each language model applied is trained and validated on the dataset (with frozen parameters) and the results of these evaluations are compared. The LLMs employed for this application are listed in the following table with their respective parameter count.\n",
    "\n",
    "| S. No. | Large Language Model                | Parameters | Developed By | Notes                                              |\n",
    "| :----: | :---------------------------------- | :--------: | :----------: | :------------------------------------------------- |\n",
    "|   1.   | GPT-2[^1]                           |    124M    |    OpenAI    | Base model from the GPT-2 family                   |\n",
    "|   2.   | GPT-2 Medium[^2]                    |    355M    |    OpenAI    | Larger variant with improved language modeling     |\n",
    "|   3.   | GPT-2 Large[^3]                     |    774M    |    OpenAI    | Capable of generating more coherent longer text    |\n",
    "|   4.   | Llama-2-7B-Chat[^4] \\*†             |     7B     |     Meta     | Chat-optimized version of LLaMA-2                  |\n",
    "|   5.   | Llama-3.1-8B-Instruct[^5] \\*†       |     8B     |     Meta     | Instruction-tuned variant for LLaMA-3.1            |\n",
    "|   6.   | Mistral-7B-Instruct-v0.2[^6]        |     7B     |  Mistral AI  | Instruct fine-tuned version of the Mistral-7B-v0.2 |\n",
    "|   7.   | DeepSeek-R1-Distill-Qwen-1.5B[^7] † |    1.5B    | DeepSeek AI  | Distilled model based on the Qwen architecture     |\n",
    "|   8.   | DeepSeek-R1-Distill-Llama-8B[^8] †  |     8B     | DeepSeek AI  | Distilled model based on the LLaMA architecture    |\n",
    "\n",
    "> Fine-tuning uses _supervised fine-tuning_\\* (SHF) and _reinforcement learning with human feedback_† (RLHF).\n",
    "\n",
    "<!-- References -->\n",
    "\n",
    "[^1]: https://huggingface.co/openai-community/gpt2\n",
    "[^2]: https://huggingface.co/openai-community/gpt2-medium\n",
    "[^3]: https://huggingface.co/openai-community/gpt2-large\n",
    "[^4]: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "[^5]: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "[^6]: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "[^7]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "[^8]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b883328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "# Map for the model identifiers: (model_key -> model_id)\n",
    "model_ids: dict[str, str] = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large\": \"openai-community/gpt2-large\",\n",
    "    \"llama-2-7b-chat\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama-3.1-8b-instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"mistral-7b-instruct-v0.2\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"deepseek-r1-distill-qwen-1.5b\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"deepseek-r1-distill-llama-8b\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "\n",
    "data_dir: Path = Path(\"../data/\")\n",
    "\n",
    "# Load the quest dataset\n",
    "quest_set: DatasetDict = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": str(data_dir / \"quests_train.txt\"),\n",
    "        \"val\": str(data_dir / \"quests_val.txt\"),\n",
    "        \"test\": str(data_dir / \"quests_test.txt\"),\n",
    "    },\n",
    "    cache_dir=str(data_dir / \".cache\"),\n",
    ")\n",
    "quest_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_set[\"train\"][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22831c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import Dataset\n",
    "from os import PathLike\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerFast,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuestGenLLM:\n",
    "    tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast\n",
    "    model: PreTrainedModel\n",
    "    model_key: str  # Alias for the model, e.g, \"gpt2\"\n",
    "    model_id: str  # Hugging Face model name, e.g., \"openai-community/gpt2\"\n",
    "    device: str = field(init=False)\n",
    "    dtype: str = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Automatically determine the device used by the model\n",
    "        self.device = str(getattr(self.model, \"device\", \"N/A\"))\n",
    "\n",
    "        # Automatically determine the dtype used by the model\n",
    "        self.dtype = str(getattr(self.model, \"dtype\", \"N/A\")).replace(\"torch.\", \"\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_key: str,\n",
    "        model_id: str,\n",
    "        cache_dir: PathLike = Path(\"../models/.cache/\"),\n",
    "    ) -> QuestGenLLM:\n",
    "        print(f\"[DOWNLOAD] {model_key} ({model_id})\")\n",
    "        start_time: float = time.time()\n",
    "\n",
    "        # Download the tokenizer using the model id\n",
    "        tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            cache_dir=(cache_dir / model_key),\n",
    "            use_fast=True,\n",
    "        )\n",
    "\n",
    "        # Download the model using the model id\n",
    "        model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "            device_map=\"auto\",\n",
    "            cache_dir=(cache_dir / model_key),\n",
    "        )\n",
    "\n",
    "        end_time: float = time.time()\n",
    "        elapsed: float = end_time - start_time\n",
    "        print(f'[COMPLETE] \"{model_key}\" ready in {elapsed:.2f}s.')\n",
    "\n",
    "        return cls(tokenizer, model, model_key, model_id)\n",
    "\n",
    "    def tokenize_and_train(\n",
    "        self,\n",
    "        dataset: DatasetDict,\n",
    "        max_length: int = 512,\n",
    "        learning_rate: int = 0.00005,\n",
    "        batch_size: int = 4,\n",
    "        epochs: int = 3,\n",
    "        seed: int = 42,\n",
    "        output_dir: PathLike = Path(\"../out\"),\n",
    "        logging_dir: PathLike = Path(\"../logs\"),\n",
    "    ) -> Trainer:\n",
    "        # Ensure the training and validation sets\n",
    "        if not all(split in dataset for split in [\"train\", \"val\"]):\n",
    "            print(\"DatasetDict must contain 'train' and 'val' splits.\")\n",
    "            return\n",
    "\n",
    "        # Ensure the output and logging directories\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "        start_time: float\n",
    "        end_time: float\n",
    "        elapsed: float\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(42)\n",
    "\n",
    "        # Set the padding token for the tokenizer\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Tokenize the dataset with `max_length` padding\n",
    "        print(f\"[TOKENIZE] {self.model_key} ({self.model_id})\")\n",
    "        start_time = time.time()\n",
    "        tokenized_data: Dataset = dataset.map(\n",
    "            lambda example: self.tokenizer(\n",
    "                example[\"text\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            ),\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"],\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {str(datetime.timedelta(seconds=elapsed))}\")\n",
    "\n",
    "        # Set the model padding token (from the tokenizer)\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        # Set up the training configurations\n",
    "        training_args: TrainingArguments = TrainingArguments(\n",
    "            output_dir=str(output_dir / self.model_key),\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=str(logging_dir / self.model_key),\n",
    "            save_total_limit=2,\n",
    "            fp16=(self.dtype == \"float16\"),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        # Set up the data collator for the model\n",
    "        data_collator: DataCollatorForLanguageModeling = (\n",
    "            DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)\n",
    "        )\n",
    "\n",
    "        # Prepare and run the trainer\n",
    "        trainer: Trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=tokenized_data[\"train\"],\n",
    "            eval_dataset=tokenized_data[\"val\"],\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "        )\n",
    "\n",
    "        print(f\"[TRAINING] {self.model_key} ({self.model_id})\")\n",
    "        start_time: float = time.time()\n",
    "        trainer.train()\n",
    "        end_time: float = time.time()\n",
    "        elapsed: float = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {str(datetime.timedelta(seconds=elapsed))}\")\n",
    "\n",
    "        # Save the model and tokenizer for later use\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(save_directory=training_args.output_dir)\n",
    "\n",
    "        return trainer\n",
    "\n",
    "    def to_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"model_key\": self.model_key,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"device\": self.device,\n",
    "            \"dtype\": self.dtype,\n",
    "            \"vocab_size\": getattr(self.tokenizer, \"vocab_size\", \"unknown\"),\n",
    "            \"max_length\": getattr(self.tokenizer, \"model_max_length\", \"unknown\"),\n",
    "            \"model_type\": getattr(\n",
    "                getattr(self.model, \"config\", None), \"model_type\", \"unknown\"\n",
    "            ),\n",
    "            \"num_parameters\": self.model.num_parameters()\n",
    "            if hasattr(self.model, \"num_parameters\")\n",
    "            else \"N/A\",\n",
    "        }\n",
    "\n",
    "    def clear_cache(self, cache_dir: PathLike = Path(\"../models/.cache/\")) -> None:\n",
    "        def remove_dir(dir_path: PathLike) -> None:\n",
    "            if os.path.exists(dir_path):\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"Cache directory '{dir_path}' removed.\")\n",
    "            else:\n",
    "                print(f\"No cache directory found at '{dir_path}'.\")\n",
    "\n",
    "        remove_dir(cache_dir / self.model_key)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.model_key} ({self.model_id})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained GPT-2 model\n",
    "gpt2_llm: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"gpt2\", model_id=model_ids[\"gpt2\"]\n",
    ")\n",
    "gpt2_llm.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d891678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and train the GPT-2 model on the quest dataset\n",
    "gpt2_trainer: Trainer = gpt2_llm.tokenize_and_train(quest_set)\n",
    "gpt2_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2113e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
