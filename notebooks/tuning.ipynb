{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aab5fc",
   "metadata": {},
   "source": [
    "## QuestGen-LLM: Fine-Tuning\n",
    "\n",
    "This notebook covers the fine-tuning of various pre-trained _large language models_ (LLMs) on the prepared [\"quest\"](../data/quests_train.json) dataset. Each language model applied is trained and validated on the dataset (with frozen parameters) and the results of these evaluations are compared. The LLMs employed for this application are listed in the following table with their respective parameter count.\n",
    "\n",
    "| S. No. | Large Language Model             | Parameters | Developed By | Notes                                                 |\n",
    "| :----: | :------------------------------- | :--------: | :----------: | :---------------------------------------------------- |\n",
    "|   1.   | GPT-2[^1]                        |    124M    |    OpenAI    | Base model from the GPT-2 family                      |\n",
    "|   2.   | GPT-2 Medium[^2]                 |    355M    |    OpenAI    | Larger variant with improved language modeling        |\n",
    "|   3.   | GPT-2 Large[^3]                  |    774M    |    OpenAI    | Capable of generating more coherent longer text       |\n",
    "|   4.   | Llama-3.2-1B-Instruct[^4] †      |     1B     |     Meta     | Instruction-tuned model for question-answering        |\n",
    "|   5.   | TinyLlama-1.1B-Chat-v1.0[^5] \\*† |    1.1B    |  TinyLlama   | Lightweight chat-tuned model for constrained hardware |\n",
    "\n",
    "> Fine-tuning uses _supervised fine-tuning_\\* (SHF) and _reinforcement learning with human feedback_† (RLHF).\n",
    "\n",
    "<!-- References -->\n",
    "\n",
    "[^1]: https://huggingface.co/openai-community/gpt2\n",
    "[^2]: https://huggingface.co/openai-community/gpt2-medium\n",
    "[^3]: https://huggingface.co/openai-community/gpt2-large\n",
    "[^4]: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "[^5]: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e29e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "from typing import Any, Final, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7633f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerFast,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.tokenization_utils_base import BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5629f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root: str = str(Path.cwd().parent.resolve())\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5215bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dirpath import get_cache_dirpath, get_target_dirpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d946469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HF access token from the environment\n",
    "HF_ACCESS_TOKEN: Final[str] = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "# Save the HF token to ~/.huggingface/token\n",
    "login(token=HF_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b883328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for the model identifiers: (model_key -> model_id)\n",
    "MODEL_IDENTIFIERS: Final[dict[str, str]] = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large\": \"openai-community/gpt2-large\",\n",
    "    \"llama-3.2-1b-instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"tinyllama-1.1b-chat\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f6fefdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 19954\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2486\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2394\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir: Path = get_target_dirpath(\"data\")\n",
    "\n",
    "# Load the quest dataset\n",
    "quest_set: DatasetDict = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": str(data_dir / \"quests_train.txt\"),\n",
    "        \"val\": str(data_dir / \"quests_val.txt\"),\n",
    "        \"test\": str(data_dir / \"quests_test.txt\"),\n",
    "    },\n",
    "    cache_dir=str(data_dir / \".cache\"),\n",
    ")\n",
    "quest_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca1a797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['### Instruction:',\n",
       "  'Generate a video game quest description based on the following structured information.',\n",
       "  '',\n",
       "  '### Input:',\n",
       "  'Quest Name: Perilous Passage',\n",
       "  'Objective: save the Mana Queen',\n",
       "  'First Tasks: go through the gate to the Forsaken Vaults',\n",
       "  'First Task Locations: Forsaken Vaults - a perilous dungeon',\n",
       "  'Quest Giver: NONE - NONE (location: NONE)',\n",
       "  'Reward: NONE -  (amount: 1)',\n",
       "  'Characters: Mana Queen - a good female spirit (location: Forsaken Vaults)',\n",
       "  'Tools: NONE',\n",
       "  'Locations: NONE',\n",
       "  'Items: NONE',\n",
       "  'Enemies: NONE',\n",
       "  'Groups: NONE',\n",
       "  'Title: Torchlight II',\n",
       "  'Motivation: NONE',\n",
       "  '',\n",
       "  '### Response:',\n",
       "  \"The Mana Queen has come and gone Through this gate, she journeyed on. Follow her and pay the cost. Hasten forth, or she'll be lost.\"]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quest_set[\"train\"][:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5335935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['### Instruction:',\n",
       "  'Generate a video game quest description based on the following structured information.',\n",
       "  '',\n",
       "  '### Input:',\n",
       "  'Quest Name: A Child in the Lighthouse',\n",
       "  \"Objective: save Ardrouine's little son from worgs\",\n",
       "  'First Tasks: go to the abandoned lighthouse',\n",
       "  'First Task Locations:  - abandoned lighthouse to the northwest',\n",
       "  'Quest Giver: NONE - NONE (location: NONE)',\n",
       "  'Reward:  - coins (amount: 60)',\n",
       "  'Characters: NONE',\n",
       "  'Tools: NONE',\n",
       "  'Locations: NONE',\n",
       "  'Items: NONE',\n",
       "  'Enemies: NONE',\n",
       "  'Groups: NONE',\n",
       "  \"Title: Baldur's Gate\",\n",
       "  'Motivation: NONE',\n",
       "  '',\n",
       "  '### Response:',\n",
       "  \"Please help me, I am just poor Ardrouine! I don't know where else to turn. My little boy was playing in that abandoned lighthouse to the northwest when a pack of worgs surrounded it. Please just turn them back, and I can coax him down. There's not much time! I can pay you 60 coins: this money is all my husband brought back from market this past week. My son's life is worth this and so much more.\"]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quest_set[\"val\"][22:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "188b14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for the target modules: (model_key -> target_modules)\n",
    "TARGET_MODULES: Final[dict[str, list[str]]] = {\n",
    "    \"gpt2\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"gpt2-medium\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"gpt2-large\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"llama-3.2-1b-instruct\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    \"tinyllama-1.1b-chat\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "967a5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constants for model tuning here\n",
    "BATCH_SIZE: Final[int] = 4\n",
    "N_EPOCHS: Final[int] = 1\n",
    "SEED: Final[int] = 42\n",
    "LR_RATE: Final[float] = 5e-7\n",
    "\n",
    "MAX_LENGTH: Final[int] = 64\n",
    "MAX_GRAD_NORM: Final[float] = 1.0\n",
    "LOGGING_STEPS: Final[int] = 20\n",
    "EVAL_STEPS: Final[int] = 50\n",
    "WARMUP_STEPS: Final[int] = 500\n",
    "\n",
    "SAVE_TOTAL_LIMIT: Final[int] = 1\n",
    "EVAL_ACCUMULATION_STEPS: Final[int] = 2\n",
    "GRADIENT_ACCUMULATION_STEPS: Final[int] = 2\n",
    "\n",
    "GRADIENT_CHECKPOINTING: Final[bool] = False\n",
    "LOAD_BEST_MODEL_AT_END: Final[bool] = True\n",
    "\n",
    "ACTIVATE_FP16: Final[bool] = False\n",
    "ACTIVATE_EVAL: Final[bool] = True\n",
    "ACTIVATE_SAVE: Final[bool] = True\n",
    "ACTIVATE_TEST: Final[bool] = True\n",
    "ACTIVATE_LOGS: Final[bool] = False\n",
    "ACTIVATE_TENSORBOARD: Final[bool] = True\n",
    "ACTIVATE_CALLBACKS: Final[bool] = True\n",
    "ACTIVATE_FULL: Final[bool] = False  # Full dataset or subset\n",
    "\n",
    "FRACTION: Final[float] = 0.01  # 1% of the Quest dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9697d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a subset of the quest dataset\n",
    "quest_subset: DatasetDict = DatasetDict(\n",
    "    {\n",
    "        \"train\": quest_set[\"train\"]\n",
    "        .shuffle(seed=SEED)\n",
    "        .select(range(int(FRACTION * quest_set[\"train\"].num_rows))),\n",
    "        \"val\": quest_set[\"val\"]\n",
    "        .shuffle(seed=SEED)\n",
    "        .select(range(int(FRACTION * quest_set[\"val\"].num_rows))),\n",
    "        \"test\": quest_set[\"test\"]\n",
    "        .shuffle(seed=SEED)\n",
    "        .select(range(int(FRACTION * quest_set[\"test\"].num_rows))),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9446f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    train_losses: list[float] = field(default_factory=list)\n",
    "    eval_losses: list[float] = field(default_factory=list)\n",
    "    eval_results: list[dict[str, float]] = field(default_factory=list)\n",
    "\n",
    "\n",
    "# Map for storing training metrics: (model_key -> metrics)\n",
    "TRAINING_METRICS: dict[str, Optional[TrainingMetrics]] = {\n",
    "    k: None for k in MODEL_IDENTIFIERS.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f98d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, metrics: TrainingMetrics):\n",
    "        self.metrics: TrainingMetrics = metrics\n",
    "\n",
    "    def on_log(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        logs: Optional[dict[str, float]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        # Capture training losses during logging\n",
    "        if \"loss\" in logs:\n",
    "            self.metrics.train_losses.append(logs[\"loss\"])\n",
    "\n",
    "        # Capture evaluation losses during logging\n",
    "        if \"eval_loss\" in logs:\n",
    "            self.metrics.eval_losses.append(logs[\"eval_loss\"])\n",
    "\n",
    "        return super().on_log(args, state, control, **kwargs)\n",
    "\n",
    "    def on_evaluate(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        metrics: Optional[dict[str, float]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if metrics is None:\n",
    "            return\n",
    "\n",
    "        # Capture evaluation results on evaluation\n",
    "        self.metrics.eval_results.append(metrics)\n",
    "\n",
    "        return super().on_evaluate(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d22831c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestGenLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "        model: PreTrainedModel,\n",
    "        model_key: str,  # Alias for the model, e.g, \"gpt2\"\n",
    "        model_id: str,  # Hugging Face model name, e.g., \"openai-community/gpt2\"\n",
    "        fp16_available: bool,  # Mixed precision\n",
    "        device: Optional[str] = None,\n",
    "        dtype: Optional[str] = None,\n",
    "        metrics: Optional[TrainingMetrics] = None,\n",
    "    ):\n",
    "        self.tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = tokenizer\n",
    "        self.model: PreTrainedModel = model\n",
    "        self.model_key: str = model_key\n",
    "        self.model_id: str = model_id\n",
    "        self.fp16_available: bool = fp16_available\n",
    "\n",
    "        # Automatically determine the device used by the model\n",
    "        self.device: str = (\n",
    "            device\n",
    "            if isinstance(device, str)\n",
    "            else str(getattr(self.model, \"device\", \"N/A\"))\n",
    "        )\n",
    "\n",
    "        # Automatically determine the dtype used by the model\n",
    "        self.dtype: str = (\n",
    "            dtype\n",
    "            if isinstance(dtype, str)\n",
    "            else str(getattr(self.model, \"dtype\", \"N/A\")).replace(\"torch.\", \"\")\n",
    "        )\n",
    "\n",
    "        # Initialize dataclass for storing training metrics\n",
    "        self.metrics: TrainingMetrics = (\n",
    "            metrics if isinstance(metrics, TrainingMetrics) else TrainingMetrics()\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_key: str,\n",
    "        model_id: str,\n",
    "        cache_dir: PathLike = get_cache_dirpath(\"models\"),\n",
    "        seed: int = SEED,\n",
    "        use_cpu: bool = False,\n",
    "    ) -> QuestGenLLM:\n",
    "        def apply_lora_adapter(\n",
    "            model: PreTrainedModel,\n",
    "            r: int = 8,\n",
    "            alpha: int = 16,\n",
    "            dropout: float = 0.1,\n",
    "            task_type: str = \"CAUSAL_LM\",\n",
    "        ) -> PreTrainedModel:\n",
    "            # Prepare model for k-bit training\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "            # Set correct `fan_in_fan_out` based on model type\n",
    "            #\n",
    "            # False [default] - for Linear layers\n",
    "            # True - for Conv1D layers (like GPT)\n",
    "            fan_in_fan_out: bool = False\n",
    "            if \"gpt\" in getattr(model.config, \"model_type\", \"\").lower():\n",
    "                fan_in_fan_out = True\n",
    "\n",
    "            # Define the LoRA config\n",
    "            lora_config: LoraConfig = LoraConfig(\n",
    "                r=r,\n",
    "                lora_alpha=alpha,\n",
    "                lora_dropout=dropout,\n",
    "                target_modules=TARGET_MODULES[model_key],\n",
    "                bias=\"none\",\n",
    "                task_type=task_type,\n",
    "                fan_in_fan_out=fan_in_fan_out,\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # Apply LoRA adapters to the model\n",
    "                model = get_peft_model(model, lora_config)\n",
    "            except Exception as e:\n",
    "                print(f\"[LoRAINFO] Adapter failed to apply: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Display information about the model parameters\n",
    "            trainable_params: int = sum(\n",
    "                p.numel() for p in model.parameters() if p.requires_grad\n",
    "            )\n",
    "            all_params: int = sum(p.numel() for p in model.parameters())\n",
    "            trainable_percent: float = 100 * trainable_params / all_params\n",
    "            print(\n",
    "                \"[LoRAINFO] trainable params: {:,} || all params: {:,} || trainable%: {:.4f}\".format(\n",
    "                    trainable_params, all_params, trainable_percent\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return model\n",
    "\n",
    "        print(f\"[DOWNLOAD] {model_key} ({model_id})\")\n",
    "        start_time: float = time.time()\n",
    "\n",
    "        # Clear PyTorch's CUDA memory cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Determine if mixed precision is available\n",
    "        fp16_available: bool = (\n",
    "            torch.cuda.is_available()\n",
    "            and torch.cuda.get_device_capability(0)[0] >= 7\n",
    "            and torch.cuda.get_device_capability(0)[1] >= 0\n",
    "        )\n",
    "\n",
    "        # Download the tokenizer using the model id\n",
    "        tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            cache_dir=(cache_dir / model_key),\n",
    "            use_fast=True,\n",
    "            token=HF_ACCESS_TOKEN,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        model: PreTrainedModel\n",
    "        if fp16_available and not use_cpu:\n",
    "            # Set the bitsandbytes configuration for quantization\n",
    "            bnb_config: BitsAndBytesConfig = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                # llm_int8_enable_fp32_cpu_offload=True,\n",
    "            )\n",
    "\n",
    "            # Download the model using the model id (for GPU)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16,\n",
    "                quantization_config=bnb_config,\n",
    "                cache_dir=(cache_dir / model_key),\n",
    "                token=HF_ACCESS_TOKEN,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            model.to(\"cuda\")\n",
    "        else:\n",
    "            # Download the model using the model id (for CPU)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float32,\n",
    "                cache_dir=(cache_dir / model_key),\n",
    "                token=HF_ACCESS_TOKEN,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            model.to(\"cpu\")\n",
    "\n",
    "        # Apply the LoRA adapters to the model\n",
    "        model = apply_lora_adapter(model)\n",
    "\n",
    "        end_time: float = time.time()\n",
    "        elapsed: float = end_time - start_time\n",
    "        print(f'[COMPLETE] \"{model_key}\" ready in {elapsed:.2f}s.\\n')\n",
    "\n",
    "        return cls(tokenizer, model, model_key, model_id, fp16_available)\n",
    "\n",
    "    def train_and_evaluate(\n",
    "        self,\n",
    "        dataset: DatasetDict = quest_set if ACTIVATE_FULL else quest_subset,\n",
    "        max_length: int = MAX_LENGTH,\n",
    "        learning_rate: int = LR_RATE,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        epochs: int = N_EPOCHS,\n",
    "        seed: int = SEED,\n",
    "        max_grad_norm: float = MAX_GRAD_NORM,\n",
    "        logging_steps: int = LOGGING_STEPS,\n",
    "        eval_steps: int = EVAL_STEPS,\n",
    "        warmup_steps: int = WARMUP_STEPS,\n",
    "        gradient_checkpointing: bool = GRADIENT_CHECKPOINTING,\n",
    "        load_best_model_at_end: bool = LOAD_BEST_MODEL_AT_END,\n",
    "        save_total_limit: int = SAVE_TOTAL_LIMIT,\n",
    "        eval_accumulation_steps: int = EVAL_ACCUMULATION_STEPS,\n",
    "        gradient_accumulation_steps: int = GRADIENT_ACCUMULATION_STEPS,\n",
    "        callbacks: list[TrainerCallback] = [\n",
    "            EarlyStoppingCallback(early_stopping_patience=2) if ACTIVATE_EVAL else None,\n",
    "            TensorBoardCallback() if ACTIVATE_TENSORBOARD else None,\n",
    "        ],\n",
    "        activate_fp16: bool = ACTIVATE_FP16,\n",
    "        activate_eval: bool = ACTIVATE_EVAL,\n",
    "        activate_save: bool = ACTIVATE_SAVE,\n",
    "        activate_test: bool = ACTIVATE_TEST,\n",
    "        activate_logs: bool = ACTIVATE_LOGS,\n",
    "        activate_tensorboard: bool = ACTIVATE_TENSORBOARD,\n",
    "        activate_callbacks: bool = ACTIVATE_CALLBACKS,\n",
    "        output_dir: PathLike = get_target_dirpath(\"out\"),\n",
    "        logging_dir: PathLike = get_target_dirpath(\"logs\"),\n",
    "    ) -> TrainingMetrics:\n",
    "        # Ensure the training and validation sets\n",
    "        if not all(split in dataset for split in [\"train\", \"val\"]):\n",
    "            raise ValueError(\"DatasetDict must contain both 'train' and 'val' splits.\")\n",
    "\n",
    "        # Ensure the output and logging directories\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "        start_time: float\n",
    "        end_time: float\n",
    "        elapsed: float\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Set the padding token for the tokenizer\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        # Tokenize the dataset with `max_length` padding\n",
    "        print(f\"[TOKENIZE] {self.model_key} ({self.model_id})\")\n",
    "        start_time = time.time()\n",
    "        tokenized_data: Dataset = dataset.map(\n",
    "            QuestGenLLM.tokenize_dataset,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"],\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer, \"max_length\": max_length},\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Set the model padding token (from the tokenizer)\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        # Turn off `use_cache` if `gradient_checkpointing` is on\n",
    "        self.model.config.use_cache = not gradient_checkpointing\n",
    "\n",
    "        # Set up the training configurations\n",
    "        training_args: TrainingArguments = TrainingArguments(\n",
    "            output_dir=(output_dir / self.model_key),\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            log_level=(\"info\" if activate_logs else \"error\"),\n",
    "            logging_steps=logging_steps,\n",
    "            eval_steps=eval_steps,\n",
    "            eval_strategy=(\"epoch\" if activate_eval else \"no\"),\n",
    "            save_strategy=(\"epoch\" if activate_save else \"no\"),\n",
    "            logging_dir=(logging_dir / self.model_key),\n",
    "            save_total_limit=save_total_limit,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "            fp16=(self.fp16_available and activate_fp16),\n",
    "            load_best_model_at_end=load_best_model_at_end,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            seed=seed,\n",
    "            report_to=(\"tensorboard\" if activate_tensorboard else \"none\"),\n",
    "            label_names=[\"labels\"],\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            warmup_steps=warmup_steps,\n",
    "            logging_nan_inf_filter=True,\n",
    "            skip_memory_metrics=True,\n",
    "        )\n",
    "\n",
    "        # Set up the data collator for the model\n",
    "        data_collator: DataCollatorForLanguageModeling = (\n",
    "            DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)\n",
    "        )\n",
    "\n",
    "        # Set up the callbacks for the trainer\n",
    "        trainer_callbacks: list[TrainerCallback] = list(\n",
    "            filter(lambda callback: callback is not None, callbacks)\n",
    "        )\n",
    "        if activate_callbacks:\n",
    "            trainer_callbacks.append(LossLoggerCallback(self.metrics))\n",
    "\n",
    "        # Prepare and run the trainer\n",
    "        trainer: Trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=tokenized_data[\"train\"],\n",
    "            eval_dataset=(tokenized_data[\"val\"] if activate_eval else None),\n",
    "            callbacks=trainer_callbacks,\n",
    "        )\n",
    "\n",
    "        print(f\"[FINETUNE] {self.model_key} ({self.model_id})\")\n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Evaluate the model with the test set\n",
    "        if activate_test:\n",
    "            print(f\"[EVALUATE] {self.model_key} ({self.model_id})\")\n",
    "            start_time = time.time()\n",
    "            trainer.evaluate(eval_dataset=tokenized_data[\"test\"])\n",
    "            end_time = time.time()\n",
    "            elapsed = end_time - start_time\n",
    "            print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\")\n",
    "\n",
    "        # Save the model and tokenizer for later use\n",
    "        if activate_save:\n",
    "            trainer.save_model()\n",
    "            self.tokenizer.save_pretrained(save_directory=training_args.output_dir)\n",
    "\n",
    "        # Add to the training metrics map\n",
    "        TRAINING_METRICS[self.model_key] = self.metrics\n",
    "\n",
    "        return self.metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_dataset(\n",
    "        examples: dict[str, list[str]],\n",
    "        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "        max_length: int = MAX_LENGTH,\n",
    "    ) -> dict[str, list[list[int]]]:\n",
    "        encodings: BatchEncoding = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids: list[list[int]] = encodings[\"input_ids\"]\n",
    "        attention_mask: list[list[int]] = encodings[\"attention_mask\"]\n",
    "\n",
    "        labels: list[list[int]] = input_ids.clone()\n",
    "        labels[input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.tolist(),\n",
    "            \"attention_mask\": attention_mask.tolist(),\n",
    "            \"labels\": labels.tolist(),\n",
    "        }\n",
    "\n",
    "    def to_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"model_key\": self.model_key,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"device\": self.device,\n",
    "            \"dtype\": self.dtype,\n",
    "            \"vocab_size\": getattr(self.tokenizer, \"vocab_size\", \"unknown\"),\n",
    "            \"max_length\": getattr(self.tokenizer, \"model_max_length\", \"unknown\"),\n",
    "            \"model_type\": getattr(\n",
    "                getattr(self.model, \"config\", None), \"model_type\", \"unknown\"\n",
    "            ),\n",
    "            \"num_parameters\": self.model.num_parameters()\n",
    "            if hasattr(self.model, \"num_parameters\")\n",
    "            else \"N/A\",\n",
    "            \"fp16_available\": self.fp16_available,\n",
    "        }\n",
    "\n",
    "    def clear_cache(self, cache_dir: PathLike = get_cache_dirpath(\"models\")) -> None:\n",
    "        def remove_dir(dir_path: PathLike) -> None:\n",
    "            if os.path.exists(dir_path):\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"Cache directory '{dir_path}' removed.\")\n",
    "            else:\n",
    "                print(f\"No cache directory found at '{dir_path}'.\")\n",
    "\n",
    "        remove_dir(cache_dir / self.model_key)\n",
    "\n",
    "    def print_model_information(self) -> None:\n",
    "        print(json.dumps(self.to_dict(), indent=2))\n",
    "\n",
    "    def inspect(self) -> None:\n",
    "        print(f\"Tokenizer ({self.model_id}):\\n{self.tokenizer}\\n\")\n",
    "        print(f\"Model ({self.model_id}):\\n{self.model}\\n\")\n",
    "        print(f\"Configuration ({self.model_id}):\\n{self.model.config}\")\n",
    "        print(f\"Padding Token [PAD]             : {self.tokenizer.pad_token}\")\n",
    "        print(f\"Begging of Sentence Token [BOS] : {self.tokenizer.bos_token}\")\n",
    "        print(f\"End of Sentence Token [EOS]     : {self.tokenizer.eos_token}\")\n",
    "        print(f\"Unknown Token [UNK]             : {self.tokenizer.unk_token}\")\n",
    "        print(f\"Padding Side                    : {self.tokenizer.padding_side}\")\n",
    "        print(f\"Padding Token ID                : {self.tokenizer.pad_token_id}\\n\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.model_key} ({self.model_id})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc25bd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] gpt2 (openai-community/gpt2)\n",
      "[LoRAINFO] trainable params: 1,179,648 || all params: 125,619,456 || trainable%: 0.9391\n",
      "[COMPLETE] \"gpt2\" ready in 1.25s.\n",
      "\n",
      "Tokenizer (openai-community/gpt2):\n",
      "GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n",
      "\n",
      "Model (openai-community/gpt2):\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=2304, nx=768)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=768, nx=768)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=3072, nx=768)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=768, nx=3072)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Configuration (openai-community/gpt2):\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"openai-community/gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Padding Token [PAD]             : None\n",
      "Begging of Sentence Token [BOS] : <|endoftext|>\n",
      "End of Sentence Token [EOS]     : <|endoftext|>\n",
      "Unknown Token [UNK]             : <|endoftext|>\n",
      "Padding Side                    : right\n",
      "Padding Token ID                : None\n",
      "\n",
      "[TOKENIZE] gpt2 (openai-community/gpt2)\n",
      "[COMPLETE] Elapsed: 0.06s\n",
      "\n",
      "[FINETUNE] gpt2 (openai-community/gpt2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.726000</td>\n",
       "      <td>4.960940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMPLETE] Elapsed: 14.99s\n",
      "\n",
      "[EVALUATE] gpt2 (openai-community/gpt2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMPLETE] Elapsed: 0.54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingMetrics(train_losses=[4.726], eval_losses=[4.960939884185791, 5.107525825500488], eval_results=[{'eval_loss': 4.960939884185791}, {'eval_loss': 5.107525825500488, 'eval_runtime': 0.5393, 'eval_samples_per_second': 42.648, 'eval_steps_per_second': 11.126, 'epoch': 1.0}])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train the GPT-2 Base model with the quest data\n",
    "gpt2_base: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"gpt2\", model_id=MODEL_IDENTIFIERS[\"gpt2\"]\n",
    ")\n",
    "gpt2_base.inspect()\n",
    "gpt2_base.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adc7e9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] gpt2-medium (openai-community/gpt2-medium)\n",
      "[LoRAINFO] trainable params: 3,145,728 || all params: 357,968,896 || trainable%: 0.8788\n",
      "[COMPLETE] \"gpt2-medium\" ready in 1.38s.\n",
      "\n",
      "Tokenizer (openai-community/gpt2-medium):\n",
      "GPT2TokenizerFast(name_or_path='openai-community/gpt2-medium', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n",
      "\n",
      "Model (openai-community/gpt2-medium):\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 1024)\n",
      "        (wpe): Embedding(1024, 1024)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-23): 24 x GPT2Block(\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=3072, nx=1024)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=1024, nx=1024)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=4096, nx=1024)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=1024, nx=4096)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Configuration (openai-community/gpt2-medium):\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"openai-community/gpt2-medium\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Padding Token [PAD]             : None\n",
      "Begging of Sentence Token [BOS] : <|endoftext|>\n",
      "End of Sentence Token [EOS]     : <|endoftext|>\n",
      "Unknown Token [UNK]             : <|endoftext|>\n",
      "Padding Side                    : right\n",
      "Padding Token ID                : None\n",
      "\n",
      "[TOKENIZE] gpt2-medium (openai-community/gpt2-medium)\n",
      "[COMPLETE] Elapsed: 0.06s\n",
      "\n",
      "[FINETUNE] gpt2-medium (openai-community/gpt2-medium)\n",
      "{'loss': 4.2756, 'grad_norm': 239.72549438476562, 'learning_rate': 2e-08, 'epoch': 0.8}\n",
      "{'eval_loss': 4.421889781951904, 'eval_runtime': 1.1975, 'eval_samples_per_second': 20.042, 'eval_steps_per_second': 5.01, 'epoch': 1.0}\n",
      "{'train_runtime': 36.3424, 'train_samples_per_second': 5.476, 'train_steps_per_second': 0.688, 'train_loss': 4.31046157836914, 'epoch': 1.0}\n",
      "[COMPLETE] Elapsed: 36.72s\n",
      "\n",
      "[EVALUATE] gpt2-medium (openai-community/gpt2-medium)\n",
      "{'eval_loss': 4.6748857498168945, 'eval_runtime': 1.7414, 'eval_samples_per_second': 13.207, 'eval_steps_per_second': 3.445, 'epoch': 1.0}\n",
      "[COMPLETE] Elapsed: 1.76s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingMetrics(train_losses=[4.2756], eval_losses=[4.421889781951904, 4.6748857498168945], eval_results=[{'eval_loss': 4.421889781951904, 'eval_runtime': 1.1975, 'eval_samples_per_second': 20.042, 'eval_steps_per_second': 5.01, 'epoch': 1.0}, {'eval_loss': 4.6748857498168945, 'eval_runtime': 1.7414, 'eval_samples_per_second': 13.207, 'eval_steps_per_second': 3.445, 'epoch': 1.0}])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train the GPT-2 Medium model with the quest data\n",
    "gpt2_medium: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"gpt2-medium\", model_id=MODEL_IDENTIFIERS[\"gpt2-medium\"]\n",
    ")\n",
    "gpt2_medium.inspect()\n",
    "gpt2_medium.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd11881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the GPT-2 Large model with the quest data\n",
    "gpt2_large: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"gpt2-large\", model_id=MODEL_IDENTIFIERS[\"gpt2-large\"]\n",
    ")\n",
    "gpt2_large.inspect()\n",
    "gpt2_large.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bda9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Llama 3.2 model with the quest data\n",
    "llama32: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"llama-3.2-1b-instruct\",\n",
    "    model_id=MODEL_IDENTIFIERS[\"llama-3.2-1b-instruct\"],\n",
    ")\n",
    "llama32.inspect()\n",
    "llama32.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the TinyLlama model with the quest data\n",
    "tinyllama: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"tinyllama-1.1b-chat\", model_id=MODEL_IDENTIFIERS[\"tinyllama-1.1b-chat\"]\n",
    ")\n",
    "tinyllama.inspect()\n",
    "tinyllama.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302fa2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
