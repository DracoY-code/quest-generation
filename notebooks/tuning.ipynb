{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aab5fc",
   "metadata": {},
   "source": [
    "## QuestGen-LLM: Fine-Tuning & Evaluation\n",
    "\n",
    "This notebook covers the fine-tuning of various pre-trained _large language models_ (LLMs) on the prepared [\"quest\"](../data/quests_train.json) dataset. Each language model applied is trained and validated on the dataset (with frozen parameters) and the results of these evaluations are compared. The LLMs employed for this application are listed in the following table with their respective parameter count.\n",
    "\n",
    "| S. No. | Large Language Model             | Parameters | Developed By | Notes                                                 |\n",
    "| :----: | :------------------------------- | :--------: | :----------: | :---------------------------------------------------- |\n",
    "|   1.   | GPT-2[^1]                        |    124M    |    OpenAI    | Base model from the GPT-2 family                      |\n",
    "|   2.   | GPT-2 Medium[^2]                 |    355M    |    OpenAI    | Larger variant with improved language modeling        |\n",
    "|   3.   | GPT-2 Large[^3]                  |    774M    |    OpenAI    | Capable of generating more coherent longer text       |\n",
    "|   4.   | Llama-3.2-1B-Instruct[^4] †      |     1B     |     Meta     | Instruction-tuned model for question-answering        |\n",
    "|   5.   | TinyLlama-1.1B-Chat-v1.0[^5] \\*† |    1.1B    |  TinyLlama   | Lightweight chat-tuned model for constrained hardware |\n",
    "\n",
    "> Fine-tuning uses _supervised fine-tuning_\\* (SHF) and _reinforcement learning with human feedback_† (RLHF).\n",
    "\n",
    "The notebook also covers the performance evaluation of these pre-trained LLMs after training on the \"quest\" dataset. The generated quest descriptions (from the test set) are compared to their reference responses. These responses are then evaluated based on the following evaluation metrics:\n",
    "\n",
    "| S. No. | Metric         | Description                                                   | Preference                               |\n",
    "| :----: | -------------- | ------------------------------------------------------------- | ---------------------------------------- |\n",
    "|   1.   | Perplexity[^6] | Measures how \"confused\" the model is about its predictions.   | Lower values indicate less uncertainty.  |\n",
    "|   2.   | BLEU[^7]       | Compares n-gram overlap between generated and reference text. | Higher values indicate more overlap.     |\n",
    "|   3.   | ROUGE[^8]      | Measures how much reference content is captured (recall).     | Higher values indicate better recall.    |\n",
    "|   4.   | METEOR[^9]     | Evaluates similarity using synonyms, stems, and word order.   | Higher values indicate better alignment. |\n",
    "\n",
    "> Additionally, a _human evaluation method_ can further assess qualities like creativity, fluency, and coherence.\n",
    "\n",
    "Note that:\n",
    "\n",
    "- **BLEU:** Bilingual Evaluation Understudy\n",
    "- **ROUGE:** Recall-Oriented Understudy for Gisting Evaluation\n",
    "- **METEOR:** Metric for Evaluation of Translation with Explicit ORdering\n",
    "\n",
    "<!-- References -->\n",
    "\n",
    "[^1]: https://huggingface.co/openai-community/gpt2\n",
    "[^2]: https://huggingface.co/openai-community/gpt2-medium\n",
    "[^3]: https://huggingface.co/openai-community/gpt2-large\n",
    "[^4]: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "[^5]: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "[^6]: https://huggingface.co/spaces/evaluate-metric/perplexity\n",
    "[^7]: https://huggingface.co/spaces/evaluate-metric/bleu\n",
    "[^8]: https://huggingface.co/spaces/evaluate-metric/rouge\n",
    "[^9]: https://huggingface.co/spaces/evaluate-metric/meteor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "from typing import Any, Final, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7633f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerFast,\n",
    "    PreTrainedTokenizer,\n",
    "    TextGenerationPipeline,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.tokenization_utils_base import BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5629f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root: str = str(Path.cwd().parent.resolve())\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dirpath import get_cache_dirpath, get_target_dirpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HF access token from the environment\n",
    "HF_ACCESS_TOKEN: Final[str] = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "# Save the HF token to ~/.huggingface/token\n",
    "login(token=HF_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b883328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for the model identifiers: (model_key -> model_id)\n",
    "MODEL_IDENTIFIERS: Final[dict[str, str]] = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large\": \"openai-community/gpt2-large\",\n",
    "    \"llama-3.2-1b-instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"tinyllama-1.1b-chat\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir: Path = get_target_dirpath(\"data\")\n",
    "\n",
    "# Load the quest dataset\n",
    "quest_set: DatasetDict = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": str(data_dir / \"quests_train.txt\"),\n",
    "        \"val\": str(data_dir / \"quests_val.txt\"),\n",
    "        \"test\": str(data_dir / \"quests_test.txt\"),\n",
    "    },\n",
    "    cache_dir=str(data_dir / \".cache\"),\n",
    ")\n",
    "quest_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_set[\"train\"][:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5335935",
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_set[\"val\"][22:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for the target modules: (model_key -> target_modules)\n",
    "TARGET_MODULES: Final[dict[str, list[str]]] = {\n",
    "    \"gpt2\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"gpt2-medium\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"gpt2-large\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"llama-3.2-1b-instruct\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    \"tinyllama-1.1b-chat\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constants for model tuning here\n",
    "BATCH_SIZE: Final[int] = 4\n",
    "SEED: Final[int] = 42\n",
    "N_EPOCHS: Final[int] = 1  # Change to 3 for full dataset\n",
    "LR_RATE: Final[float] = 5e-7\n",
    "\n",
    "MAX_LENGTH: Final[int] = 128\n",
    "MAX_GRAD_NORM: Final[float] = 1.0\n",
    "LOGGING_STEPS: Final[int] = 20\n",
    "EVAL_STEPS: Final[int] = 50\n",
    "WARMUP_STEPS: Final[int] = 50\n",
    "\n",
    "SAVE_TOTAL_LIMIT: Final[int] = 1\n",
    "EVAL_ACCUMULATION_STEPS: Final[int] = 2\n",
    "GRADIENT_ACCUMULATION_STEPS: Final[int] = 2\n",
    "\n",
    "GRADIENT_CHECKPOINTING: Final[bool] = False  # Turn off for CPU training\n",
    "LOAD_BEST_MODEL_AT_END: Final[bool] = True\n",
    "\n",
    "ACTIVATE_FP16: Final[bool] = False\n",
    "ACTIVATE_EVAL: Final[bool] = True\n",
    "ACTIVATE_SAVE: Final[bool] = True\n",
    "ACTIVATE_LOGS: Final[bool] = False\n",
    "ACTIVATE_TENSORBOARD: Final[bool] = True\n",
    "ACTIVATE_CALLBACKS: Final[bool] = True\n",
    "ACTIVATE_FULL: Final[bool] = False  # Full dataset or subset\n",
    "\n",
    "FRACTION: Final[float] = 0.01  # 1% of the Quest dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a subset of the quest dataset\n",
    "quest_subset: DatasetDict = DatasetDict(\n",
    "    {\n",
    "        \"train\": quest_set[\"train\"].select(\n",
    "            range(int(FRACTION * quest_set[\"train\"].num_rows))\n",
    "        ),\n",
    "        \"val\": quest_set[\"val\"].select(\n",
    "            range(int(FRACTION * quest_set[\"val\"].num_rows))\n",
    "        ),\n",
    "        \"test\": quest_set[\"test\"].select(\n",
    "            range(int(FRACTION * quest_set[\"test\"].num_rows))\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    train_losses: list[float] = field(default_factory=list)\n",
    "    eval_losses: list[float] = field(default_factory=list)\n",
    "    learning_rates: list[float] = field(default_factory=list)\n",
    "    grad_norms: list[float] = field(default_factory=list)\n",
    "    global_steps: list[int] = field(default_factory=list)\n",
    "    epochs: list[float] = field(default_factory=list)\n",
    "    eval_results: list[dict[str, float]] = field(default_factory=list)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"TrainingMetrics(\\n\"\n",
    "            f\"  train_losses={self.train_losses},\\n\"\n",
    "            f\"  eval_losses={self.eval_losses},\\n\"\n",
    "            f\"  learning_rates={self.learning_rates},\\n\"\n",
    "            f\"  grad_norms={self.grad_norms},\\n\"\n",
    "            f\"  global_steps={self.global_steps},\\n\"\n",
    "            f\"  epochs={self.epochs},\\n\"\n",
    "            f\")\"\n",
    "        )\n",
    "\n",
    "    def to_dict(self) -> dict[str, list[int | float]]:\n",
    "        return {\n",
    "            \"train_losses\": self.train_losses,\n",
    "            \"eval_losses\": self.eval_losses,\n",
    "            \"learning_rates\": self.learning_rates,\n",
    "            \"grad_norms\": self.grad_norms,\n",
    "            \"global_steps\": self.global_steps,\n",
    "            \"epochs\": self.epochs,\n",
    "        }\n",
    "\n",
    "\n",
    "# Map for storing training metrics: (model_key -> metrics)\n",
    "TRAINING_METRICS: dict[str, Optional[TrainingMetrics]] = {\n",
    "    k: None for k in MODEL_IDENTIFIERS.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f98d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, metrics: TrainingMetrics):\n",
    "        self.metrics: TrainingMetrics = metrics\n",
    "        self.prev_epoch: Optional[float] = None\n",
    "\n",
    "    def on_log(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        logs: Optional[dict[str, float]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        # Capture training losses during logging\n",
    "        if \"loss\" in logs:\n",
    "            self.metrics.train_losses.append(logs[\"loss\"])\n",
    "\n",
    "        # Capture evaluation losses during logging\n",
    "        if \"eval_loss\" in logs:\n",
    "            self.metrics.eval_losses.append(logs[\"eval_loss\"])\n",
    "\n",
    "        # Capture learning rates during logging\n",
    "        if \"learning_rate\" in logs:\n",
    "            self.metrics.learning_rates.append(logs[\"learning_rate\"])\n",
    "\n",
    "        # Capture gradient norms during logging\n",
    "        if \"grad_norm\" in logs:\n",
    "            self.metrics.grad_norms.append(logs[\"grad_norm\"])\n",
    "\n",
    "        # Capture global steps consistently\n",
    "        self.metrics.global_steps.append(state.global_step)\n",
    "\n",
    "        # Only log the epoch once per epoch change\n",
    "        if state.epoch is not None and state.epoch != self.prev_epoch:\n",
    "            self.metrics.epochs.append(state.epoch)\n",
    "            self.prev_epoch = state.epoch\n",
    "\n",
    "        return super().on_log(args, state, control, **kwargs)\n",
    "\n",
    "    def on_evaluate(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        metrics: Optional[dict[str, float]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if metrics is None:\n",
    "            return\n",
    "\n",
    "        # Capture evaluation results on evaluation\n",
    "        self.metrics.eval_results.append(metrics)\n",
    "\n",
    "        return super().on_evaluate(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22831c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestGenLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "        model: PreTrainedModel,\n",
    "        model_key: str,  # Alias for the model, e.g, \"gpt2\"\n",
    "        model_id: str,  # Hugging Face model name, e.g., \"openai-community/gpt2\"\n",
    "        fp16_available: bool,  # Mixed precision\n",
    "        device: Optional[str] = None,\n",
    "        dtype: Optional[str] = None,\n",
    "        metrics: Optional[TrainingMetrics] = None,\n",
    "    ):\n",
    "        self.tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = tokenizer\n",
    "        self.model: PreTrainedModel = model\n",
    "        self.model_key: str = model_key\n",
    "        self.model_id: str = model_id\n",
    "        self.fp16_available: bool = fp16_available\n",
    "\n",
    "        # Automatically determine the device used by the model\n",
    "        self.device: str = (\n",
    "            device\n",
    "            if isinstance(device, str)\n",
    "            else str(getattr(self.model, \"device\", \"N/A\"))\n",
    "        )\n",
    "\n",
    "        # Automatically determine the dtype used by the model\n",
    "        self.dtype: str = (\n",
    "            dtype\n",
    "            if isinstance(dtype, str)\n",
    "            else str(getattr(self.model, \"dtype\", \"N/A\")).replace(\"torch.\", \"\")\n",
    "        )\n",
    "\n",
    "        # Initialize dataclass for storing training metrics\n",
    "        self.metrics: TrainingMetrics = (\n",
    "            metrics if isinstance(metrics, TrainingMetrics) else TrainingMetrics()\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_key: str,\n",
    "        model_id: str,\n",
    "        cache_dir: PathLike = get_cache_dirpath(\"models\"),\n",
    "        seed: int = SEED,\n",
    "        use_cpu: bool = False,\n",
    "    ) -> QuestGenLLM:\n",
    "        def apply_lora_adapter(\n",
    "            model: PreTrainedModel,\n",
    "            r: int = 8,\n",
    "            alpha: int = 16,\n",
    "            dropout: float = 0.1,\n",
    "            task_type: str = \"CAUSAL_LM\",\n",
    "        ) -> PreTrainedModel:\n",
    "            # Prepare model for k-bit training\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "            # Set correct `fan_in_fan_out` based on model type\n",
    "            #\n",
    "            # False [default] - for Linear layers\n",
    "            # True - for Conv1D layers (like GPT)\n",
    "            fan_in_fan_out: bool = False\n",
    "            if \"gpt\" in getattr(model.config, \"model_type\", \"\").lower():\n",
    "                fan_in_fan_out = True\n",
    "\n",
    "            # Define the LoRA config\n",
    "            lora_config: LoraConfig = LoraConfig(\n",
    "                r=r,\n",
    "                lora_alpha=alpha,\n",
    "                lora_dropout=dropout,\n",
    "                target_modules=TARGET_MODULES[model_key],\n",
    "                bias=\"none\",\n",
    "                task_type=task_type,\n",
    "                fan_in_fan_out=fan_in_fan_out,\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # Apply LoRA adapters to the model\n",
    "                model = get_peft_model(model, lora_config)\n",
    "            except Exception as e:\n",
    "                print(f\"[LoRAINFO] Adapter failed to apply: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Display information about the model parameters\n",
    "            trainable_params: int = sum(\n",
    "                p.numel() for p in model.parameters() if p.requires_grad\n",
    "            )\n",
    "            all_params: int = sum(p.numel() for p in model.parameters())\n",
    "            trainable_percent: float = 100 * trainable_params / all_params\n",
    "            print(\n",
    "                \"[LoRAINFO] trainable params: {:,} || all params: {:,} || trainable%: {:.4f}\".format(\n",
    "                    trainable_params, all_params, trainable_percent\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return model\n",
    "\n",
    "        print(f\"[DOWNLOAD] {model_key} ({model_id})\")\n",
    "        start_time: float = time.time()\n",
    "\n",
    "        # Clear PyTorch's CUDA memory cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Determine if mixed precision is available\n",
    "        fp16_available: bool = (\n",
    "            torch.cuda.is_available()\n",
    "            and torch.cuda.get_device_capability(0)[0] >= 7\n",
    "            and torch.cuda.get_device_capability(0)[1] >= 0\n",
    "        )\n",
    "\n",
    "        # Download the tokenizer using the model id\n",
    "        tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            cache_dir=(Path(cache_dir) / model_key),\n",
    "            use_fast=True,\n",
    "            token=HF_ACCESS_TOKEN,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        model: PreTrainedModel\n",
    "        if fp16_available and not use_cpu:\n",
    "            # Set the bitsandbytes configuration for quantization\n",
    "            bnb_config: BitsAndBytesConfig = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                # llm_int8_enable_fp32_cpu_offload=True,\n",
    "            )\n",
    "\n",
    "            # Download the model using the model id (for GPU)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16,\n",
    "                quantization_config=bnb_config,\n",
    "                cache_dir=(Path(cache_dir) / model_key),\n",
    "                token=HF_ACCESS_TOKEN,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            model.to(\"cuda\")\n",
    "        else:\n",
    "            # Download the model using the model id (for CPU)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float32,\n",
    "                cache_dir=(Path(cache_dir) / model_key),\n",
    "                token=HF_ACCESS_TOKEN,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            model.to(\"cpu\")\n",
    "\n",
    "        # Apply the LoRA adapters to the model\n",
    "        model = apply_lora_adapter(model)\n",
    "\n",
    "        end_time: float = time.time()\n",
    "        elapsed: float = end_time - start_time\n",
    "        print(f'[COMPLETE] \"{model_key}\" ready in {elapsed:.2f}s.\\n')\n",
    "\n",
    "        return cls(tokenizer, model, model_key, model_id, fp16_available)\n",
    "\n",
    "    def train_and_evaluate(\n",
    "        self,\n",
    "        dataset: DatasetDict = quest_set if ACTIVATE_FULL else quest_subset,\n",
    "        max_length: int = MAX_LENGTH,\n",
    "        learning_rate: int = LR_RATE,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        epochs: int = N_EPOCHS,\n",
    "        seed: int = SEED,\n",
    "        max_grad_norm: float = MAX_GRAD_NORM,\n",
    "        logging_steps: int = LOGGING_STEPS,\n",
    "        eval_steps: int = EVAL_STEPS,\n",
    "        warmup_steps: int = WARMUP_STEPS,\n",
    "        gradient_checkpointing: bool = GRADIENT_CHECKPOINTING,\n",
    "        load_best_model_at_end: bool = LOAD_BEST_MODEL_AT_END,\n",
    "        save_total_limit: int = SAVE_TOTAL_LIMIT,\n",
    "        eval_accumulation_steps: int = EVAL_ACCUMULATION_STEPS,\n",
    "        gradient_accumulation_steps: int = GRADIENT_ACCUMULATION_STEPS,\n",
    "        callbacks: list[TrainerCallback] = [\n",
    "            EarlyStoppingCallback(early_stopping_patience=2) if ACTIVATE_EVAL else None,\n",
    "            TensorBoardCallback() if ACTIVATE_TENSORBOARD else None,\n",
    "        ],\n",
    "        activate_fp16: bool = ACTIVATE_FP16,\n",
    "        activate_eval: bool = ACTIVATE_EVAL,\n",
    "        activate_save: bool = ACTIVATE_SAVE,\n",
    "        activate_logs: bool = ACTIVATE_LOGS,\n",
    "        activate_tensorboard: bool = ACTIVATE_TENSORBOARD,\n",
    "        activate_callbacks: bool = ACTIVATE_CALLBACKS,\n",
    "        output_dir: PathLike = get_target_dirpath(\"out\"),\n",
    "        logging_dir: PathLike = get_target_dirpath(\"logs\"),\n",
    "    ) -> TrainingMetrics:\n",
    "        # Ensure the training and validation sets\n",
    "        if not all(split in dataset for split in [\"train\", \"val\"]):\n",
    "            raise ValueError(\"DatasetDict must contain both 'train' and 'val' splits.\")\n",
    "\n",
    "        # Ensure the output and logging directories\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "        start_time: float\n",
    "        end_time: float\n",
    "        elapsed: float\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Set the padding token for the tokenizer\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        # Tokenize the dataset with `max_length` padding\n",
    "        print(f\"[TOKENIZE] {self.model_key} ({self.model_id})\")\n",
    "        start_time = time.time()\n",
    "        tokenized_data: Dataset = dataset.map(\n",
    "            QuestGenLLM.tokenize_dataset,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"],\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer, \"max_length\": max_length},\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Set the model padding token (from the tokenizer)\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        # Turn off `use_cache` if `gradient_checkpointing` is on\n",
    "        self.model.config.use_cache = not gradient_checkpointing\n",
    "\n",
    "        # Set up the training configurations\n",
    "        training_args: TrainingArguments = TrainingArguments(\n",
    "            output_dir=(Path(output_dir) / self.model_key),\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            log_level=(\"info\" if activate_logs else \"error\"),\n",
    "            logging_steps=logging_steps,\n",
    "            eval_steps=eval_steps,\n",
    "            eval_strategy=(\"epoch\" if activate_eval else \"no\"),\n",
    "            save_strategy=(\"epoch\" if activate_save else \"no\"),\n",
    "            logging_dir=(Path(logging_dir) / self.model_key),\n",
    "            save_total_limit=save_total_limit,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "            fp16=(self.fp16_available and activate_fp16),\n",
    "            load_best_model_at_end=load_best_model_at_end,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            seed=seed,\n",
    "            report_to=(\"tensorboard\" if activate_tensorboard else \"none\"),\n",
    "            label_names=[\"labels\"],\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            warmup_steps=warmup_steps,\n",
    "            logging_nan_inf_filter=True,\n",
    "            skip_memory_metrics=True,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "\n",
    "        # Set up the data collator for the model\n",
    "        data_collator: DataCollatorForLanguageModeling = (\n",
    "            DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)\n",
    "        )\n",
    "\n",
    "        # Set up the callbacks for the trainer\n",
    "        trainer_callbacks: list[TrainerCallback] = list(\n",
    "            filter(lambda callback: callback is not None, callbacks)\n",
    "        )\n",
    "        if activate_callbacks:\n",
    "            trainer_callbacks.append(LossLoggerCallback(self.metrics))\n",
    "\n",
    "        # Prepare and run the trainer\n",
    "        trainer: Trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=tokenized_data[\"train\"],\n",
    "            eval_dataset=(tokenized_data[\"val\"] if activate_eval else None),\n",
    "            callbacks=trainer_callbacks,\n",
    "        )\n",
    "\n",
    "        print(f\"[FINETUNE] {self.model_key} ({self.model_id})\")\n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Save the model and tokenizer for later use\n",
    "        if activate_save:\n",
    "            trainer.save_model()\n",
    "            self.tokenizer.save_pretrained(save_directory=training_args.output_dir)\n",
    "\n",
    "        # Add to the training metrics map\n",
    "        TRAINING_METRICS[self.model_key] = self.metrics\n",
    "\n",
    "        return self.metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_dataset(\n",
    "        examples: dict[str, list[str]],\n",
    "        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "        max_length: int = MAX_LENGTH,\n",
    "    ) -> dict[str, list[list[int]]]:\n",
    "        encodings: BatchEncoding = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids: list[list[int]] = encodings[\"input_ids\"]\n",
    "        attention_mask: list[list[int]] = encodings[\"attention_mask\"]\n",
    "\n",
    "        labels: list[list[int]] = input_ids.clone()\n",
    "        labels[input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.tolist(),\n",
    "            \"attention_mask\": attention_mask.tolist(),\n",
    "            \"labels\": labels.tolist(),\n",
    "        }\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 100,\n",
    "        max_length: int = 128,\n",
    "        num_return_sequences: int = 1,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        top_k: float = 50,\n",
    "        do_sample: bool = True,\n",
    "    ) -> list[str]:\n",
    "        self.model.base_model.model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Create a pipeline for quest description generation\n",
    "        generator: TextGenerationPipeline = TextGenerationPipeline(\n",
    "            model=self.model.base_model.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=(0 if self.device == \"cuda\" else -1),\n",
    "        )\n",
    "\n",
    "        # Generate the quest description from the pipeline\n",
    "        outputs: list[dict[str.str]] = generator(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            do_sample=do_sample,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        return [output.get(\"generated_text\", \"N/A\") for output in outputs]\n",
    "\n",
    "    def to_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"model_key\": self.model_key,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"device\": self.device,\n",
    "            \"dtype\": self.dtype,\n",
    "            \"vocab_size\": getattr(self.tokenizer, \"vocab_size\", \"unknown\"),\n",
    "            \"max_length\": getattr(self.tokenizer, \"model_max_length\", \"unknown\"),\n",
    "            \"model_type\": getattr(\n",
    "                getattr(self.model, \"config\", None), \"model_type\", \"unknown\"\n",
    "            ),\n",
    "            \"num_parameters\": self.model.num_parameters()\n",
    "            if hasattr(self.model, \"num_parameters\")\n",
    "            else \"N/A\",\n",
    "            \"fp16_available\": self.fp16_available,\n",
    "        }\n",
    "\n",
    "    def clear_cache(self, cache_dir: PathLike = get_cache_dirpath(\"models\")) -> None:\n",
    "        def remove_dir(dir_path: PathLike) -> None:\n",
    "            if os.path.exists(dir_path):\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"Cache directory '{dir_path}' removed.\")\n",
    "            else:\n",
    "                print(f\"No cache directory found at '{dir_path}'.\")\n",
    "\n",
    "        remove_dir(Path(cache_dir) / self.model_key)\n",
    "\n",
    "    def print_model_information(self) -> None:\n",
    "        print(json.dumps(self.to_dict(), indent=2))\n",
    "\n",
    "    def inspect(self) -> None:\n",
    "        print(f\"Tokenizer ({self.model_id}):\\n{self.tokenizer}\\n\")\n",
    "        print(f\"Model ({self.model_id}):\\n{self.model}\\n\")\n",
    "        print(f\"Configuration ({self.model_id}):\\n{self.model.config}\")\n",
    "        print(f\"Padding Token [PAD]             : {self.tokenizer.pad_token}\")\n",
    "        print(f\"Begging of Sentence Token [BOS] : {self.tokenizer.bos_token}\")\n",
    "        print(f\"End of Sentence Token [EOS]     : {self.tokenizer.eos_token}\")\n",
    "        print(f\"Unknown Token [UNK]             : {self.tokenizer.unk_token}\")\n",
    "        print(f\"Padding Side                    : {self.tokenizer.padding_side}\")\n",
    "        print(f\"Padding Token ID                : {self.tokenizer.pad_token_id}\\n\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.model_key} ({self.model_id})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the GPT-2 Base model with the quest data\n",
    "gpt2_base: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"gpt2\", model_id=MODEL_IDENTIFIERS[\"gpt2\"]\n",
    ")\n",
    "gpt2_base.inspect()\n",
    "gpt2_base.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt: str = \"\\n\".join(quest_subset[\"test\"][0:20][\"text\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e333f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt2_base.generate(prompt)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the GPT-2 Medium model with the quest data\n",
    "gpt2_medium: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"gpt2-medium\", model_id=MODEL_IDENTIFIERS[\"gpt2-medium\"]\n",
    ")\n",
    "gpt2_medium.inspect()\n",
    "gpt2_medium.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd11881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the GPT-2 Large model with the quest data\n",
    "gpt2_large: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"gpt2-large\", model_id=MODEL_IDENTIFIERS[\"gpt2-large\"]\n",
    ")\n",
    "gpt2_large.inspect()\n",
    "gpt2_large.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bda9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Llama 3.2 model with the quest data\n",
    "llama32: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"llama-3.2-1b-instruct\",\n",
    "    model_id=MODEL_IDENTIFIERS[\"llama-3.2-1b-instruct\"],\n",
    ")\n",
    "llama32.inspect()\n",
    "llama32.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the TinyLlama model with the quest data\n",
    "tinyllama: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"tinyllama-1.1b-chat\", model_id=MODEL_IDENTIFIERS[\"tinyllama-1.1b-chat\"]\n",
    ")\n",
    "tinyllama.inspect()\n",
    "tinyllama.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_metrics(output_dir: PathLike = get_target_dirpath(\"out\")) -> None:\n",
    "    metrics_dict: dict[str, dict[str, list[int | float]]] = {\n",
    "        k: v.to_dict() if isinstance(v, TrainingMetrics) else None\n",
    "        for k, v in TRAINING_METRICS.items()\n",
    "    }\n",
    "\n",
    "    json_file_path: Path = Path(output_dir) / \"training_metrics.json\"\n",
    "    with open(json_file_path, \"w\") as json_writer:\n",
    "        json.dump(metrics_dict, json_writer, indent=2)\n",
    "\n",
    "    print(f\"Saved to {json_file_path}\")\n",
    "\n",
    "\n",
    "save_training_metrics()  # Save metrics for future evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf2ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
