{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aab5fc",
   "metadata": {},
   "source": [
    "## QuestGen-LLM: Fine-Tuning\n",
    "\n",
    "This notebook covers the fine-tuning of various pre-trained _large language models_ (LLMs) on the prepared [\"quest\"](../data/quests_train.json) dataset. Each language model applied is trained and validated on the dataset (with frozen parameters) and the results of these evaluations are compared. The LLMs employed for this application are listed in the following table with their respective parameter count.\n",
    "\n",
    "| S. No. | Large Language Model                | Parameters | Developed By | Notes                                                 |\n",
    "| :----: | :---------------------------------- | :--------: | :----------: | :---------------------------------------------------- |\n",
    "|   1.   | GPT-2[^1]                           |    124M    |    OpenAI    | Base model from the GPT-2 family                      |\n",
    "|   2.   | GPT-2 Medium[^2]                    |    355M    |    OpenAI    | Larger variant with improved language modeling        |\n",
    "|   3.   | GPT-2 Large[^3]                     |    774M    |    OpenAI    | Capable of generating more coherent longer text       |\n",
    "|   4.   | TinyLlama-1.1B-Chat-v1.0[^4] \\*†    |    1.1B    |  TinyLlama   | Lightweight chat-tuned model for constrained hardware |\n",
    "|   5.   | DeepSeek-R1-Distill-Qwen-1.5B[^5] † |    1.5B    | DeepSeek AI  | Distilled model based on the Qwen architecture        |\n",
    "\n",
    "> Fine-tuning uses _supervised fine-tuning_\\* (SHF) and _reinforcement learning with human feedback_† (RLHF).\n",
    "\n",
    "<!-- References -->\n",
    "\n",
    "[^1]: https://huggingface.co/openai-community/gpt2\n",
    "[^2]: https://huggingface.co/openai-community/gpt2-medium\n",
    "[^3]: https://huggingface.co/openai-community/gpt2-large\n",
    "[^4]: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "[^5]: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7633f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "from typing import Any, Final\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerFast,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from utils.dirpath import get_cache_dirpath, get_target_dirpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b883328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for the model identifiers: (model_key -> model_id)\n",
    "model_ids: dict[str, str] = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large\": \"openai-community/gpt2-large\",\n",
    "    \"tinyllama-1.1b-chat\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"deepseek-r1-distill-qwen-1.5b\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "}\n",
    "\n",
    "# Get the HF access token from the environment\n",
    "HF_ACCESS_TOKEN: Final[str] = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "# Save the HF token to ~/.huggingface/token\n",
    "login(token=HF_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6fefdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 19954\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2486\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir: Path = get_target_dirpath(\"data\")\n",
    "\n",
    "# Load the quest dataset\n",
    "quest_set: DatasetDict = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": str(data_dir / \"quests_train.txt\"),\n",
    "        \"val\": str(data_dir / \"quests_val.txt\"),\n",
    "    },\n",
    "    cache_dir=str(data_dir / \".cache\"),\n",
    ")\n",
    "quest_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca1a797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['### Instruction:',\n",
       "  'Generate a video game quest description based on the following structured information.',\n",
       "  '',\n",
       "  '### Input:',\n",
       "  'Quest Name: Perilous Passage',\n",
       "  'Objective: save the Mana Queen',\n",
       "  'First Tasks: go through the gate to the Forsaken Vaults',\n",
       "  'First Task Locations: Forsaken Vaults - a perilous dungeon',\n",
       "  'Quest Giver: NONE - NONE (location: NONE)',\n",
       "  'Reward: NONE -  (amount: 1)',\n",
       "  'Characters: Mana Queen - a good female spirit (location: Forsaken Vaults)',\n",
       "  'Tools: NONE',\n",
       "  'Locations: NONE',\n",
       "  'Items: NONE',\n",
       "  'Enemies: NONE',\n",
       "  'Groups: NONE',\n",
       "  'Title: Torchlight II',\n",
       "  'Motivation: NONE',\n",
       "  '',\n",
       "  '### Response:',\n",
       "  \"The Mana Queen has come and gone Through this gate, she journeyed on. Follow her and pay the cost. Hasten forth, or she'll be lost.\"]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quest_set[\"train\"][:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5335935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Generate a video game quest description based on the following structured information.',\n",
       "  '',\n",
       "  '### Input:',\n",
       "  'Quest Name: A Child in the Lighthouse',\n",
       "  \"Objective: save Ardrouine's little son from worgs\",\n",
       "  'First Tasks: go to the abandoned lighthouse',\n",
       "  'First Task Locations:  - abandoned lighthouse to the northwest',\n",
       "  'Quest Giver: NONE - NONE (location: NONE)',\n",
       "  'Reward:  - coins (amount: 60)',\n",
       "  'Characters: NONE',\n",
       "  'Tools: NONE',\n",
       "  'Locations: NONE',\n",
       "  'Items: NONE',\n",
       "  'Enemies: NONE',\n",
       "  'Groups: NONE',\n",
       "  \"Title: Baldur's Gate\",\n",
       "  'Motivation: NONE',\n",
       "  '',\n",
       "  '### Response:',\n",
       "  \"Please help me, I am just poor Ardrouine! I don't know where else to turn. My little boy was playing in that abandoned lighthouse to the northwest when a pack of worgs surrounded it. Please just turn them back, and I can coax him down. There's not much time! I can pay you 60 coins: this money is all my husband brought back from market this past week. My son's life is worth this and so much more.\",\n",
       "  '']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quest_set[\"val\"][23:44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22831c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QuestGenLLM:\n",
    "    tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast\n",
    "    model: PreTrainedModel\n",
    "    model_key: str  # Alias for the model, e.g, \"gpt2\"\n",
    "    model_id: str  # Hugging Face model name, e.g., \"openai-community/gpt2\"\n",
    "    fp16_available: bool  # Mixed precision\n",
    "    device: str = field(init=False)\n",
    "    dtype: str = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Automatically determine the device used by the model\n",
    "        self.device = str(getattr(self.model, \"device\", \"N/A\"))\n",
    "\n",
    "        # Automatically determine the dtype used by the model\n",
    "        self.dtype = str(getattr(self.model, \"dtype\", \"N/A\")).replace(\"torch.\", \"\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_key: str,\n",
    "        model_id: str,\n",
    "        cache_dir: PathLike = get_cache_dirpath(\"models\"),\n",
    "        seed: int = 42,\n",
    "        use_cpu: bool = False,\n",
    "    ) -> QuestGenLLM:\n",
    "        def apply_lora_adapter(\n",
    "            model: PreTrainedModel,\n",
    "            r: int = 8,\n",
    "            alpha: int = 16,\n",
    "            dropout: float = 0.1,\n",
    "            target_modules: list[str] = [\"q_proj\", \"v_proj\"],\n",
    "            task_type: str = \"CAUSAL_LM\",\n",
    "        ) -> PreTrainedModel:\n",
    "            # Prepare model for k-bit training\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "            # Define the LoRA config\n",
    "            lora_config: LoraConfig = LoraConfig(\n",
    "                r=r,\n",
    "                lora_alpha=alpha,\n",
    "                lora_dropout=dropout,\n",
    "                target_modules=target_modules,\n",
    "                bias=\"none\",\n",
    "                task_type=task_type,\n",
    "            )\n",
    "\n",
    "            # Apply LoRA adapters to the model\n",
    "            try:\n",
    "                model = get_peft_model(model, lora_config)\n",
    "            except Exception as e:\n",
    "                print(f\"[LoRAINFO] Adapter failed to apply: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Display information about the model parameters\n",
    "            trainable_params: int = sum(\n",
    "                p.numel() for p in model.parameters() if p.requires_grad\n",
    "            )\n",
    "            all_params: int = sum(p.numel() for p in model.parameters())\n",
    "            trainable_percent: float = 100 * trainable_params / all_params\n",
    "            print(\n",
    "                \"[LoRAINFO] trainable params: {:,} || all params: {:,} || trainable%: {:.4f}\".format(\n",
    "                    trainable_params, all_params, trainable_percent\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return model\n",
    "\n",
    "        print(f\"[DOWNLOAD] {model_key} ({model_id})\")\n",
    "        start_time: float = time.time()\n",
    "\n",
    "        # Clear PyTorch's CUDA memory cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Determine if mixed precision is available\n",
    "        fp16_available: bool = (\n",
    "            torch.cuda.is_available()\n",
    "            and torch.cuda.get_device_capability(0)[0] >= 7\n",
    "            and torch.cuda.get_device_capability(0)[1] >= 0\n",
    "        )\n",
    "\n",
    "        # Download the tokenizer using the model id\n",
    "        tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            cache_dir=(cache_dir / model_key),\n",
    "            use_fast=True,\n",
    "            token=HF_ACCESS_TOKEN,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        model: PreTrainedModel\n",
    "        if fp16_available and not use_cpu:\n",
    "            # Set the bitsandbytes configuration for quantization\n",
    "            bnb_config: BitsAndBytesConfig = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                llm_int8_enable_fp32_cpu_offload=True,\n",
    "            )\n",
    "\n",
    "            # Download the model using the model id (for GPU)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16,\n",
    "                quantization_config=bnb_config,\n",
    "                cache_dir=(cache_dir / model_key),\n",
    "                token=HF_ACCESS_TOKEN,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "        else:\n",
    "            # Download the model using the model id (for CPU)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float32,\n",
    "                cache_dir=(cache_dir / model_key),\n",
    "                token=HF_ACCESS_TOKEN,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            model.to(\"cpu\")\n",
    "\n",
    "        # Apply the LoRA adapters to the model\n",
    "        model = apply_lora_adapter(model)\n",
    "\n",
    "        end_time: float = time.time()\n",
    "        elapsed: float = end_time - start_time\n",
    "        print(f'[COMPLETE] \"{model_key}\" ready in {elapsed:.2f}s.\\n')\n",
    "\n",
    "        return cls(tokenizer, model, model_key, model_id, fp16_available)\n",
    "\n",
    "    def tokenize_and_train(\n",
    "        self,\n",
    "        dataset: DatasetDict,\n",
    "        max_length: int = 256,\n",
    "        learning_rate: int = 5e-6,\n",
    "        batch_size: int = 1,\n",
    "        epochs: int = 1,\n",
    "        seed: int = 42,\n",
    "        logging_steps: int = 10,\n",
    "        output_dir: PathLike = get_target_dirpath(\"out\"),\n",
    "        logging_dir: PathLike = get_target_dirpath(\"logs\"),\n",
    "        gradient_checkpointing: bool = False,\n",
    "        load_best_model_at_end: bool = False,\n",
    "        callbacks: list[TrainerCallback] = [\n",
    "            EarlyStoppingCallback(early_stopping_patience=2)\n",
    "        ],\n",
    "        activate_fp16: bool = False,\n",
    "        activate_eval: bool = False,\n",
    "        activate_save: bool = False,\n",
    "        activate_tensorboard: bool = False,\n",
    "        activate_callbacks: bool = False,\n",
    "    ) -> Trainer:\n",
    "        # Ensure the training and validation sets\n",
    "        if not all(split in dataset for split in [\"train\", \"val\"]):\n",
    "            raise ValueError(\"DatasetDict must contain both 'train' and 'val' splits.\")\n",
    "\n",
    "        # Ensure the output and logging directories\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "        start_time: float\n",
    "        end_time: float\n",
    "        elapsed: float\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Set some configurations for the tokenizer\n",
    "        if self.tokenizer.pad_token is None:  # Padding token\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.model_max_length = 2048  # Max length\n",
    "        self.tokenizer.padding_side = \"left\"  # Padding side\n",
    "\n",
    "        # Tokenize the dataset with `max_length` padding\n",
    "        print(f\"[TOKENIZE] {self.model_key} ({self.model_id})\")\n",
    "        start_time = time.time()\n",
    "        tokenized_data: Dataset = dataset.map(\n",
    "            QuestGenLLM.tokenize_dataset,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"],\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer, \"max_length\": max_length},\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Set the model padding token (from the tokenizer)\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        # Turn off `use_cache` if `gradient_checkpointing` is on\n",
    "        self.model.config.use_cache = not gradient_checkpointing\n",
    "\n",
    "        # Set up the training configurations\n",
    "        training_args: TrainingArguments = TrainingArguments(\n",
    "            output_dir=(output_dir / self.model_key),\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            log_level=\"info\",\n",
    "            logging_steps=logging_steps,\n",
    "            eval_strategy=(\"epoch\" if activate_eval else \"no\"),\n",
    "            save_strategy=(\"epoch\" if activate_save else \"no\"),\n",
    "            logging_dir=(logging_dir / self.model_key),\n",
    "            save_total_limit=2,\n",
    "            gradient_accumulation_steps=2,\n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "            fp16=(self.fp16_available and activate_fp16),\n",
    "            load_best_model_at_end=load_best_model_at_end,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            seed=seed,\n",
    "            report_to=(\"tensorboard\" if activate_tensorboard else \"none\"),\n",
    "            label_names=[\"labels\"],\n",
    "        )\n",
    "\n",
    "        # Set up the data collator for the model\n",
    "        data_collator: DataCollatorForLanguageModeling = (\n",
    "            DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)\n",
    "        )\n",
    "\n",
    "        # Prepare and run the trainer\n",
    "        trainer: Trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=tokenized_data[\"train\"],\n",
    "            eval_dataset=(tokenized_data[\"val\"] if activate_eval else None),\n",
    "            callbacks=(callbacks if activate_callbacks else []),\n",
    "        )\n",
    "\n",
    "        print(f\"[FINETUNE] {self.model_key} ({self.model_id})\")\n",
    "        start_time: float = time.time()\n",
    "        trainer.train()\n",
    "        end_time: float = time.time()\n",
    "        elapsed: float = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Save the model and tokenizer for later use\n",
    "        if activate_save:\n",
    "            trainer.save_model()\n",
    "            self.tokenizer.save_pretrained(save_directory=training_args.output_dir)\n",
    "\n",
    "        return trainer\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_dataset(\n",
    "        examples: dict[str, list[str]],\n",
    "        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "        max_length: int = 256,\n",
    "    ) -> dict[str, list[int]]:\n",
    "        encodings: Dataset = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
    "        return encodings\n",
    "\n",
    "    def to_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"model_key\": self.model_key,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"device\": self.device,\n",
    "            \"dtype\": self.dtype,\n",
    "            \"vocab_size\": getattr(self.tokenizer, \"vocab_size\", \"unknown\"),\n",
    "            \"max_length\": getattr(self.tokenizer, \"model_max_length\", \"unknown\"),\n",
    "            \"model_type\": getattr(\n",
    "                getattr(self.model, \"config\", None), \"model_type\", \"unknown\"\n",
    "            ),\n",
    "            \"num_parameters\": self.model.num_parameters()\n",
    "            if hasattr(self.model, \"num_parameters\")\n",
    "            else \"N/A\",\n",
    "            \"fp16_available\": self.fp16_available,\n",
    "        }\n",
    "\n",
    "    def clear_cache(self, cache_dir: PathLike = get_cache_dirpath(\"models\")) -> None:\n",
    "        def remove_dir(dir_path: PathLike) -> None:\n",
    "            if os.path.exists(dir_path):\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"Cache directory '{dir_path}' removed.\")\n",
    "            else:\n",
    "                print(f\"No cache directory found at '{dir_path}'.\")\n",
    "\n",
    "        remove_dir(cache_dir / self.model_key)\n",
    "\n",
    "    def print_model_information(self) -> None:\n",
    "        print(json.dumps(self.to_dict(), indent=2))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.model_key} ({self.model_id})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c41be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a subset of the quest dataset\n",
    "quest_subset: DatasetDict = DatasetDict(\n",
    "    {\n",
    "        \"train\": quest_set[\"train\"].select(range(100)),\n",
    "        \"val\": quest_set[\"val\"].select(range(20)),\n",
    "    }\n",
    ")\n",
    "quest_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Llama 2 model\n",
    "llama2_model: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"llama-2-7b-chat\", model_id=\"meta-llama/Llama-2-7b-chat-hf\", use_cpu=True\n",
    ")\n",
    "llama2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f85486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Llama-2 model with the quest data\n",
    "llama2_trainer: Trainer = llama2_model.tokenize_and_train(dataset=quest_subset)\n",
    "llama2_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b96fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
