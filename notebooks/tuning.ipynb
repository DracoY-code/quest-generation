{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aab5fc",
   "metadata": {},
   "source": [
    "## QuestGen-LLM: Fine-Tuning & Evaluation\n",
    "\n",
    "This notebook covers the fine-tuning of various pre-trained _large language models_ (LLMs) on the prepared [\"quest\"](../data/quests_train.json) dataset. Each language model applied is trained and validated on the dataset (with frozen parameters) and the results of these evaluations are compared. The LLMs employed for this application are listed in the following table with their respective parameter count.\n",
    "\n",
    "| S. No. | Large Language Model             | Parameters | Developed By | Notes                                                 |\n",
    "| :----: | :------------------------------- | :--------: | :----------: | :---------------------------------------------------- |\n",
    "|   1.   | GPT-2[^1]                        |    124M    |    OpenAI    | Base model from the GPT-2 family                      |\n",
    "|   2.   | GPT-2 Medium[^2]                 |    355M    |    OpenAI    | Larger variant with improved language modeling        |\n",
    "|   3.   | GPT-2 Large[^3]                  |    774M    |    OpenAI    | Capable of generating more coherent longer text       |\n",
    "|   4.   | Llama-3.2-1B-Instruct[^4] †      |     1B     |     Meta     | Instruction-tuned model for question-answering        |\n",
    "|   5.   | TinyLlama-1.1B-Chat-v1.0[^5] \\*† |    1.1B    |  TinyLlama   | Lightweight chat-tuned model for constrained hardware |\n",
    "\n",
    "> Fine-tuning uses _supervised fine-tuning_\\* (SHF) and _reinforcement learning with human feedback_† (RLHF).\n",
    "\n",
    "The notebook also covers the performance evaluation of these pre-trained LLMs after training on the \"quest\" dataset. The generated quest descriptions (from the test set) are compared to their reference responses. These responses are then evaluated based on the following evaluation metrics:\n",
    "\n",
    "| S. No. | Metric         | Description                                                              | Preference                                                   |\n",
    "| :----: | -------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "|   1.   | Perplexity[^6] | Measures the uncertainty or unpredictability of the model's predictions. | Lower values indicate better performance (less uncertainty). |\n",
    "|   2.   | BLEU[^7]       | Measures the overlap of n-grams between generated and reference text.    | Higher values indicate better quality (more overlap).        |\n",
    "|   3.   | ROUGE[^8]      | Focuses on the recall of n-grams from the reference text.                | Higher values indicate better recall and performance.        |\n",
    "|   4.   | METEOR[^9]     | Considers synonymy, stemming, and word order to compare text similarity. | Higher values indicate better alignment with reference.      |\n",
    "\n",
    "> Additionally, a human evaluation method may be implemented to manually assess various qualities of the generated quest descriptions.\n",
    "\n",
    "Note that:\n",
    "\n",
    "- **BLEU:** Bilingual Evaluation Understudy\n",
    "- **ROUGE:** Recall-Oriented Understudy for Gisting Evaluation\n",
    "- **METEOR:** Metric for Evaluation of Translation with Explicit ORdering\n",
    "\n",
    "<!-- References -->\n",
    "\n",
    "[^1]: https://huggingface.co/openai-community/gpt2\n",
    "[^2]: https://huggingface.co/openai-community/gpt2-medium\n",
    "[^3]: https://huggingface.co/openai-community/gpt2-large\n",
    "[^4]: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "[^5]: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "[^6]: https://huggingface.co/spaces/evaluate-metric/perplexity\n",
    "[^7]: https://huggingface.co/spaces/evaluate-metric/bleu\n",
    "[^8]: https://huggingface.co/spaces/evaluate-metric/rouge\n",
    "[^9]: https://huggingface.co/spaces/evaluate-metric/meteor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e29e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "from typing import Any, Final, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7633f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerFast,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.tokenization_utils_base import BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5629f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root: str = str(Path.cwd().parent.resolve())\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5215bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dirpath import get_cache_dirpath, get_target_dirpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d946469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HF access token from the environment\n",
    "HF_ACCESS_TOKEN: Final[str] = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "# Save the HF token to ~/.huggingface/token\n",
    "login(token=HF_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b883328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for the model identifiers: (model_key -> model_id)\n",
    "MODEL_IDENTIFIERS: Final[dict[str, str]] = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large\": \"openai-community/gpt2-large\",\n",
    "    \"llama-3.2-1b-instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"tinyllama-1.1b-chat\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f6fefdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2328f6241aa49e5ba8994ec822089f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee664967886a42da828e61f1dd79d40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95293cf47d95417186c3f9b92ef91663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 19954\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2486\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2394\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir: Path = get_target_dirpath(\"data\")\n",
    "\n",
    "# Load the quest dataset\n",
    "quest_set: DatasetDict = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": str(data_dir / \"quests_train.txt\"),\n",
    "        \"val\": str(data_dir / \"quests_val.txt\"),\n",
    "        \"test\": str(data_dir / \"quests_test.txt\"),\n",
    "    },\n",
    "    cache_dir=str(data_dir / \".cache\"),\n",
    ")\n",
    "quest_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca1a797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['### Instruction:',\n",
       "  'Generate a video game quest description based on the following structured information.',\n",
       "  '',\n",
       "  '### Input:',\n",
       "  'Quest Name: Perilous Passage',\n",
       "  'Objective: save the Mana Queen',\n",
       "  'First Tasks: go through the gate to the Forsaken Vaults',\n",
       "  'First Task Locations: Forsaken Vaults - a perilous dungeon',\n",
       "  'Quest Giver: NONE - NONE (location: NONE)',\n",
       "  'Reward: NONE -  (amount: 1)',\n",
       "  'Characters: Mana Queen - a good female spirit (location: Forsaken Vaults)',\n",
       "  'Tools: NONE',\n",
       "  'Locations: NONE',\n",
       "  'Items: NONE',\n",
       "  'Enemies: NONE',\n",
       "  'Groups: NONE',\n",
       "  'Title: Torchlight II',\n",
       "  'Motivation: NONE',\n",
       "  '',\n",
       "  '### Response:',\n",
       "  \"The Mana Queen has come and gone Through this gate, she journeyed on. Follow her and pay the cost. Hasten forth, or she'll be lost.\"]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quest_set[\"train\"][:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5335935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['### Instruction:',\n",
       "  'Generate a video game quest description based on the following structured information.',\n",
       "  '',\n",
       "  '### Input:',\n",
       "  'Quest Name: A Child in the Lighthouse',\n",
       "  \"Objective: save Ardrouine's little son from worgs\",\n",
       "  'First Tasks: go to the abandoned lighthouse',\n",
       "  'First Task Locations:  - abandoned lighthouse to the northwest',\n",
       "  'Quest Giver: NONE - NONE (location: NONE)',\n",
       "  'Reward:  - coins (amount: 60)',\n",
       "  'Characters: NONE',\n",
       "  'Tools: NONE',\n",
       "  'Locations: NONE',\n",
       "  'Items: NONE',\n",
       "  'Enemies: NONE',\n",
       "  'Groups: NONE',\n",
       "  \"Title: Baldur's Gate\",\n",
       "  'Motivation: NONE',\n",
       "  '',\n",
       "  '### Response:',\n",
       "  \"Please help me, I am just poor Ardrouine! I don't know where else to turn. My little boy was playing in that abandoned lighthouse to the northwest when a pack of worgs surrounded it. Please just turn them back, and I can coax him down. There's not much time! I can pay you 60 coins: this money is all my husband brought back from market this past week. My son's life is worth this and so much more.\"]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quest_set[\"val\"][22:43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "188b14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for the target modules: (model_key -> target_modules)\n",
    "TARGET_MODULES: Final[dict[str, list[str]]] = {\n",
    "    \"gpt2\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"gpt2-medium\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"gpt2-large\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"llama-3.2-1b-instruct\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    \"tinyllama-1.1b-chat\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "967a5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constants for model tuning here\n",
    "BATCH_SIZE: Final[int] = 4\n",
    "SEED: Final[int] = 42\n",
    "N_EPOCHS: Final[int] = 1  # Change to 3 for full dataset\n",
    "LR_RATE: Final[float] = 5e-7\n",
    "\n",
    "MAX_LENGTH: Final[int] = 64\n",
    "MAX_GRAD_NORM: Final[float] = 1.0\n",
    "LOGGING_STEPS: Final[int] = 20\n",
    "EVAL_STEPS: Final[int] = 50\n",
    "WARMUP_STEPS: Final[int] = 50\n",
    "\n",
    "SAVE_TOTAL_LIMIT: Final[int] = 1\n",
    "EVAL_ACCUMULATION_STEPS: Final[int] = 2\n",
    "GRADIENT_ACCUMULATION_STEPS: Final[int] = 2\n",
    "\n",
    "GRADIENT_CHECKPOINTING: Final[bool] = True  # Turn off for CPU training\n",
    "LOAD_BEST_MODEL_AT_END: Final[bool] = True\n",
    "\n",
    "ACTIVATE_FP16: Final[bool] = False\n",
    "ACTIVATE_EVAL: Final[bool] = True\n",
    "ACTIVATE_SAVE: Final[bool] = True\n",
    "ACTIVATE_TEST: Final[bool] = True\n",
    "ACTIVATE_LOGS: Final[bool] = False\n",
    "ACTIVATE_TENSORBOARD: Final[bool] = True\n",
    "ACTIVATE_CALLBACKS: Final[bool] = True\n",
    "ACTIVATE_FULL: Final[bool] = False  # Full dataset or subset\n",
    "\n",
    "FRACTION: Final[float] = 0.01  # 1% of the Quest dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9697d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a subset of the quest dataset\n",
    "quest_subset: DatasetDict = DatasetDict(\n",
    "    {\n",
    "        \"train\": quest_set[\"train\"]\n",
    "        .shuffle(seed=SEED)\n",
    "        .select(range(int(FRACTION * quest_set[\"train\"].num_rows))),\n",
    "        \"val\": quest_set[\"val\"]\n",
    "        .shuffle(seed=SEED)\n",
    "        .select(range(int(FRACTION * quest_set[\"val\"].num_rows))),\n",
    "        \"test\": quest_set[\"test\"]\n",
    "        .shuffle(seed=SEED)\n",
    "        .select(range(int(FRACTION * quest_set[\"test\"].num_rows))),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9446f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    train_losses: list[float] = field(default_factory=list)\n",
    "    eval_losses: list[float] = field(default_factory=list)\n",
    "    learning_rates: list[float] = field(default_factory=list)\n",
    "    grad_norms: list[float] = field(default_factory=list)\n",
    "    global_steps: list[int] = field(default_factory=list)\n",
    "    epochs: list[float] = field(default_factory=list)\n",
    "    eval_results: list[dict[str, float]] = field(default_factory=list)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"TrainingMetrics(\\n\"\n",
    "            f\"  train_losses={self.train_losses},\\n\"\n",
    "            f\"  eval_losses={self.eval_losses[:-1]},\\n\"\n",
    "            f\"  test_losses={self.eval_losses[-1:]},\\n\"\n",
    "            f\"  learning_rates={self.learning_rates},\\n\"\n",
    "            f\"  grad_norms={self.grad_norms},\\n\"\n",
    "            f\"  global_steps={self.global_steps},\\n\"\n",
    "            f\"  epochs={self.epochs},\\n\"\n",
    "            f\")\"\n",
    "        )\n",
    "\n",
    "    def to_dict(self) -> dict[str, list[int | float]]:\n",
    "        return {\n",
    "            \"train_losses\": self.train_losses,\n",
    "            \"eval_losses\": self.eval_losses,\n",
    "            \"learning_rates\": self.learning_rates,\n",
    "            \"grad_norms\": self.grad_norms,\n",
    "            \"global_steps\": self.global_steps,\n",
    "            \"epochs\": self.epochs,\n",
    "        }\n",
    "\n",
    "\n",
    "# Map for storing training metrics: (model_key -> metrics)\n",
    "TRAINING_METRICS: dict[str, Optional[TrainingMetrics]] = {\n",
    "    k: None for k in MODEL_IDENTIFIERS.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f98d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, metrics: TrainingMetrics):\n",
    "        self.metrics: TrainingMetrics = metrics\n",
    "        self.prev_epoch: Optional[float] = None\n",
    "\n",
    "    def on_log(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        logs: Optional[dict[str, float]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        # Capture training losses during logging\n",
    "        if \"loss\" in logs:\n",
    "            self.metrics.train_losses.append(logs[\"loss\"])\n",
    "\n",
    "        # Capture evaluation losses during logging\n",
    "        if \"eval_loss\" in logs:\n",
    "            self.metrics.eval_losses.append(logs[\"eval_loss\"])\n",
    "\n",
    "        # Capture learning rates during logging\n",
    "        if \"learning_rate\" in logs:\n",
    "            self.metrics.learning_rates.append(logs[\"learning_rate\"])\n",
    "\n",
    "        # Capture gradient norms during logging\n",
    "        if \"grad_norm\" in logs:\n",
    "            self.metrics.grad_norms.append(logs[\"grad_norm\"])\n",
    "\n",
    "        # Capture global steps consistently\n",
    "        self.metrics.global_steps.append(state.global_step)\n",
    "\n",
    "        # Only log the epoch once per epoch change\n",
    "        if state.epoch is not None and state.epoch != self.prev_epoch:\n",
    "            self.metrics.epochs.append(state.epoch)\n",
    "            self.prev_epoch = state.epoch\n",
    "\n",
    "        return super().on_log(args, state, control, **kwargs)\n",
    "\n",
    "    def on_evaluate(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        metrics: Optional[dict[str, float]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if metrics is None:\n",
    "            return\n",
    "\n",
    "        # Capture evaluation results on evaluation\n",
    "        self.metrics.eval_results.append(metrics)\n",
    "\n",
    "        return super().on_evaluate(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d22831c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestGenLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "        model: PreTrainedModel,\n",
    "        model_key: str,  # Alias for the model, e.g, \"gpt2\"\n",
    "        model_id: str,  # Hugging Face model name, e.g., \"openai-community/gpt2\"\n",
    "        fp16_available: bool,  # Mixed precision\n",
    "        device: Optional[str] = None,\n",
    "        dtype: Optional[str] = None,\n",
    "        metrics: Optional[TrainingMetrics] = None,\n",
    "    ):\n",
    "        self.tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = tokenizer\n",
    "        self.model: PreTrainedModel = model\n",
    "        self.model_key: str = model_key\n",
    "        self.model_id: str = model_id\n",
    "        self.fp16_available: bool = fp16_available\n",
    "\n",
    "        # Automatically determine the device used by the model\n",
    "        self.device: str = (\n",
    "            device\n",
    "            if isinstance(device, str)\n",
    "            else str(getattr(self.model, \"device\", \"N/A\"))\n",
    "        )\n",
    "\n",
    "        # Automatically determine the dtype used by the model\n",
    "        self.dtype: str = (\n",
    "            dtype\n",
    "            if isinstance(dtype, str)\n",
    "            else str(getattr(self.model, \"dtype\", \"N/A\")).replace(\"torch.\", \"\")\n",
    "        )\n",
    "\n",
    "        # Initialize dataclass for storing training metrics\n",
    "        self.metrics: TrainingMetrics = (\n",
    "            metrics if isinstance(metrics, TrainingMetrics) else TrainingMetrics()\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_key: str,\n",
    "        model_id: str,\n",
    "        cache_dir: PathLike = get_cache_dirpath(\"models\"),\n",
    "        seed: int = SEED,\n",
    "        use_cpu: bool = False,\n",
    "    ) -> QuestGenLLM:\n",
    "        def apply_lora_adapter(\n",
    "            model: PreTrainedModel,\n",
    "            r: int = 8,\n",
    "            alpha: int = 16,\n",
    "            dropout: float = 0.1,\n",
    "            task_type: str = \"CAUSAL_LM\",\n",
    "        ) -> PreTrainedModel:\n",
    "            # Prepare model for k-bit training\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "            # Set correct `fan_in_fan_out` based on model type\n",
    "            #\n",
    "            # False [default] - for Linear layers\n",
    "            # True - for Conv1D layers (like GPT)\n",
    "            fan_in_fan_out: bool = False\n",
    "            if \"gpt\" in getattr(model.config, \"model_type\", \"\").lower():\n",
    "                fan_in_fan_out = True\n",
    "\n",
    "            # Define the LoRA config\n",
    "            lora_config: LoraConfig = LoraConfig(\n",
    "                r=r,\n",
    "                lora_alpha=alpha,\n",
    "                lora_dropout=dropout,\n",
    "                target_modules=TARGET_MODULES[model_key],\n",
    "                bias=\"none\",\n",
    "                task_type=task_type,\n",
    "                fan_in_fan_out=fan_in_fan_out,\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # Apply LoRA adapters to the model\n",
    "                model = get_peft_model(model, lora_config)\n",
    "            except Exception as e:\n",
    "                print(f\"[LoRAINFO] Adapter failed to apply: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Display information about the model parameters\n",
    "            trainable_params: int = sum(\n",
    "                p.numel() for p in model.parameters() if p.requires_grad\n",
    "            )\n",
    "            all_params: int = sum(p.numel() for p in model.parameters())\n",
    "            trainable_percent: float = 100 * trainable_params / all_params\n",
    "            print(\n",
    "                \"[LoRAINFO] trainable params: {:,} || all params: {:,} || trainable%: {:.4f}\".format(\n",
    "                    trainable_params, all_params, trainable_percent\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return model\n",
    "\n",
    "        print(f\"[DOWNLOAD] {model_key} ({model_id})\")\n",
    "        start_time: float = time.time()\n",
    "\n",
    "        # Clear PyTorch's CUDA memory cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Determine if mixed precision is available\n",
    "        fp16_available: bool = (\n",
    "            torch.cuda.is_available()\n",
    "            and torch.cuda.get_device_capability(0)[0] >= 7\n",
    "            and torch.cuda.get_device_capability(0)[1] >= 0\n",
    "        )\n",
    "\n",
    "        # Download the tokenizer using the model id\n",
    "        tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            cache_dir=(Path(cache_dir) / model_key),\n",
    "            use_fast=True,\n",
    "            token=HF_ACCESS_TOKEN,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        model: PreTrainedModel\n",
    "        if fp16_available and not use_cpu:\n",
    "            # Set the bitsandbytes configuration for quantization\n",
    "            bnb_config: BitsAndBytesConfig = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                # llm_int8_enable_fp32_cpu_offload=True,\n",
    "            )\n",
    "\n",
    "            # Download the model using the model id (for GPU)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16,\n",
    "                quantization_config=bnb_config,\n",
    "                cache_dir=(Path(cache_dir) / model_key),\n",
    "                token=HF_ACCESS_TOKEN,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            model.to(\"cuda\")\n",
    "        else:\n",
    "            # Download the model using the model id (for CPU)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float32,\n",
    "                cache_dir=(Path(cache_dir) / model_key),\n",
    "                token=HF_ACCESS_TOKEN,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            model.to(\"cpu\")\n",
    "\n",
    "        # Apply the LoRA adapters to the model\n",
    "        model = apply_lora_adapter(model)\n",
    "\n",
    "        end_time: float = time.time()\n",
    "        elapsed: float = end_time - start_time\n",
    "        print(f'[COMPLETE] \"{model_key}\" ready in {elapsed:.2f}s.\\n')\n",
    "\n",
    "        return cls(tokenizer, model, model_key, model_id, fp16_available)\n",
    "\n",
    "    def train_and_evaluate(\n",
    "        self,\n",
    "        dataset: DatasetDict = quest_set if ACTIVATE_FULL else quest_subset,\n",
    "        max_length: int = MAX_LENGTH,\n",
    "        learning_rate: int = LR_RATE,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        epochs: int = N_EPOCHS,\n",
    "        seed: int = SEED,\n",
    "        max_grad_norm: float = MAX_GRAD_NORM,\n",
    "        logging_steps: int = LOGGING_STEPS,\n",
    "        eval_steps: int = EVAL_STEPS,\n",
    "        warmup_steps: int = WARMUP_STEPS,\n",
    "        gradient_checkpointing: bool = GRADIENT_CHECKPOINTING,\n",
    "        load_best_model_at_end: bool = LOAD_BEST_MODEL_AT_END,\n",
    "        save_total_limit: int = SAVE_TOTAL_LIMIT,\n",
    "        eval_accumulation_steps: int = EVAL_ACCUMULATION_STEPS,\n",
    "        gradient_accumulation_steps: int = GRADIENT_ACCUMULATION_STEPS,\n",
    "        callbacks: list[TrainerCallback] = [\n",
    "            EarlyStoppingCallback(early_stopping_patience=2) if ACTIVATE_EVAL else None,\n",
    "            TensorBoardCallback() if ACTIVATE_TENSORBOARD else None,\n",
    "        ],\n",
    "        activate_fp16: bool = ACTIVATE_FP16,\n",
    "        activate_eval: bool = ACTIVATE_EVAL,\n",
    "        activate_save: bool = ACTIVATE_SAVE,\n",
    "        activate_test: bool = ACTIVATE_TEST,\n",
    "        activate_logs: bool = ACTIVATE_LOGS,\n",
    "        activate_tensorboard: bool = ACTIVATE_TENSORBOARD,\n",
    "        activate_callbacks: bool = ACTIVATE_CALLBACKS,\n",
    "        output_dir: PathLike = get_target_dirpath(\"out\"),\n",
    "        logging_dir: PathLike = get_target_dirpath(\"logs\"),\n",
    "    ) -> TrainingMetrics:\n",
    "        # Ensure the training and validation sets\n",
    "        if not all(split in dataset for split in [\"train\", \"val\", \"test\"]):\n",
    "            raise ValueError(\n",
    "                \"DatasetDict must contain 'train', 'val', and 'test' splits.\"\n",
    "            )\n",
    "\n",
    "        # Ensure the output and logging directories\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "        start_time: float\n",
    "        end_time: float\n",
    "        elapsed: float\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Set the padding token for the tokenizer\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        # Tokenize the dataset with `max_length` padding\n",
    "        print(f\"[TOKENIZE] {self.model_key} ({self.model_id})\")\n",
    "        start_time = time.time()\n",
    "        tokenized_data: Dataset = dataset.map(\n",
    "            QuestGenLLM.tokenize_dataset,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"],\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer, \"max_length\": max_length},\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Set the model padding token (from the tokenizer)\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        # Turn off `use_cache` if `gradient_checkpointing` is on\n",
    "        self.model.config.use_cache = not gradient_checkpointing\n",
    "\n",
    "        # Set up the training configurations\n",
    "        training_args: TrainingArguments = TrainingArguments(\n",
    "            output_dir=(Path(output_dir) / self.model_key),\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            log_level=(\"info\" if activate_logs else \"error\"),\n",
    "            logging_steps=logging_steps,\n",
    "            eval_steps=eval_steps,\n",
    "            eval_strategy=(\"epoch\" if activate_eval else \"no\"),\n",
    "            save_strategy=(\"epoch\" if activate_save else \"no\"),\n",
    "            logging_dir=(Path(logging_dir) / self.model_key),\n",
    "            save_total_limit=save_total_limit,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "            fp16=(self.fp16_available and activate_fp16),\n",
    "            load_best_model_at_end=load_best_model_at_end,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            seed=seed,\n",
    "            report_to=(\"tensorboard\" if activate_tensorboard else \"none\"),\n",
    "            label_names=[\"labels\"],\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            warmup_steps=warmup_steps,\n",
    "            logging_nan_inf_filter=True,\n",
    "            skip_memory_metrics=True,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "\n",
    "        # Set up the data collator for the model\n",
    "        data_collator: DataCollatorForLanguageModeling = (\n",
    "            DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)\n",
    "        )\n",
    "\n",
    "        # Set up the callbacks for the trainer\n",
    "        trainer_callbacks: list[TrainerCallback] = list(\n",
    "            filter(lambda callback: callback is not None, callbacks)\n",
    "        )\n",
    "        if activate_callbacks:\n",
    "            trainer_callbacks.append(LossLoggerCallback(self.metrics))\n",
    "\n",
    "        # Prepare and run the trainer\n",
    "        trainer: Trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=tokenized_data[\"train\"],\n",
    "            eval_dataset=(tokenized_data[\"val\"] if activate_eval else None),\n",
    "            callbacks=trainer_callbacks,\n",
    "        )\n",
    "\n",
    "        print(f\"[FINETUNE] {self.model_key} ({self.model_id})\")\n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Evaluate the model with the test set\n",
    "        if activate_test:\n",
    "            print(f\"[EVALUATE] {self.model_key} ({self.model_id})\")\n",
    "            start_time = time.time()\n",
    "            trainer.evaluate(eval_dataset=tokenized_data[\"test\"])\n",
    "            end_time = time.time()\n",
    "            elapsed = end_time - start_time\n",
    "            print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\")\n",
    "\n",
    "        # Save the model and tokenizer for later use\n",
    "        if activate_save:\n",
    "            trainer.save_model()\n",
    "            self.tokenizer.save_pretrained(save_directory=training_args.output_dir)\n",
    "\n",
    "        # Add to the training metrics map\n",
    "        TRAINING_METRICS[self.model_key] = self.metrics\n",
    "\n",
    "        return self.metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_dataset(\n",
    "        examples: dict[str, list[str]],\n",
    "        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "        max_length: int = MAX_LENGTH,\n",
    "    ) -> dict[str, list[list[int]]]:\n",
    "        encodings: BatchEncoding = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids: list[list[int]] = encodings[\"input_ids\"]\n",
    "        attention_mask: list[list[int]] = encodings[\"attention_mask\"]\n",
    "\n",
    "        labels: list[list[int]] = input_ids.clone()\n",
    "        labels[input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.tolist(),\n",
    "            \"attention_mask\": attention_mask.tolist(),\n",
    "            \"labels\": labels.tolist(),\n",
    "        }\n",
    "\n",
    "    def to_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"model_key\": self.model_key,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"device\": self.device,\n",
    "            \"dtype\": self.dtype,\n",
    "            \"vocab_size\": getattr(self.tokenizer, \"vocab_size\", \"unknown\"),\n",
    "            \"max_length\": getattr(self.tokenizer, \"model_max_length\", \"unknown\"),\n",
    "            \"model_type\": getattr(\n",
    "                getattr(self.model, \"config\", None), \"model_type\", \"unknown\"\n",
    "            ),\n",
    "            \"num_parameters\": self.model.num_parameters()\n",
    "            if hasattr(self.model, \"num_parameters\")\n",
    "            else \"N/A\",\n",
    "            \"fp16_available\": self.fp16_available,\n",
    "        }\n",
    "\n",
    "    def clear_cache(self, cache_dir: PathLike = get_cache_dirpath(\"models\")) -> None:\n",
    "        def remove_dir(dir_path: PathLike) -> None:\n",
    "            if os.path.exists(dir_path):\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"Cache directory '{dir_path}' removed.\")\n",
    "            else:\n",
    "                print(f\"No cache directory found at '{dir_path}'.\")\n",
    "\n",
    "        remove_dir(Path(cache_dir) / self.model_key)\n",
    "\n",
    "    def print_model_information(self) -> None:\n",
    "        print(json.dumps(self.to_dict(), indent=2))\n",
    "\n",
    "    def inspect(self) -> None:\n",
    "        print(f\"Tokenizer ({self.model_id}):\\n{self.tokenizer}\\n\")\n",
    "        print(f\"Model ({self.model_id}):\\n{self.model}\\n\")\n",
    "        print(f\"Configuration ({self.model_id}):\\n{self.model.config}\")\n",
    "        print(f\"Padding Token [PAD]             : {self.tokenizer.pad_token}\")\n",
    "        print(f\"Begging of Sentence Token [BOS] : {self.tokenizer.bos_token}\")\n",
    "        print(f\"End of Sentence Token [EOS]     : {self.tokenizer.eos_token}\")\n",
    "        print(f\"Unknown Token [UNK]             : {self.tokenizer.unk_token}\")\n",
    "        print(f\"Padding Side                    : {self.tokenizer.padding_side}\")\n",
    "        print(f\"Padding Token ID                : {self.tokenizer.pad_token_id}\\n\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.model_key} ({self.model_id})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc25bd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] gpt2 (openai-community/gpt2)\n",
      "[LoRAINFO] trainable params: 1,179,648 || all params: 83,152,128 || trainable%: 1.4187\n",
      "[COMPLETE] \"gpt2\" ready in 13.57s.\n",
      "\n",
      "Tokenizer (openai-community/gpt2):\n",
      "GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n",
      "\n",
      "Model (openai-community/gpt2):\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Configuration (openai-community/gpt2):\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"openai-community/gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Padding Token [PAD]             : None\n",
      "Begging of Sentence Token [BOS] : <|endoftext|>\n",
      "End of Sentence Token [EOS]     : <|endoftext|>\n",
      "Unknown Token [UNK]             : <|endoftext|>\n",
      "Padding Side                    : right\n",
      "Padding Token ID                : None\n",
      "\n",
      "[TOKENIZE] gpt2 (openai-community/gpt2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e371bf9cdd724e8c86275943be1862d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55afe558ccc4ca6bf2bf45160c1b133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13192f68dc454a6d944b9605fbfbbb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMPLETE] Elapsed: 0.22s\n",
      "\n",
      "[FINETUNE] gpt2 (openai-community/gpt2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.840100</td>\n",
       "      <td>5.018475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMPLETE] Elapsed: 9.07s\n",
      "\n",
      "[EVALUATE] gpt2 (openai-community/gpt2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMPLETE] Elapsed: 0.29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingMetrics(\n",
       "  train_losses=[4.8401],\n",
       "  eval_losses=[5.01847505569458],\n",
       "  test_losses=[5.1758575439453125],\n",
       "  learning_rates=[2e-07],\n",
       "  grad_norms=[0.6808575391769409],\n",
       "  global_steps=[20, 25, 25, 25],\n",
       "  epochs=[0.8, 1.0],\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train the GPT-2 Base model with the quest data\n",
    "gpt2_base: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"gpt2\", model_id=MODEL_IDENTIFIERS[\"gpt2\"]\n",
    ")\n",
    "gpt2_base.inspect()\n",
    "gpt2_base.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adc7e9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] gpt2-medium (openai-community/gpt2-medium)\n",
      "[LoRAINFO] trainable params: 3,145,728 || all params: 206,973,952 || trainable%: 1.5199\n",
      "[COMPLETE] \"gpt2-medium\" ready in 32.23s.\n",
      "\n",
      "Tokenizer (openai-community/gpt2-medium):\n",
      "GPT2TokenizerFast(name_or_path='openai-community/gpt2-medium', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n",
      "\n",
      "Model (openai-community/gpt2-medium):\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 1024)\n",
      "        (wpe): Embedding(1024, 1024)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-23): 24 x GPT2Block(\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Configuration (openai-community/gpt2-medium):\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"openai-community/gpt2-medium\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Padding Token [PAD]             : None\n",
      "Begging of Sentence Token [BOS] : <|endoftext|>\n",
      "End of Sentence Token [EOS]     : <|endoftext|>\n",
      "Unknown Token [UNK]             : <|endoftext|>\n",
      "Padding Side                    : right\n",
      "Padding Token ID                : None\n",
      "\n",
      "[TOKENIZE] gpt2-medium (openai-community/gpt2-medium)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b468320308f40f3aa25be04904818f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a53293db7c44c7b87472cdf01ab54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540976409a224eab814836b4359e08ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMPLETE] Elapsed: 0.22s\n",
      "\n",
      "[FINETUNE] gpt2-medium (openai-community/gpt2-medium)\n",
      "{'loss': 4.4273, 'grad_norm': 0.6081013083457947, 'learning_rate': 2e-07, 'epoch': 0.8}\n",
      "{'eval_loss': 4.694542407989502, 'eval_runtime': 0.5545, 'eval_samples_per_second': 43.282, 'eval_steps_per_second': 10.82, 'epoch': 1.0}\n",
      "{'train_runtime': 18.3637, 'train_samples_per_second': 10.837, 'train_steps_per_second': 1.361, 'train_loss': 4.440611267089844, 'epoch': 1.0}\n",
      "[COMPLETE] Elapsed: 18.57s\n",
      "\n",
      "[EVALUATE] gpt2-medium (openai-community/gpt2-medium)\n",
      "{'eval_loss': 4.817290306091309, 'eval_runtime': 0.4834, 'eval_samples_per_second': 47.581, 'eval_steps_per_second': 12.412, 'epoch': 1.0}\n",
      "[COMPLETE] Elapsed: 0.51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingMetrics(\n",
       "  train_losses=[4.4273],\n",
       "  eval_losses=[4.694542407989502],\n",
       "  test_losses=[4.817290306091309],\n",
       "  learning_rates=[2e-07],\n",
       "  grad_norms=[0.6081013083457947],\n",
       "  global_steps=[20, 25, 25, 25],\n",
       "  epochs=[0.8, 1.0],\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train the GPT-2 Medium model with the quest data\n",
    "gpt2_medium: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"gpt2-medium\", model_id=MODEL_IDENTIFIERS[\"gpt2-medium\"]\n",
    ")\n",
    "gpt2_medium.inspect()\n",
    "gpt2_medium.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddd11881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] gpt2-large (openai-community/gpt2-large)\n",
      "[LoRAINFO] trainable params: 5,898,240 || all params: 426,033,920 || trainable%: 1.3845\n",
      "[COMPLETE] \"gpt2-large\" ready in 68.75s.\n",
      "\n",
      "Tokenizer (openai-community/gpt2-large):\n",
      "GPT2TokenizerFast(name_or_path='openai-community/gpt2-large', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n",
      "\n",
      "Model (openai-community/gpt2-large):\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 1280)\n",
      "        (wpe): Embedding(1024, 1280)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-35): 36 x GPT2Block(\n",
      "            (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3840, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Configuration (openai-community/gpt2-large):\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"openai-community/gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Padding Token [PAD]             : None\n",
      "Begging of Sentence Token [BOS] : <|endoftext|>\n",
      "End of Sentence Token [EOS]     : <|endoftext|>\n",
      "Unknown Token [UNK]             : <|endoftext|>\n",
      "Padding Side                    : right\n",
      "Padding Token ID                : None\n",
      "\n",
      "[TOKENIZE] gpt2-large (openai-community/gpt2-large)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d6fdf65ce447e981f2b1d192fe9552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b1275c82224f6c8f3c7770ef6655a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df4412b8d334b7986b563eabdba9494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMPLETE] Elapsed: 0.33s\n",
      "\n",
      "[FINETUNE] gpt2-large (openai-community/gpt2-large)\n",
      "{'loss': 4.1431, 'grad_norm': 0.8129162192344666, 'learning_rate': 2e-07, 'epoch': 0.8}\n",
      "{'eval_loss': 4.435880184173584, 'eval_runtime': 1.4164, 'eval_samples_per_second': 16.944, 'eval_steps_per_second': 4.236, 'epoch': 1.0}\n",
      "{'train_runtime': 35.6592, 'train_samples_per_second': 5.581, 'train_steps_per_second': 0.701, 'train_loss': 4.176668472290039, 'epoch': 1.0}\n",
      "[COMPLETE] Elapsed: 35.90s\n",
      "\n",
      "[EVALUATE] gpt2-large (openai-community/gpt2-large)\n",
      "{'eval_loss': 4.608356952667236, 'eval_runtime': 1.1135, 'eval_samples_per_second': 20.655, 'eval_steps_per_second': 5.388, 'epoch': 1.0}\n",
      "[COMPLETE] Elapsed: 1.14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingMetrics(\n",
       "  train_losses=[4.1431],\n",
       "  eval_losses=[4.435880184173584],\n",
       "  test_losses=[4.608356952667236],\n",
       "  learning_rates=[2e-07],\n",
       "  grad_norms=[0.8129162192344666],\n",
       "  global_steps=[20, 25, 25, 25],\n",
       "  epochs=[0.8, 1.0],\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train the GPT-2 Large model with the quest data\n",
    "gpt2_large: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"gpt2-large\", model_id=MODEL_IDENTIFIERS[\"gpt2-large\"]\n",
    ")\n",
    "gpt2_large.inspect()\n",
    "gpt2_large.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9bda9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] llama-3.2-1b-instruct (meta-llama/Llama-3.2-1B-Instruct)\n",
      "[LoRAINFO] trainable params: 5,636,096 || all params: 754,911,232 || trainable%: 0.7466\n",
      "[COMPLETE] \"llama-3.2-1b-instruct\" ready in 57.34s.\n",
      "\n",
      "Tokenizer (meta-llama/Llama-3.2-1B-Instruct):\n",
      "PreTrainedTokenizerFast(name_or_path='meta-llama/Llama-3.2-1B-Instruct', vocab_size=128000, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128004: AddedToken(\"<|finetune_right_pad_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128005: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128008: AddedToken(\"<|eom_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128010: AddedToken(\"<|python_tag|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128011: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128012: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128013: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128014: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128015: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128016: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128017: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128018: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128019: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128020: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128021: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128022: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128023: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128024: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128025: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128026: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128027: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128028: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128029: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128030: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128031: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128032: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128033: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128034: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128035: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128036: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128037: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128038: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128039: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128040: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128041: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128042: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128043: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128044: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128045: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128046: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128047: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128048: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128049: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128050: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128051: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128052: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128053: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128054: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128055: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128056: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128057: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128058: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128059: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128060: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128061: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128062: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128063: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128064: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128065: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128066: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128067: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128068: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128069: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128070: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128071: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128072: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128073: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128074: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128075: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128076: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128077: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128078: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128079: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128080: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128081: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128082: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128083: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128084: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128085: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128086: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128087: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128088: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128089: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128090: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128091: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128092: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128093: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128094: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128095: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128096: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128097: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128098: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128099: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128100: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128101: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128102: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128103: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128104: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128105: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128106: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128107: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128108: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128109: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128110: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128111: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128112: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128113: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128114: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128115: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128116: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128117: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128118: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128119: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128120: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128121: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128122: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128123: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128124: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128125: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128126: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128127: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128128: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128129: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128130: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128131: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128132: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128133: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128134: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128135: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128136: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128137: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128138: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128139: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128140: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128141: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128142: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128143: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128144: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128145: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128146: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128147: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128148: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128149: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128150: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128151: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128152: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128153: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128154: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128155: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128156: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128157: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128158: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128159: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128160: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128161: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128162: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128163: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128164: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128165: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128166: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128167: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128168: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128169: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128170: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128171: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128172: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128173: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128174: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128175: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128176: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128177: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128178: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128179: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128180: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128181: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128182: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128183: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128184: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128185: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128186: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128187: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128188: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128189: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128190: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128191: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128192: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128193: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128194: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128195: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128196: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128197: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128198: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128199: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128200: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128201: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128202: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128203: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128204: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128205: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128206: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128207: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128208: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128209: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128210: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128211: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128212: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128213: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128214: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128215: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128216: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128217: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128218: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128219: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128220: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128221: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128222: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128223: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128224: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128225: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128226: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128227: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128228: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128229: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128230: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128231: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128232: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128233: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128234: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128235: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128236: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128237: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128238: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128239: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128240: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128241: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128242: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128243: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128244: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128245: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128246: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128247: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128248: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128249: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128250: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128251: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128252: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128253: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128254: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128255: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "\n",
      "Model (meta-llama/Llama-3.2-1B-Instruct):\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 2048)\n",
      "        (layers): ModuleList(\n",
      "          (0-15): 16 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Configuration (meta-llama/Llama-3.2-1B-Instruct):\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Padding Token [PAD]             : None\n",
      "Begging of Sentence Token [BOS] : <|begin_of_text|>\n",
      "End of Sentence Token [EOS]     : <|eot_id|>\n",
      "Unknown Token [UNK]             : None\n",
      "Padding Side                    : right\n",
      "Padding Token ID                : None\n",
      "\n",
      "[TOKENIZE] llama-3.2-1b-instruct (meta-llama/Llama-3.2-1B-Instruct)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03a85a21e3642e78a159da75496fdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ae1228c2984df1843ee5b0aa5822ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5124c0d24c44ecab195dd56391d878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMPLETE] Elapsed: 0.94s\n",
      "\n",
      "[FINETUNE] llama-3.2-1b-instruct (meta-llama/Llama-3.2-1B-Instruct)\n",
      "{'loss': 5.0441, 'grad_norm': 3.2810745239257812, 'learning_rate': 2e-07, 'epoch': 0.8}\n",
      "{'eval_loss': 6.402923583984375, 'eval_runtime': 1.4467, 'eval_samples_per_second': 16.589, 'eval_steps_per_second': 4.147, 'epoch': 1.0}\n",
      "{'train_runtime': 42.2842, 'train_samples_per_second': 4.706, 'train_steps_per_second': 0.591, 'train_loss': 5.0435488891601565, 'epoch': 1.0}\n",
      "[COMPLETE] Elapsed: 42.55s\n",
      "\n",
      "[EVALUATE] llama-3.2-1b-instruct (meta-llama/Llama-3.2-1B-Instruct)\n",
      "{'eval_loss': 5.629403114318848, 'eval_runtime': 1.5542, 'eval_samples_per_second': 14.799, 'eval_steps_per_second': 3.861, 'epoch': 1.0}\n",
      "[COMPLETE] Elapsed: 1.61s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingMetrics(\n",
       "  train_losses=[5.0441],\n",
       "  eval_losses=[6.402923583984375],\n",
       "  test_losses=[5.629403114318848],\n",
       "  learning_rates=[2e-07],\n",
       "  grad_norms=[3.2810745239257812],\n",
       "  global_steps=[20, 25, 25, 25],\n",
       "  epochs=[0.8, 1.0],\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train the Llama 3.2 model with the quest data\n",
    "llama32: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"llama-3.2-1b-instruct\",\n",
    "    model_id=MODEL_IDENTIFIERS[\"llama-3.2-1b-instruct\"],\n",
    ")\n",
    "llama32.inspect()\n",
    "llama32.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7bf488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] tinyllama-1.1b-chat (TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
      "[LoRAINFO] trainable params: 6,307,840 || all params: 621,914,112 || trainable%: 1.0143\n",
      "[COMPLETE] \"tinyllama-1.1b-chat\" ready in 59.91s.\n",
      "\n",
      "Tokenizer (TinyLlama/TinyLlama-1.1B-Chat-v1.0):\n",
      "LlamaTokenizerFast(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "\n",
      "Model (TinyLlama/TinyLlama-1.1B-Chat-v1.0):\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 2048)\n",
      "        (layers): ModuleList(\n",
      "          (0-21): 22 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5632, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Configuration (TinyLlama/TinyLlama-1.1B-Chat-v1.0):\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Padding Token [PAD]             : </s>\n",
      "Begging of Sentence Token [BOS] : <s>\n",
      "End of Sentence Token [EOS]     : </s>\n",
      "Unknown Token [UNK]             : <unk>\n",
      "Padding Side                    : right\n",
      "Padding Token ID                : 2\n",
      "\n",
      "[TOKENIZE] tinyllama-1.1b-chat (TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571f5160dedd4a2b90e1ba2e90029059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58f481dc0f54933b5cfa3f1c6cb1a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4770ebb4870549bd9dc8d44364c4cff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMPLETE] Elapsed: 8.95s\n",
      "\n",
      "[FINETUNE] tinyllama-1.1b-chat (TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
      "{'loss': 4.2654, 'grad_norm': 2.563000202178955, 'learning_rate': 2e-07, 'epoch': 0.8}\n",
      "{'eval_loss': 4.8114800453186035, 'eval_runtime': 2.889, 'eval_samples_per_second': 8.307, 'eval_steps_per_second': 2.077, 'epoch': 1.0}\n",
      "{'train_runtime': 63.8514, 'train_samples_per_second': 3.117, 'train_steps_per_second': 0.392, 'train_loss': 4.303471221923828, 'epoch': 1.0}\n",
      "[COMPLETE] Elapsed: 64.15s\n",
      "\n",
      "[EVALUATE] tinyllama-1.1b-chat (TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
      "{'eval_loss': 4.568667888641357, 'eval_runtime': 3.3849, 'eval_samples_per_second': 6.795, 'eval_steps_per_second': 1.773, 'epoch': 1.0}\n",
      "[COMPLETE] Elapsed: 3.42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingMetrics(\n",
       "  train_losses=[4.2654],\n",
       "  eval_losses=[4.8114800453186035],\n",
       "  test_losses=[4.568667888641357],\n",
       "  learning_rates=[2e-07],\n",
       "  grad_norms=[2.563000202178955],\n",
       "  global_steps=[20, 25, 25, 25],\n",
       "  epochs=[0.8, 1.0],\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train the TinyLlama model with the quest data\n",
    "tinyllama: QuestGenLLM = QuestGenLLM.from_pretrained(\n",
    "    model_key=\"tinyllama-1.1b-chat\", model_id=MODEL_IDENTIFIERS[\"tinyllama-1.1b-chat\"]\n",
    ")\n",
    "tinyllama.inspect()\n",
    "tinyllama.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d302fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /app/out/training_metrics.json\n"
     ]
    }
   ],
   "source": [
    "def save_training_metrics(output_dir: PathLike = get_target_dirpath(\"out\")) -> None:\n",
    "    metrics_dict: dict[str, dict[str, list[int | float]]] = {\n",
    "        k: v.to_dict() if isinstance(v, TrainingMetrics) else None\n",
    "        for k, v in TRAINING_METRICS.items()\n",
    "    }\n",
    "\n",
    "    json_file_path: Path = Path(output_dir) / \"training_metrics.json\"\n",
    "    with open(json_file_path, \"w\") as json_writer:\n",
    "        json.dump(metrics_dict, json_writer)\n",
    "\n",
    "    print(f\"Saved to {json_file_path}\")\n",
    "\n",
    "\n",
    "save_training_metrics()  # Save metrics for future evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf2ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
