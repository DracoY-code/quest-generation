{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aab5fc",
   "metadata": {},
   "source": [
    "## QuestGen-LLM: Fine-Tuning & Evaluation\n",
    "\n",
    "This notebook covers the fine-tuning of various pre-trained _large language models_ (LLMs) on the prepared [\"quest\"](../data/quests_train.json) dataset. Each language model applied is trained and validated on the dataset (with frozen parameters) and the results of these evaluations are compared. The LLMs employed for this application are listed in the following table with their respective parameter count.\n",
    "\n",
    "| S. No. | Large Language Model             | Parameters | Developed By | Notes                                                 |\n",
    "| :----: | :------------------------------- | :--------: | :----------: | :---------------------------------------------------- |\n",
    "|   1.   | GPT-2[^1]                        |    124M    |    OpenAI    | Base model from the GPT-2 family                      |\n",
    "|   2.   | GPT-2 Medium[^2]                 |    355M    |    OpenAI    | Larger variant with improved language modeling        |\n",
    "|   3.   | GPT-2 Large[^3]                  |    774M    |    OpenAI    | Capable of generating more coherent longer text       |\n",
    "|   4.   | Llama-3.2-1B-Instruct[^4] †      |     1B     |     Meta     | Instruction-tuned model for question-answering        |\n",
    "|   5.   | TinyLlama-1.1B-Chat-v1.0[^5] \\*† |    1.1B    |  TinyLlama   | Lightweight chat-tuned model for constrained hardware |\n",
    "\n",
    "> Fine-tuning uses _supervised fine-tuning_\\* (SHF) and _reinforcement learning with human feedback_† (RLHF).\n",
    "\n",
    "The notebook also covers the performance evaluation of these pre-trained LLMs after training on the \"quest\" dataset. The generated quest descriptions (from the test set) are compared to their reference responses. These responses are then evaluated based on the following evaluation metrics:\n",
    "\n",
    "| S. No. | Metric         | Description                                                                                                          | Preference                                |\n",
    "| :----: | -------------- | -------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- |\n",
    "|   1.   | Perplexity[^6] | Measures how \"confused\" the model is about its predictions.                                                          | Lower values indicate less uncertainty.   |\n",
    "|   2.   | BLEU[^7]       | Compares n-gram overlap between generated and reference text.                                                        | Higher values indicate more overlap.      |\n",
    "|   3.   | METEOR[^8]     | Evaluates similarity using synonyms, stems, and word order.                                                          | Higher values indicate better alignment.  |\n",
    "|   4.   | BERTScore[^9]  | Uses [\"BERT\"](https://huggingface.co/docs/transformers/en/model_doc/bert) embeddings to measure semantic similarity. | Higher values indicate better similarity. |\n",
    "\n",
    "> Additionally, a _human evaluation method_ can further assess qualities like creativity, fluency, and coherence.\n",
    "\n",
    "Note that:\n",
    "\n",
    "- **BLEU:** Bilingual Evaluation Understudy\n",
    "- **METEOR:** Metric for Evaluation of Translation with Explicit ORdering\n",
    "- **BERT:** Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "<!-- References -->\n",
    "\n",
    "[^1]: https://huggingface.co/openai-community/gpt2\n",
    "[^2]: https://huggingface.co/openai-community/gpt2-medium\n",
    "[^3]: https://huggingface.co/openai-community/gpt2-large\n",
    "[^4]: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "[^5]: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "[^6]: https://huggingface.co/spaces/evaluate-metric/perplexity\n",
    "[^7]: https://huggingface.co/spaces/evaluate-metric/bleu\n",
    "[^8]: https://huggingface.co/spaces/evaluate-metric/meteor\n",
    "[^9]: https://huggingface.co/spaces/evaluate-metric/bertscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "from typing import Any, Final, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7633f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from evaluate import EvaluationModule\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch import LongTensor\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerFast,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from transformers.generation.utils import GenerateOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5629f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root: str = str(Path.cwd().parent.resolve())\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dirpath import get_cache_dirpath, get_target_dirpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HF access token from the environment\n",
    "HF_ACCESS_TOKEN: Final[str] = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "# Save the HF token to ~/.huggingface/token\n",
    "login(token=HF_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off progress bars for the datasets\n",
    "datasets.disable_progress_bars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b883328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for the model identifiers: (model_key -> model_id)\n",
    "MODEL_IDENTIFIERS: Final[dict[str, str]] = {\n",
    "    \"gpt2\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large\": \"openai-community/gpt2-large\",\n",
    "    \"llama-3.2-1b-instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"tinyllama-1.1b-chat\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for the target modules: (model_key -> target_modules)\n",
    "TARGET_MODULES: Final[dict[str, list[str]]] = {\n",
    "    \"gpt2\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"gpt2-medium\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"gpt2-large\": [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    \"llama-3.2-1b-instruct\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    \"tinyllama-1.1b-chat\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constants for model tuning here\n",
    "BATCH_SIZE: Final[int] = 4  # Per-device training batch size\n",
    "SEED: Final[int] = 42  # Random seed for reproducibility\n",
    "N_EPOCHS: Final[int] = 5  # Number of training epochs\n",
    "LR_RATE: Final[float] = 5e-7  # Learning rate\n",
    "\n",
    "MAX_LENGTH: Final[int] = 512  # Max token length for input sequences\n",
    "MAX_GRAD_NORM: Final[float] = 1.0  # Gradient clipping threshold\n",
    "LOGGING_STEPS: Final[int] = 10  # Steps between logging metrics\n",
    "EVAL_STEPS: Final[int] = 10  # Steps between evaluations\n",
    "WARMUP_STEPS: Final[int] = 50  # Learning rate warmup steps\n",
    "\n",
    "SAVE_TOTAL_LIMIT: Final[int] = 1  # Max number of saved checkpoints\n",
    "EVAL_ACCUMULATION_STEPS: Final[int] = 2  # Eval batch accumulation steps\n",
    "GRADIENT_ACCUMULATION_STEPS: Final[int] = 2  # Grad batch accumulation steps\n",
    "\n",
    "GRADIENT_CHECKPOINTING: Final[bool] = True  # Reduce memory usage (slower)\n",
    "LOAD_BEST_MODEL_AT_END: Final[bool] = True  # Load best checkpoint by eval loss\n",
    "\n",
    "ACTIVATE_FP16: Final[bool] = False  # Enable 16-bit mixed precision training\n",
    "ACTIVATE_EVAL: Final[bool] = True  # Enable evaluation\n",
    "ACTIVATE_SAVE: Final[bool] = True  # Enable checkpoint saving\n",
    "ACTIVATE_LOGS: Final[bool] = False  # Enable logging to stdout\n",
    "ACTIVATE_TENSORBOARD: Final[bool] = True  # Enable TensorBoard logging\n",
    "ACTIVATE_CALLBACKS: Final[bool] = True  # Enable trainer callbacks\n",
    "ACTIVATE_FULL: Final[bool] = False  # Use full dataset or 10% subset\n",
    "\n",
    "FRACTION: Final[float] = 0.1  # % of dataset to use when not full (e.g., 10%)\n",
    "\n",
    "MAX_NEW_TOKEN: Final[int] = 100  # Max new tokens to generate during inference\n",
    "NUM_RETURN_SEQUENCES: Final[int] = 1  # Number of completions per prompt\n",
    "\n",
    "TEMPERATURE: Final[float] = 0.8  # Controls randomness; lower is more deterministic\n",
    "TOP_P: Final[float] = 0.9  # Sample from smallest set with cumulative prob ≥ top_p\n",
    "TOP_K: Final[int] = 50  # Sample from the top-k most likely tokens (fixed size)\n",
    "DO_SAMPLE: Final[bool] = True  # Enables sampling (if False, uses greedy decoding)\n",
    "REPETITION_PENALTY: Final[float] = 1.1  # Penalizes repeated tokens; >1 discourages\n",
    "\n",
    "TRAIN_AND_EVALUATE: Final[bool] = True  # Enable model training and evaluation\n",
    "GENERATE_AND_EVALUATE: Final[bool] = True  # Enable model generate functionalities\n",
    "CLEAR_CACHE: Final[bool] = False  # Enable cache clear after model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035df89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_response_dataset(\n",
    "    data_dir: PathLike = get_target_dirpath(\"data\"),\n",
    "    cache_dir: PathLike = get_cache_dirpath(\"data\"),\n",
    ") -> tuple[DatasetDict, list[str]]:\n",
    "    def load_split_dataset(split_type: str) -> tuple[Dataset, Optional[str]]:\n",
    "        nonlocal cache_dir\n",
    "\n",
    "        split_dir: Path = Path(data_dir) / split_type\n",
    "        split: Dataset = load_dataset(\n",
    "            \"json\",\n",
    "            data_files={split_type: str(split_dir / \"quests.json\")},\n",
    "            split=split_type,\n",
    "            cache_dir=str(Path(cache_dir) / split_type),\n",
    "        )\n",
    "\n",
    "        # If test set, remove response field and extract it\n",
    "        if split_type == \"test\":\n",
    "            test_responses: list[str] = split[\"response\"]\n",
    "            split = split.remove_columns([\"response\"])\n",
    "            return split, test_responses\n",
    "\n",
    "        return split, None\n",
    "\n",
    "    train_set: Dataset\n",
    "    val_set: Dataset\n",
    "    test_set: Dataset\n",
    "    test_references: list[str]\n",
    "\n",
    "    train_set, _ = load_split_dataset(\"train\")\n",
    "    val_set, _ = load_split_dataset(\"val\")\n",
    "    test_set, test_references = load_split_dataset(\"test\")\n",
    "\n",
    "    dataset: DatasetDict = DatasetDict(\n",
    "        {\n",
    "            \"train\": train_set,\n",
    "            \"val\": val_set,\n",
    "            \"test\": test_set,\n",
    "        }\n",
    "    )\n",
    "    return dataset, test_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QuestDataset:\n",
    "    records: DatasetDict\n",
    "    references: list[str]\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls,\n",
    "        data_dir: PathLike = get_target_dirpath(\"data\"),\n",
    "        cache_dir: PathLike = get_cache_dirpath(\"data\"),\n",
    "    ) -> QuestDataset:\n",
    "        return cls(*load_prompt_response_dataset(data_dir, cache_dir))\n",
    "\n",
    "    def get_subset(self, fraction: float = FRACTION) -> QuestDataset:\n",
    "        def get_split_set(split_type: str) -> Dataset:\n",
    "            num_rows: int = int(fraction * self.records[split_type].num_rows)\n",
    "            return self.records[split_type].select(range(num_rows))\n",
    "\n",
    "        return QuestDataset(\n",
    "            DatasetDict(\n",
    "                {\n",
    "                    \"train\": get_split_set(\"train\"),\n",
    "                    \"val\": get_split_set(\"val\"),\n",
    "                    \"test\": get_split_set(\"test\"),\n",
    "                }\n",
    "            ),\n",
    "            self.references[: int(fraction * len(self.references))],\n",
    "        )\n",
    "\n",
    "    def select_splits(self, splits: list[str]) -> DatasetDict:\n",
    "        valid_splits: set[str] = {\"train\", \"val\", \"test\"}\n",
    "        if not any(split in valid_splits for split in splits):\n",
    "            raise ValueError(\n",
    "                \"`splits` must contain at least one of: 'train', 'val', 'test'.\"\n",
    "            )\n",
    "        return DatasetDict(\n",
    "            {split: self.records[split] for split in splits if split in self.records}\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.records.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1713c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the quest dataset\n",
    "quest_set: QuestDataset = QuestDataset.load()\n",
    "quest_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a subset of the quest dataset\n",
    "quest_subset: QuestDataset = quest_set.get_subset()\n",
    "quest_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    train_losses: list[float] = field(default_factory=list)\n",
    "    eval_losses: list[float] = field(default_factory=list)\n",
    "    learning_rates: list[float] = field(default_factory=list)\n",
    "    grad_norms: list[float] = field(default_factory=list)\n",
    "    global_steps: list[int] = field(default_factory=list)\n",
    "    epochs: list[float] = field(default_factory=list)\n",
    "    eval_results: list[dict[str, float]] = field(default_factory=list)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"TrainingMetrics(\\n\"\n",
    "            f\"  train_losses={self.train_losses},\\n\"\n",
    "            f\"  eval_losses={self.eval_losses},\\n\"\n",
    "            f\"  learning_rates={self.learning_rates},\\n\"\n",
    "            f\"  grad_norms={self.grad_norms},\\n\"\n",
    "            f\"  global_steps={self.global_steps},\\n\"\n",
    "            f\"  epochs={self.epochs},\\n\"\n",
    "            f\")\"\n",
    "        )\n",
    "\n",
    "    def to_dict(self) -> dict[str, list[int | float]]:\n",
    "        return {\n",
    "            \"train_losses\": self.train_losses,\n",
    "            \"eval_losses\": self.eval_losses,\n",
    "            \"learning_rates\": self.learning_rates,\n",
    "            \"grad_norms\": self.grad_norms,\n",
    "            \"global_steps\": self.global_steps,\n",
    "            \"epochs\": self.epochs,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerationMetrics:\n",
    "    perplexity: float = 0.0\n",
    "    bleu: float = 0.0\n",
    "    meteor: float = 0.0\n",
    "    bertscore: dict[str, float] = field(default_factory=dict)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"GenerationMetrics(\\n\"\n",
    "            f\"  perplexity={self.perplexity},\\n\"\n",
    "            f\"  bleu={self.bleu},\\n\"\n",
    "            f\"  meteor={self.meteor},\\n\"\n",
    "            f\"  bertscore={self.bertscore},\\n\"\n",
    "            f\")\"\n",
    "        )\n",
    "\n",
    "    def to_dict(self) -> dict[str, float | dict[str, float]]:\n",
    "        return {\n",
    "            \"perplexity\": self.perplexity,\n",
    "            \"bleu\": self.bleu,\n",
    "            \"meteor\": self.meteor,\n",
    "            \"bertscore\": self.bertscore,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c85754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for storing training metrics: (model_key -> metrics)\n",
    "TRAINING_METRICS: dict[str, Optional[TrainingMetrics]] = {\n",
    "    k: None for k in MODEL_IDENTIFIERS.keys()\n",
    "}\n",
    "\n",
    "# Map for storing generation metrics: (model_key -> gen_metrics)\n",
    "GENERATION_METRICS: dict[str, Optional[GenerationMetrics]] = {\n",
    "    k: None for k in MODEL_IDENTIFIERS.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f98d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, metrics: TrainingMetrics):\n",
    "        self.metrics: TrainingMetrics = metrics\n",
    "        self.prev_epoch: Optional[float] = None\n",
    "\n",
    "    def on_log(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        logs: Optional[dict[str, float]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        # Capture training losses during logging\n",
    "        if \"loss\" in logs:\n",
    "            self.metrics.train_losses.append(logs[\"loss\"])\n",
    "\n",
    "        # Capture evaluation losses during logging\n",
    "        if \"eval_loss\" in logs:\n",
    "            self.metrics.eval_losses.append(logs[\"eval_loss\"])\n",
    "\n",
    "        # Capture learning rates during logging\n",
    "        if \"learning_rate\" in logs:\n",
    "            self.metrics.learning_rates.append(logs[\"learning_rate\"])\n",
    "\n",
    "        # Capture gradient norms during logging\n",
    "        if \"grad_norm\" in logs:\n",
    "            self.metrics.grad_norms.append(logs[\"grad_norm\"])\n",
    "\n",
    "        # Capture global steps consistently\n",
    "        self.metrics.global_steps.append(state.global_step)\n",
    "\n",
    "        # Only log the epoch once per epoch change\n",
    "        if state.epoch is not None and state.epoch != self.prev_epoch:\n",
    "            self.metrics.epochs.append(state.epoch)\n",
    "            self.prev_epoch = state.epoch\n",
    "\n",
    "        return super().on_log(args, state, control, **kwargs)\n",
    "\n",
    "    def on_evaluate(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        metrics: Optional[dict[str, float]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if metrics is None:\n",
    "            return\n",
    "\n",
    "        # Capture evaluation results on evaluation\n",
    "        self.metrics.eval_results.append(metrics)\n",
    "\n",
    "        return super().on_evaluate(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22831c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestGenLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "        model: PreTrainedModel,\n",
    "        model_key: str,  # Alias for the model, e.g, \"gpt2\"\n",
    "        model_id: str,  # Hugging Face model name, e.g., \"openai-community/gpt2\"\n",
    "        fp16_available: bool,  # Mixed precision\n",
    "        device: Optional[str] = None,\n",
    "        dtype: Optional[str] = None,\n",
    "        training_metrics: Optional[TrainingMetrics] = None,\n",
    "        generation_metrics: Optional[GenerationMetrics] = None,\n",
    "    ):\n",
    "        self.tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = tokenizer\n",
    "        self.model: PreTrainedModel = model\n",
    "        self.model_key: str = model_key\n",
    "        self.model_id: str = model_id\n",
    "        self.fp16_available: bool = fp16_available\n",
    "\n",
    "        # Automatically determine the device used by the model\n",
    "        self.device: str = (\n",
    "            device\n",
    "            if isinstance(device, str)\n",
    "            else str(getattr(self.model, \"device\", \"N/A\"))\n",
    "        )\n",
    "\n",
    "        # Automatically determine the dtype used by the model\n",
    "        self.dtype: str = (\n",
    "            dtype\n",
    "            if isinstance(dtype, str)\n",
    "            else str(getattr(self.model, \"dtype\", \"N/A\")).replace(\"torch.\", \"\")\n",
    "        )\n",
    "\n",
    "        # Initialize dataclass for storing training metrics\n",
    "        self.training_metrics: TrainingMetrics = (\n",
    "            training_metrics\n",
    "            if isinstance(training_metrics, TrainingMetrics)\n",
    "            else TrainingMetrics()\n",
    "        )\n",
    "\n",
    "        # Initialize dataclass for storing generation metrics\n",
    "        self.generation_metrics: GenerationMetrics = (\n",
    "            generation_metrics\n",
    "            if isinstance(generation_metrics, GenerationMetrics)\n",
    "            else GenerationMetrics()\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_key: str,\n",
    "        model_id: Optional[str] = None,\n",
    "        cache_dir: PathLike = get_cache_dirpath(\"models\"),\n",
    "        seed: int = SEED,\n",
    "        use_cpu: bool = False,\n",
    "    ) -> QuestGenLLM:\n",
    "        def apply_lora_adapter(\n",
    "            model: PreTrainedModel,\n",
    "            r: int = 8,\n",
    "            alpha: int = 16,\n",
    "            dropout: float = 0.1,\n",
    "            task_type: str = \"CAUSAL_LM\",\n",
    "        ) -> PreTrainedModel:\n",
    "            # Prepare model for k-bit training\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "            # Set correct `fan_in_fan_out` based on model type\n",
    "            #\n",
    "            # False [default] - for Linear layers\n",
    "            # True - for Conv1D layers (like GPT)\n",
    "            fan_in_fan_out: bool = False\n",
    "            if \"gpt\" in getattr(model.config, \"model_type\", \"\").lower():\n",
    "                fan_in_fan_out = True\n",
    "\n",
    "            # Define the LoRA config\n",
    "            lora_config: LoraConfig = LoraConfig(\n",
    "                r=r,\n",
    "                lora_alpha=alpha,\n",
    "                lora_dropout=dropout,\n",
    "                target_modules=TARGET_MODULES[model_key],\n",
    "                bias=\"none\",\n",
    "                task_type=task_type,\n",
    "                fan_in_fan_out=fan_in_fan_out,\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # Apply LoRA adapters to the model\n",
    "                model = get_peft_model(model, lora_config)\n",
    "            except Exception as e:\n",
    "                print(f\"[LoRAINFO] Adapter failed to apply: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Display information about the model parameters\n",
    "            trainable_params: int = sum(\n",
    "                p.numel() for p in model.parameters() if p.requires_grad\n",
    "            )\n",
    "            all_params: int = sum(p.numel() for p in model.parameters())\n",
    "            trainable_percent: float = 100 * trainable_params / all_params\n",
    "            print(\n",
    "                \"[LoRAINFO] trainable params: {:,} || all params: {:,} || trainable%: {:.4f}\".format(\n",
    "                    trainable_params, all_params, trainable_percent\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return model\n",
    "\n",
    "        if not model_id:\n",
    "            model_id = MODEL_IDENTIFIERS[model_key]\n",
    "\n",
    "        print(f\"[DOWNLOAD] {model_key} ({model_id})\")\n",
    "        start_time: float = time.time()\n",
    "\n",
    "        # Clear PyTorch's CUDA memory cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Determine if mixed precision is available\n",
    "        fp16_available: bool = (\n",
    "            torch.cuda.is_available()\n",
    "            and torch.cuda.get_device_capability(0)[0] >= 7\n",
    "            and torch.cuda.get_device_capability(0)[1] >= 0\n",
    "        )\n",
    "\n",
    "        # Download the tokenizer using the model id\n",
    "        tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            cache_dir=(Path(cache_dir) / model_key),\n",
    "            use_fast=True,\n",
    "            token=HF_ACCESS_TOKEN,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        model: PreTrainedModel\n",
    "        cache_dir: str = str(Path(cache_dir) / model_key)\n",
    "\n",
    "        if fp16_available and not use_cpu:\n",
    "            # Set the bitsandbytes configuration for quantization\n",
    "            bnb_config: BitsAndBytesConfig = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                # llm_int8_enable_fp32_cpu_offload=True,\n",
    "            )\n",
    "\n",
    "            # Download the model using the model id (for GPU)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16,\n",
    "                quantization_config=bnb_config,\n",
    "                cache_dir=cache_dir,\n",
    "                token=HF_ACCESS_TOKEN,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            model.to(\"cuda\")\n",
    "        else:\n",
    "            # Download the model using the model id (for CPU)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float32,\n",
    "                cache_dir=cache_dir,\n",
    "                token=HF_ACCESS_TOKEN,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            model.to(\"cpu\")\n",
    "\n",
    "        # Apply the LoRA adapters to the model\n",
    "        model = apply_lora_adapter(model)\n",
    "\n",
    "        end_time: float = time.time()\n",
    "        elapsed: float = end_time - start_time\n",
    "        print(f'[COMPLETE] \"{model_key}\" ready in {elapsed:.2f}s.\\n')\n",
    "\n",
    "        return cls(tokenizer, model, model_key, model_id, fp16_available)\n",
    "\n",
    "    @staticmethod\n",
    "    def perform_tokenization(\n",
    "        input_texts: list[str],\n",
    "        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "        max_length: int = MAX_LENGTH,\n",
    "    ) -> BatchEncoding:\n",
    "        return tokenizer(\n",
    "            input_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_dataset(\n",
    "        examples: dict[str, list[str]],\n",
    "        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "        max_length: int = MAX_LENGTH,\n",
    "    ) -> dict[str, list[list[int]]]:\n",
    "        inputs: BatchEncoding = QuestGenLLM.perform_tokenization(\n",
    "            examples[\"prompt\"], tokenizer, max_length\n",
    "        )\n",
    "        labels: BatchEncoding = QuestGenLLM.perform_tokenization(\n",
    "            examples[\"response\"], tokenizer, max_length\n",
    "        )\n",
    "\n",
    "        input_ids: list[list[int]] = inputs[\"input_ids\"]\n",
    "        attention_mask: list[list[int]] = inputs[\"attention_mask\"]\n",
    "        label_ids: list[list[int]] = labels[\"input_ids\"]\n",
    "        label_ids[label_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.tolist(),\n",
    "            \"attention_mask\": attention_mask.tolist(),\n",
    "            \"labels\": label_ids.tolist(),\n",
    "        }\n",
    "\n",
    "    def train_and_evaluate(\n",
    "        self,\n",
    "        dataset: DatasetDict = (\n",
    "            quest_set.select_splits([\"train\", \"val\"])\n",
    "            if ACTIVATE_FULL\n",
    "            else quest_subset.select_splits([\"train\", \"val\"])\n",
    "        ),\n",
    "        max_length: int = MAX_LENGTH,\n",
    "        learning_rate: int = LR_RATE,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        epochs: int = N_EPOCHS,\n",
    "        seed: int = SEED,\n",
    "        max_grad_norm: float = MAX_GRAD_NORM,\n",
    "        logging_steps: int = LOGGING_STEPS,\n",
    "        eval_steps: int = EVAL_STEPS,\n",
    "        warmup_steps: int = WARMUP_STEPS,\n",
    "        gradient_checkpointing: bool = GRADIENT_CHECKPOINTING,\n",
    "        load_best_model_at_end: bool = LOAD_BEST_MODEL_AT_END,\n",
    "        save_total_limit: int = SAVE_TOTAL_LIMIT,\n",
    "        eval_accumulation_steps: int = EVAL_ACCUMULATION_STEPS,\n",
    "        gradient_accumulation_steps: int = GRADIENT_ACCUMULATION_STEPS,\n",
    "        callbacks: list[TrainerCallback] = [\n",
    "            EarlyStoppingCallback(early_stopping_patience=2) if ACTIVATE_EVAL else None,\n",
    "            TensorBoardCallback() if ACTIVATE_TENSORBOARD else None,\n",
    "        ],\n",
    "        activate_fp16: bool = ACTIVATE_FP16,\n",
    "        activate_eval: bool = ACTIVATE_EVAL,\n",
    "        activate_save: bool = ACTIVATE_SAVE,\n",
    "        activate_logs: bool = ACTIVATE_LOGS,\n",
    "        activate_tensorboard: bool = ACTIVATE_TENSORBOARD,\n",
    "        activate_callbacks: bool = ACTIVATE_CALLBACKS,\n",
    "        output_dir: PathLike = get_target_dirpath(\"out\"),\n",
    "        logging_dir: PathLike = get_target_dirpath(\"logs\"),\n",
    "    ) -> TrainingMetrics:\n",
    "        # Ensure the training and validation sets\n",
    "        if not all(split in dataset for split in [\"train\", \"val\"]):\n",
    "            raise ValueError(\"DatasetDict must contain both 'train' and 'val' splits.\")\n",
    "\n",
    "        # Ensure the output and logging directories\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "        start_time: float\n",
    "        end_time: float\n",
    "        elapsed: float\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Set the padding token for the tokenizer\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        # Tokenize the dataset with `max_length` padding\n",
    "        print(f\"[TOKENIZE] {self.model_key} ({self.model_id})\")\n",
    "        start_time = time.time()\n",
    "        tokenized_data: Dataset = dataset.map(\n",
    "            QuestGenLLM.tokenize_dataset,\n",
    "            batched=True,\n",
    "            remove_columns=[\"prompt\", \"response\"],\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer, \"max_length\": max_length},\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Set the model padding token (from the tokenizer)\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        # Turn off `use_cache` if `gradient_checkpointing` is on\n",
    "        self.model.config.use_cache = not gradient_checkpointing\n",
    "\n",
    "        # Set up the training configurations\n",
    "        training_args: TrainingArguments = TrainingArguments(\n",
    "            output_dir=(Path(output_dir) / self.model_key),\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            log_level=(\"info\" if activate_logs else \"error\"),\n",
    "            logging_steps=logging_steps,\n",
    "            eval_steps=eval_steps,\n",
    "            eval_strategy=(\"epoch\" if activate_eval else \"no\"),\n",
    "            save_strategy=(\"epoch\" if activate_save else \"no\"),\n",
    "            logging_dir=(Path(logging_dir) / self.model_key),\n",
    "            save_total_limit=save_total_limit,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "            fp16=(self.fp16_available and activate_fp16),\n",
    "            load_best_model_at_end=load_best_model_at_end,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            seed=seed,\n",
    "            report_to=(\"tensorboard\" if activate_tensorboard else \"none\"),\n",
    "            label_names=[\"labels\"],\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            warmup_steps=warmup_steps,\n",
    "            logging_nan_inf_filter=True,\n",
    "            skip_memory_metrics=True,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            push_to_hub=False,\n",
    "            disable_tqdm=False,\n",
    "        )\n",
    "\n",
    "        # Set up the data collator for the model\n",
    "        data_collator: DataCollatorForLanguageModeling = (\n",
    "            DataCollatorForLanguageModeling(\n",
    "                self.tokenizer, mlm=False, return_tensors=\"pt\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Set up the callbacks for the trainer\n",
    "        trainer_callbacks: list[TrainerCallback] = list(\n",
    "            filter(lambda callback: callback is not None, callbacks)\n",
    "        )\n",
    "        if activate_callbacks:\n",
    "            trainer_callbacks.append(LossLoggerCallback(self.training_metrics))\n",
    "\n",
    "        # Prepare and run the trainer\n",
    "        trainer: Trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=tokenized_data[\"train\"],\n",
    "            eval_dataset=(tokenized_data[\"val\"] if activate_eval else None),\n",
    "            callbacks=trainer_callbacks,\n",
    "        )\n",
    "\n",
    "        print(f\"[FINETUNE] {self.model_key} ({self.model_id})\")\n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Save the model and tokenizer for later use\n",
    "        if activate_save:\n",
    "            trainer.save_model()\n",
    "            self.tokenizer.save_pretrained(save_directory=training_args.output_dir)\n",
    "\n",
    "        # Add to the training metrics map\n",
    "        TRAINING_METRICS[self.model_key] = self.training_metrics\n",
    "\n",
    "        return self.training_metrics\n",
    "\n",
    "    def generate_and_evaluate(\n",
    "        self,\n",
    "        dataset: DatasetDict = (\n",
    "            quest_set.select_splits([\"test\"])\n",
    "            if ACTIVATE_FULL\n",
    "            else quest_subset.select_splits([\"test\"])\n",
    "        ),\n",
    "        references: list[str] = (\n",
    "            quest_set.references if ACTIVATE_FULL else quest_subset.references\n",
    "        ),\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        seed: int = SEED,\n",
    "        max_length: int = MAX_LENGTH,\n",
    "        max_new_tokens: int = MAX_NEW_TOKEN,\n",
    "        num_return_sequences: int = NUM_RETURN_SEQUENCES,\n",
    "        temperature: float = TEMPERATURE,\n",
    "        top_p: float = TOP_P,\n",
    "        top_k: int = TOP_K,\n",
    "        do_sample: bool = True,\n",
    "        repetition_penalty: float = REPETITION_PENALTY,\n",
    "    ) -> GenerationMetrics:\n",
    "        # Ensure the testing set in the dataset\n",
    "        if \"test\" not in dataset:\n",
    "            raise ValueError(\"DatasetDict must contain a 'test' split.\")\n",
    "\n",
    "        input_model: PreTrainedModel = self.model.base_model.model\n",
    "        input_model.eval()  # Set the model to evaluation mode\n",
    "        self.tokenizer.padding_side = \"left\"  # Change the padding side to left\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        predictions: list[str] = []\n",
    "        test_split: Dataset = dataset[\"test\"]\n",
    "\n",
    "        print(f\"[GENERATE] {self.model_key} ({self.model_id})\")\n",
    "        start_time: float = time.time()\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_prompts: list[str] = test_split[i : i + batch_size][\"prompt\"]\n",
    "\n",
    "            # Tokenize the batched input prompts\n",
    "            tokenized_prompts: BatchEncoding = QuestGenLLM.perform_tokenization(\n",
    "                batch_prompts, self.tokenizer, max_length\n",
    "            ).to(input_model.device)\n",
    "\n",
    "            # Generate output tokens from the tokenized inputs\n",
    "            with torch.no_grad():\n",
    "                gen_tokens: GenerateOutput | LongTensor = input_model.generate(\n",
    "                    **tokenized_prompts,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_return_sequences=num_return_sequences,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    do_sample=do_sample,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                )\n",
    "\n",
    "            # Decode the generated tokens into response outputs\n",
    "            decoded: list[str] = self.tokenizer.batch_decode(\n",
    "                gen_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            predictions.extend(decoded)\n",
    "        end_time: float = time.time()\n",
    "        elapsed: float = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Repeat the references if more than one output sequences are generated\n",
    "        if num_return_sequences > 1:\n",
    "            references = [\n",
    "                ref for ref in references for _ in range(num_return_sequences)\n",
    "            ]\n",
    "\n",
    "        # Perform metric evaluation and add to the generation metrics map\n",
    "        self.compute_generation_metrics(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "            batch_size=batch_size,\n",
    "            seed=seed,\n",
    "        )\n",
    "        GENERATION_METRICS[self.model_key] = self.generation_metrics\n",
    "\n",
    "        return self.generation_metrics\n",
    "\n",
    "    def compute_generation_metrics(\n",
    "        self,\n",
    "        *,\n",
    "        predictions: list[str],\n",
    "        references: list[str],\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        seed: int = SEED,\n",
    "    ) -> None:\n",
    "        # Set the random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Set tokenizer padding back to right\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        print(f\"[EVALUATE] {self.model_key} ({self.model_id})\")\n",
    "        start_time: float = time.time()\n",
    "\n",
    "        # Load metrics from evaluate\n",
    "        bleu: EvaluationModule = evaluate.load(\"bleu\")\n",
    "        meteor: EvaluationModule = evaluate.load(\"meteor\")\n",
    "        bertscore: EvaluationModule = evaluate.load(\"bertscore\")\n",
    "\n",
    "        # Compute metric scores (BLEU, ROUGE, METEOR, BERTScore)\n",
    "        bleu_results: Optional[dict] = bleu.compute(\n",
    "            predictions=predictions, references=references\n",
    "        )\n",
    "        meteor_results: Optional[dict] = meteor.compute(\n",
    "            predictions=predictions, references=references\n",
    "        )\n",
    "        bert_results: Optional[dict] = bertscore.compute(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "            lang=\"en\",\n",
    "            batch_size=batch_size,\n",
    "            use_fast_tokenizer=True,\n",
    "        )\n",
    "\n",
    "        # Compute perplexity score (from eval losses)\n",
    "        perplexity_score: float = 0.0\n",
    "        if self.training_metrics and self.training_metrics.eval_losses:\n",
    "            avg_eval_loss: float = np.mean(self.training_metrics.eval_losses)\n",
    "            perplexity_score = math.exp(avg_eval_loss)\n",
    "\n",
    "        end_time: float = time.time()\n",
    "        elapsed: float = end_time - start_time\n",
    "        print(f\"[COMPLETE] Elapsed: {elapsed:.2f}s\\n\")\n",
    "\n",
    "        # Compile the results into the generation metrics\n",
    "        self.generation_metrics.perplexity = perplexity_score\n",
    "        self.generation_metrics.bleu = bleu_results.get(\"bleu\", 0.0)\n",
    "        self.generation_metrics.meteor = meteor_results.get(\"meteor\", 0.0)\n",
    "        self.generation_metrics.bertscore = {\n",
    "            \"precision\": float(np.mean(bert_results.get(\"precision\", [0.0]))),\n",
    "            \"recall\": float(np.mean(bert_results.get(\"recall\", [0.0]))),\n",
    "            \"f1\": float(np.mean(bert_results.get(\"f1\", [0.0]))),\n",
    "        }\n",
    "\n",
    "    def to_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"model_key\": self.model_key,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"device\": self.device,\n",
    "            \"dtype\": self.dtype,\n",
    "            \"vocab_size\": getattr(self.tokenizer, \"vocab_size\", \"unknown\"),\n",
    "            \"max_length\": getattr(self.tokenizer, \"model_max_length\", \"unknown\"),\n",
    "            \"model_type\": getattr(\n",
    "                getattr(self.model, \"config\", None), \"model_type\", \"unknown\"\n",
    "            ),\n",
    "            \"num_parameters\": self.model.num_parameters()\n",
    "            if hasattr(self.model, \"num_parameters\")\n",
    "            else \"N/A\",\n",
    "            \"fp16_available\": self.fp16_available,\n",
    "        }\n",
    "\n",
    "    def clear_cache(self, cache_dir: PathLike = get_cache_dirpath(\"models\")) -> None:\n",
    "        def remove_dir(dir_path: PathLike) -> None:\n",
    "            if os.path.exists(dir_path):\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"Cache directory '{dir_path}' removed.\")\n",
    "            else:\n",
    "                print(f\"No cache directory found at '{dir_path}'.\")\n",
    "\n",
    "        remove_dir(Path(cache_dir) / self.model_key)\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        dataset: QuestDataset = quest_set if ACTIVATE_FULL else quest_subset,\n",
    "        max_length: int = MAX_LENGTH,\n",
    "        learning_rate: int = LR_RATE,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        epochs: int = N_EPOCHS,\n",
    "        seed: int = SEED,\n",
    "        max_grad_norm: float = MAX_GRAD_NORM,\n",
    "        logging_steps: int = LOGGING_STEPS,\n",
    "        eval_steps: int = EVAL_STEPS,\n",
    "        warmup_steps: int = WARMUP_STEPS,\n",
    "        gradient_checkpointing: bool = GRADIENT_CHECKPOINTING,\n",
    "        load_best_model_at_end: bool = LOAD_BEST_MODEL_AT_END,\n",
    "        save_total_limit: int = SAVE_TOTAL_LIMIT,\n",
    "        eval_accumulation_steps: int = EVAL_ACCUMULATION_STEPS,\n",
    "        gradient_accumulation_steps: int = GRADIENT_ACCUMULATION_STEPS,\n",
    "        max_new_tokens: int = MAX_NEW_TOKEN,\n",
    "        num_return_sequences: int = NUM_RETURN_SEQUENCES,\n",
    "        temperature: float = TEMPERATURE,\n",
    "        top_p: float = TOP_P,\n",
    "        top_k: int = TOP_K,\n",
    "        do_sample: bool = True,\n",
    "        repetition_penalty: float = REPETITION_PENALTY,\n",
    "        callbacks: list[TrainerCallback] = [\n",
    "            EarlyStoppingCallback(early_stopping_patience=2) if ACTIVATE_EVAL else None,\n",
    "            TensorBoardCallback() if ACTIVATE_TENSORBOARD else None,\n",
    "        ],\n",
    "        activate_fp16: bool = ACTIVATE_FP16,\n",
    "        activate_eval: bool = ACTIVATE_EVAL,\n",
    "        activate_save: bool = ACTIVATE_SAVE,\n",
    "        activate_logs: bool = ACTIVATE_LOGS,\n",
    "        activate_tensorboard: bool = ACTIVATE_TENSORBOARD,\n",
    "        activate_callbacks: bool = ACTIVATE_CALLBACKS,\n",
    "        output_dir: PathLike = get_target_dirpath(\"out\"),\n",
    "        logging_dir: PathLike = get_target_dirpath(\"logs\"),\n",
    "        cache_dir: PathLike = get_cache_dirpath(\"models\"),\n",
    "        enable_train_and_evaluate: bool = TRAIN_AND_EVALUATE,\n",
    "        enable_generate_and_evaluate: bool = GENERATE_AND_EVALUATE,\n",
    "        enable_clear_cache: bool = CLEAR_CACHE,\n",
    "    ) -> None:\n",
    "        self.inspect()\n",
    "\n",
    "        if enable_train_and_evaluate:\n",
    "            self.train_and_evaluate(\n",
    "                dataset=dataset.select_splits([\"train\", \"val\"]),\n",
    "                max_length=max_length,\n",
    "                learning_rate=learning_rate,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                seed=seed,\n",
    "                max_grad_norm=max_grad_norm,\n",
    "                logging_steps=logging_steps,\n",
    "                eval_steps=eval_steps,\n",
    "                warmup_steps=warmup_steps,\n",
    "                gradient_checkpointing=gradient_checkpointing,\n",
    "                load_best_model_at_end=load_best_model_at_end,\n",
    "                save_total_limit=save_total_limit,\n",
    "                eval_accumulation_steps=eval_accumulation_steps,\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                callbacks=callbacks,\n",
    "                activate_fp16=activate_fp16,\n",
    "                activate_eval=activate_eval,\n",
    "                activate_save=activate_save,\n",
    "                activate_logs=activate_logs,\n",
    "                activate_tensorboard=activate_tensorboard,\n",
    "                activate_callbacks=activate_callbacks,\n",
    "                output_dir=output_dir,\n",
    "                logging_dir=logging_dir,\n",
    "            )\n",
    "            display(self.training_metrics)\n",
    "\n",
    "        if enable_generate_and_evaluate:\n",
    "            self.generate_and_evaluate(\n",
    "                dataset=dataset.select_splits([\"test\"]),\n",
    "                references=dataset.references,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                do_sample=do_sample,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "            )\n",
    "            display(self.generation_metrics)\n",
    "\n",
    "        if enable_clear_cache:\n",
    "            self.clear_cache(cache_dir=cache_dir)\n",
    "\n",
    "    def print_model_information(self) -> None:\n",
    "        print(json.dumps(self.to_dict(), indent=2))\n",
    "\n",
    "    def inspect(self) -> None:\n",
    "        print(f\"{self.tokenizer}\\n\\n{self.model}\\n\\n{self.model.config}\")\n",
    "        print(\n",
    "            f\"Token Type                        | Value\\n\"\n",
    "            f\"----------------------------------+-------------------\\n\"\n",
    "            f\"Padding Token [PAD]               | {self.tokenizer.pad_token}\\n\"\n",
    "            f\"Beginning of Sentence Token [BOS] | {self.tokenizer.bos_token}\\n\"\n",
    "            f\"End of Sentence Token [EOS]       | {self.tokenizer.eos_token}\\n\"\n",
    "            f\"Unknown Token [UNK]               | {self.tokenizer.unk_token}\\n\"\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.model_key} ({self.model_id})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the GPT-2 Base model with the quest data\n",
    "gpt2_base: QuestGenLLM = QuestGenLLM.from_pretrained(\"gpt2\")\n",
    "gpt2_base.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the GPT-2 Medium model with the quest data\n",
    "gpt2_medium: QuestGenLLM = QuestGenLLM.from_pretrained(\"gpt2-medium\")\n",
    "gpt2_medium.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd11881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the GPT-2 Large model with the quest data\n",
    "gpt2_large: QuestGenLLM = QuestGenLLM.from_pretrained(\"gpt2-large\")\n",
    "gpt2_large.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bda9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Llama 3.2 model with the quest data\n",
    "llama32: QuestGenLLM = QuestGenLLM.from_pretrained(\"llama-3.2-1b-instruct\")\n",
    "llama32.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the TinyLlama model with the quest data\n",
    "tinyllama: QuestGenLLM = QuestGenLLM.from_pretrained(\"tinyllama-1.1b-chat\")\n",
    "tinyllama.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(\n",
    "    name: str,\n",
    "    metrics: dict[str, GenerationMetrics | TrainingMetrics],\n",
    "    output_dir: PathLike = get_target_dirpath(\"out\"),\n",
    ") -> None:\n",
    "    metrics_dict: dict[str, dict[str, list[int | float]]] = {\n",
    "        k: v.to_dict() if isinstance(v, TrainingMetrics) else None\n",
    "        for k, v in metrics.items()\n",
    "    }\n",
    "\n",
    "    json_file_path: Path = Path(output_dir) / f\"{name}.json\"\n",
    "    with open(json_file_path, \"w\") as json_writer:\n",
    "        json.dump(metrics_dict, json_writer, indent=2)\n",
    "\n",
    "    print(f\"Saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics for future evaluations\n",
    "save_metrics(\"training_metrics\", TRAINING_METRICS)\n",
    "save_metrics(\"generation_metrics\", GENERATION_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944b04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
