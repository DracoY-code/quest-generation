\clearpage

\chapter{Introduction}

In this chapter, we present an overview of the foundational concepts, motivations, and
methodological approaches underlying the development of a procedural quest generation
(PQG) system for role-playing games (RPGs) using large language models (LLMs). The
field of procedural content generation (PCG) has been a long-standing area of interest in
game design, offering a scalable and adaptive means of creating game elements such as
maps, levels, and narratives.

The process of procedural content generation involves algorithmically creating game
content—such as environments, items, characters, and narratives—through predefined
computational methods, rather than relying solely on manual authoring by human designers.
The central aim of PCG is to automate aspects of game content creation in a
way that reduces development time and labor while increasing variability, replayability,
and scalability.

Procedural generation techniques are typically rule-based or stochastic in nature, incorporating
elements of randomness, heuristics, grammar systems, or machine learning to
ensure content diversity and structural coherence. Depending on the design goals, PCG
systems can be fully autonomous or require designer-guided parameters to maintain thematic
and gameplay consistency. These systems are particularly useful in large-scale or
open-world games, where the manual creation of unique assets for every player experience
is impractical.

The history of procedural content generation spans both digital and analog gaming
traditions. Early digital games such as \textit{Rogue} (1980)~\cite{rogue1980} pioneered procedurally generated
dungeons, laying the foundation for the "roguelike" genre and demonstrating how
algorithmic systems can create replayable gameplay experiences. On the analog side,
table-top game systems like \textit{Dungeons \& Dragons}~\cite{dungeonsanddragons} introduced rule-driven generation of
encounters and storylines, influencing digital design philosophies. Modern examples such
as \textit{Minecraft}~\cite{minecraft} employ PCG to generate endless, explorable terrain, while \textit{AI Dungeon}~\cite{ai-dungeon}
uses language models to dynamically create interactive narrative experiences. These
systems highlight the versatility of PCG in generating spatial, mechanical, and narrative
content across genres.

Game content generated through procedural methods covers a broad spectrum, including
spatial elements like terrain, maps, and dungeons (as seen in \textit{Rogue}~\cite{rogue1980} and
\textit{Minecraft}~\cite{minecraft}), mechanical components such as enemy behaviors and item properties, and
narrative structures involving dialogue trees, quests, and branching storylines. Early PCG
systems often produced repetitive or structurally simple outputs focused primarily on level
or map generation. However, recent advances in computational techniques—particularly
the integration of artificial intelligence (AI) and natural language processing (NLP)—have
greatly expanded the expressive power of PCG. Modern methods now enable the generation
of complex, human-like storytelling and adaptive game experiences that respond
dynamically to player interactions, thereby enhancing immersion and replayability.

However, quest generation, in general, presents a unique challenge due to the structured
yet narrative-rich nature of quests that must align with both gameplay mechanics
and storytelling goals.

Traditional approaches to quest generation have relied heavily on handcrafted templates,
which often lack the linguistic fluency and variability required for immersive player
experiences. Recent advances in NLP, particularly with transformer-based architectures
such as GPT and LLaMA, have demonstrated exceptional capabilities in generative tasks,
including dialogue modeling, summarization, and storytelling~\cite{vaswani2017attention,brown2020language}. These models are pre-trained
on large corpora and can be fine-tuned with domain-specific datasets to generate
coherent and context-aware narrative content.

To address the specific task of quest generation, this project proposes a hybrid methodology
combining quantized language models with parameter-efficient fine-tuning (PEFT)
via LoRA adapters~\cite{peft,hu2022lora,dettmers2023qlora}. By leveraging dialogue and quest corpora extracted from well-established
RPGs such as the \textit{Fallout} series~\cite{fallout1,fallout2}, the \textit{Baldur's Gate} series~\cite{baldursgate,baldursgate2shadowsofamn}, and
the \textit{Elder Scrolls} series~\cite{theelderscrollsivoblivion,theelderscrollsvskyrim}, the system aims to generate semantically valid, context-sensitive
quests using a structured prompt format. The resulting system is designed to be
both memory-efficient and scalable for integration in real-time or offline game pipelines.

The remainder of this chapter elaborates on the academic and technical background
of the topic, identifies existing research gaps, formulates the problem being addressed,
outlines the specific research objectives, and introduces the proposed solution in detail.

\section{Background}

RPGs have long relied on meticulously authored content to deliver immersive narratives
and interactive experiences. Traditionally, the creation of such content—especially quests
and dialogue—has required extensive manual effort from writers and designers. This
manual process limits scalability and often constrains replayability. To address these
issues, PCG techniques have been explored to automate the creation of game elements.
While PCG has been successfully used for generating maps, characters, terrains, and
items, narrative generation remains a relatively underdeveloped area due to its dependence
on linguistic coherence, contextual consistency, and player agency~\cite{togelius2013procedural}.

The integration of natural language generation (NLG) into PCG pipelines has opened
new possibilities for automating quest creation that balances narrative richness with structural
logic. Early procedural narrative systems often relied on handcrafted templates or
symbolic AI techniques, such as story grammars~\cite{black1979evaluation} and planning-based approaches~\cite{riedl2013interactive}.
Some systems also model quests as directed graphs or trees, encoding dependencies and
branching logic to support replayable and nonlinear narratives~\cite{hendrikx2013procedural}. However, these methods
struggled to produce diverse, semantically rich, or context-aware quests, often resulting
in repetitive or mechanically constrained narratives.

Recent advances in large-scale transformer-based language models—such as GPT~\cite{brown2020language}
and LLaMA~\cite{touvron2023llama}—have significantly altered this landscape. These models, pre-trained
on massive text corpora, are capable of generating fluent, coherent, and contextually
relevant textual outputs that approximate human writing styles across diverse domains.
This enables the procedural generation of branching quests, dialogue trees, and interactive
storytelling sequences with greater variability and linguistic naturalness. For example, \textit{AI Dungeon}~\cite{ai-dungeon}
leverages GPT-based models to create open-ended, player-driven narratives
that adapt dynamically to input prompts.

To effectively apply these models in games, structured datasets that include annotated
dialogue and quest sequences are essential. Extracted corpora from games such as
\textit{Fallout}~\cite{fallout1,fallout2}, \textit{Baldur's Gate}~\cite{baldursgate,baldursgate2shadowsofamn}, and \textit{The Elder Scrolls}~\cite{theelderscrollsivoblivion,theelderscrollsvskyrim} provide valuable
resources for training these systems. These datasets allow the models to learn not only
the language used in quests but also their structure, tone, and logical flow.

Additionally, the ability to generate semantically relevant and structurally sound
quests can significantly enhance dynamic storytelling and replayability, thus paving the
way for adaptive and personalized game experiences.

\section{Research Gaps}

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[
    node distance=1cm,
    main/.style={
      circle, draw, thick, fill=gray!30, minimum size=1cm,
      font=\scriptsize\bfseries, align=center
    },
    cluster/.style={
      rectangle, draw, rounded corners, fill=#1!40, minimum width=3cm, minimum height=1cm,
      font=\scriptsize, text width=2.5cm, align=center, inner sep=6pt, drop shadow
    },
    subgap/.style={
      rectangle, draw, rounded corners, fill=#1!20, minimum width=3cm, minimum height=1cm,
      font=\scriptsize, text width=2.5cm, align=center, inner sep=6pt, drop shadow
    },
    arrow/.style={-Stealth, thick}
  ]
    \node[main] (root) {Research Gaps in \\Procedural Quest \\Generation};
    \node[cluster=blue, above right=0.5cm and 1cm of root] (narrative) {Narrative \& Control};
    \node[cluster=green, above left=0.5cm and 1cm of root] (data) {Data \& Training};
    \node[cluster=orange, below left=0.5cm and 1cm of root] (evaluation) {Evaluation};
    \node[cluster=purple, below right=0.5cm and 1cm of root] (integration) {Integration \& Adaptivity};

    \node[subgap=blue, above right=1cm and -1cm of narrative] (planning) {Lack of structured narrative planning causing incoherent quest flows};
    \node[subgap=blue, above right=1cm and -5cm of narrative] (semantic) {Semantic inconsistencies from missing world state tracking};

    \node[subgap=green, above left=1cm and -1cm of data] (dataset) {Scarcity of domain-specific, annotated RPG quest datasets};
    \node[subgap=green, above left=1cm and -5cm of data] (scalability) {Challenges in scaling and efficient fine-tuning of large models};

    \node[subgap=orange, below left=1cm and -2cm of evaluation] (metrics) {Lack of robust, domain-specific narrative evaluation metrics};

    \node[subgap=purple, below right=1cm and -5cm of integration] (realtime) {Difficulties in real-time integration with dynamic game states};
    \node[subgap=purple, below right=1cm and -1cm of integration] (personalization) {Limited research on adaptive, personalized quest generation};

    \draw[arrow] (root) -- (narrative);
    \draw[arrow] (root) -- (data);
    \draw[arrow] (root) -- (evaluation);
    \draw[arrow] (root) -- (integration);

    \draw[arrow] (narrative) -- (planning);
    \draw[arrow] (narrative) -- (semantic);

    \draw[arrow] (data) -- (dataset);
    \draw[arrow] (data) -- (scalability);

    \draw[arrow] (evaluation) -- (metrics);

    \draw[arrow] (integration) -- (realtime);
    \draw[arrow] (integration) -- (personalization);
  \end{tikzpicture}
  \caption{Radial diagram showing key thematic research gaps in procedural quest generation with LLMs, highlighting challenges in narrative control, datasets, coherence, evaluation, integration, scalability, and personalization.}
  \label{fig:research-gaps}
\end{figure}

While the use of LLMs for PQG presents exciting possibilities, a number of research
gaps hinder the development of reliable, scalable, and context-sensitive systems. These
challenges are rooted in the intersection of natural language generation, narrative theory,
and real-time game design, and must be addressed to advance the field toward more
adaptive and immersive game experiences (see Figure~\ref{fig:research-gaps}).

One of the most fundamental limitations is the lack of structured narrative control in
existing LLMs. Although models like GPT and LLaMA demonstrate strong surface-level
fluency, they are inherently autoregressive and lack high-level narrative planning mechanisms~\cite{bommasani2021opportunities}.
This leads to inconsistencies in quest structure—such as missing subgoals,
conflicting motivations, or incoherent resolution paths. Unlike manually authored quests,
which follow well-defined narrative arcs (e.g., setup\textrightarrow complication\textrightarrow resolution), LLM outputs
often lose track of long-range dependencies across quest stages. Some hybrid systems
have introduced graph-based planning or symbolic scaffolds (e.g., plot graphs, quest
trees), but few have been effectively integrated with LLMs in a seamless and generalizable
way~\cite{ammanabrolu2019toward,ashby2023personalized}.

A second challenge involves the limited availability of domain-specific fine-tuning
resources. While general-purpose language models are trained on massive, heterogeneous
corpora, they lack detailed exposure to the linguistic patterns, structural tropes, and character
archetypes specific to RPGs. The adopted hypothesis from related works that the
dataset used to pretrain GPT-2 does not have enough number of quest examples to generate
high-quality quest results without further fine-tuning with specialized datasets~\cite{vartinen2022generating,van2021fine}.
Fine-tuning on curated corpora from various games can help models learn quest-specific
syntax and semantic flow. However, such datasets are rarely available in clean,
annotated formats that are aligned with narrative objectives or quest semantics. The
absence of open-source, standardized RPG quest datasets remains a major bottleneck for
reproducible research.

Even after fine-tuning, LLMs often exhibit semantic inconsistencies and logical drift \cite{luo2023empirical,huang2025survey}.
These issues commonly manifest as contradictions between non-player character
(NPC) dialogue and quest objectives, incoherent character behaviors, or violations of the
game world's internal logic. Such breakdowns largely arise from the absence of explicit
state tracking or grounding mechanisms in these models. Since LLMs lack an inherent understanding
of world state, quest dependencies, and previously generated content, they are
prone to disrupting narrative causality and coherence. Although integrating LLMs with
structured memory systems, finite state machines, or symbolic representations of world
models holds promise, such hybrid approaches remain relatively underexplored within the
domain of PCG, particularly for narrative tasks.

Another significant research gap exists in the evaluation of procedurally generated
narratives. Unlike spatial or mechanical content—such as levels or terrains—that can be
assessed using objective metrics (e.g., pathfinding solvability or object density), narrative
content is inherently subjective and difficult to quantify. Current approaches often rely
on human evaluations, expert annotations, or engagement proxies, which are costly, non-scalable,
and lack standardization. Conventional text generation metrics like BLEU~\cite{papineni02bleu},
and METEOR~\cite{banarjee2005} offer limited utility in this domain, as they primarily measure lexical
similarity rather than deeper narrative attributes such as coherence, causality, or character
consistency~\cite{celikyilmaz2020evaluation}.

Recent advances in natural language generation propose more nuanced evaluation
methods, including structure-aware metrics, discourse modeling, and embedding-based
similarity measures such as BERTScore~\cite{zhang2019bertscore}. However, these techniques remain underutilized
in the context of interactive storytelling and quest generation. Key narrative
dimensions—such as world-state alignment, progression logic, and character intent—are
often ignored by current metrics. This highlights the pressing need for domain-specific
evaluation frameworks that can robustly assess both the semantic clarity and narrative
integrity of procedurally generated quests.

The integration of quest generation systems into real-time, interactive game environments
presents a range of practical and technical challenges. Most LLM-based quest
generation experiments are conducted offline or in isolated testing environments, without
the constraints of live player interaction. For games with dynamic states, generated quests
must work in tandem with the ever-evolving game state that encompasses various factors
such as available inventory, completed objectives, and player positioning. This signifies
the need for a bidirectional interface between the generative model and the underlying
game engine. The development of such tightly coupled systems poses ongoing challenges
in both computational design and real-time narrative integration.

The issue of scalability and performance is equally important. Large-scale models
are computationally expensive and often unsuitable for real-time generation on consumer
hardware. While techniques like model quantization and PEFT (e.g., LoRA adapters)
offer promising trade-offs between performance and quality~\cite{hu2022lora}, their use in long-form,
context-sensitive narrative generation has not been extensively benchmarked. The question
regarding the fidelity and controllability of such compressed models in creative applications
remains to be answered.

Finally, limited research has been conducted on personalization and adaptive quest
generation. Existing systems typically produce static quest content that fails to account
for a player's prior actions, decisions, or preferences. Developing adaptive systems capable
of tailoring quests to individual player profiles holds potential for enhancing engagement
and narrative richness. However, building such a system would require robust user modeling
and dynamic content adaptation mechanisms.

\section{Problem Statement}

The design and delivery of compelling quest content in RPGs represent a significant
narrative and technical undertaking. While manually authored quests allow for tightly
controlled storytelling, they come with scalability limitations, high production costs, and
limited replayability. PCG offers a pathway toward automating narrative design, yet
current capabilities fall short when it comes to generating quests that are both narratively
rich and mechanically coherent.

Automated quest generation in RPGs presents a promising but challenging application
of NLG. While recent advances in LLMs such as GPT, LLaMA, and their variants have
shown remarkable fluency in open-ended text generation, their effectiveness in structured
narrative tasks like quest description generation remains insufficiently understood. In particular,
the trade-offs between model size, resource requirements, and generation quality
are underexplored, especially for constrained computing environments where fine-tuning
and inference must be optimized for low memory availability.

This study seeks to fill the gap in systematic, empirical evaluations of state-of-the-art
LLMs for the specific task of generating RPG quest descriptions. While existing research
has primarily relied on subjective assessments or in-game testing, there remains a lack
of standardized and replicable benchmarking frameworks. By employing established
NLG metrics—such as BLEU, METEOR, BERTScore—and model perplexity, this study
enables a multi-dimensional evaluation of model outputs. Such an approach provides a
more nuanced understanding of each model's narrative fluency, semantic alignment, and
overall linguistic competence within the domain of quest generation.

Furthermore, fine-tuning large models in typical game development pipelines presents
practical challenges, which is emphasized especially under limited hardware constraints.
This research adopts a PEFT strategy using LoRA~\cite{hu2022lora,peft} and explores quantized model
variants to reduce memory usage while maintaining generation quality. The effectiveness
of these optimizations in real-world, resource-limited scenarios remains an important
consideration for scalable deployment of LLM-based quest generators.

Thus, this study explores the problem of evaluating large language models in their
capacity to generate coherent and semantically rich quest descriptions, with a focus on
standardized NLG metrics such as BLEU, METEOR, BERTScore, and perplexity. In
parallel, it investigates how model quantization and PEFT techniques influence both the
narrative quality and computational efficiency of these models when deployed in resource-constrained
environments.

\section{Research Objectives}

\begin{table}[t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.3}
  \begin{tabularx}{0.95\textwidth}{
    >{\centering\arraybackslash}p{1cm}
    >{\raggedright\arraybackslash}X
    >{\raggedright\arraybackslash}X
  }
  \toprule
  \textbf{RQ\#} & \textbf{Research Question} & \textbf{Objective} \\
  \midrule
  RQ1
    & How can existing quest and dialogue datasets be structured to train LLMs effectively?
    & To design a prompt-aligned, structured dataset derived from classic RPGs to train generative models. \\
  RQ2
    & What are the trade-offs in using quantized LLMs for quest generation?
    & To evaluate performance, memory efficiency, and generation quality of quantized models. \\
  RQ3
    & How effective is parameter-efficient fine-tuning (LoRA) for adapting LLMs to domain-specific tasks?
    & To implement LoRA adapters and assess their contribution to generation quality and training efficiency. \\
  RQ4
    & What metrics can best evaluate the semantic validity and narrative coherence of generated quests?
    & To apply both automatic (BLEU, METEOR, BERTScore, perplexity) and qualitative evaluation techniques. \\
  RQ5
    & Can a low-resource pipeline produce results comparable to full fine-tuning approaches?
    & To validate that the proposed system balances efficiency and output quality in constrained environments. \\
  \bottomrule
  \end{tabularx}
  \caption{Mapping of research questions to corresponding thesis objectives}
  \label{table:research_questions}
\end{table}

The primary aim of this project is to explore the effectiveness of LLMs in generating contextually
coherent and semantically valid quests for RPGs. Given the complex narrative
and structural demands of quests, this project adopts a methodologically layered approach
involving dataset construction, model quantization, and parameter-efficient fine-tuning.
The research is guided by key questions that address both the technical implementation
and the qualitative impact of the generated quests. The central research questions and
their corresponding objectives are listed in Table~\ref{table:research_questions}.

\section{Proposed Solution}

\begin{table}[t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.3}
  \begin{tabularx}{0.95\textwidth}{
    >{\raggedright\arraybackslash}p{5cm}
    >{\raggedright\arraybackslash}X
  }
    \toprule
    \textbf{Scope Aspect} & \textbf{Description} \\
    \midrule
    Dataset Construction
      & Focused on structuring quest and dialogue data extracted from a select set of classic RPGs using prompt-aligned, XML-like annotations. \\
    Modeling Approach
      & Applies quantized causal LLMs with parameter-efficient fine-tuning (LoRA) to enable low-resource adaptation to the domain. \\
    Task Objective
      & Targets single-instance procedural quest generation with coherent tasks and minimal dependencies across sessions. \\
    Evaluation Strategy
      & Combines automated metrics (BLEU, METEOR, BERTScore, perplexity) with human-assessed attributes (goal alignment, coherence, diversity). \\
    Application Domain
      & Tailored toward fantasy-themed role-playing games with linear or semi-branching quest structures. \\
    \bottomrule
  \end{tabularx}
  \caption{Scope of the PQG system, including key features and constraints}
  \label{table:scope}
\end{table}

\begin{table}[t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.3}
  \begin{tabularx}{0.95\textwidth}{
    >{\raggedright\arraybackslash}p{5cm}
    >{\raggedright\arraybackslash}X
  }
    \toprule
    \textbf{Limitation} & \textbf{Description} \\
    \midrule
    Dataset Scope
      & The dataset is restricted to a small number of RPGs, i.e., 8, which may limit genre diversity and model generalizability. \\
    Model Capacity
      & Fine-tuning is performed on quantized LLMs, potentially reducing expressiveness compared to full-precision, high-parameter models. \\
    Dialog Context Management
      & The system does not track persistent dialogue state, leading to isolated quest generations without long-term context awareness. \\
    Narrative Depth Evaluation
      & Evaluation focuses on surface-level lexical similarity and short-range coherence, omitting in-depth narrative or plot arc consistency. \\
    Genre Generalization
      & The model and prompt format are optimized for fantasy RPGs and require more adaptation for sci-fi, modern, or non-narrative games. \\
    \bottomrule
  \end{tabularx}
  \caption{Summary of key limitations observed in the proposed PQG approach}
  \label{table:limitations}
\end{table}

This study proposes a modular and resource-conscious system for PQG using quantized
LLMs augmented with PEFT techniques. The solution is designed to function within
the constraints of limited computational resources while ensuring semantic coherence and
domain relevance in the generated quest descriptions.

At its core, the system employs causal LLMs (e.g., GPT- or LLaMA-based architectures)
that have been quantized to 4-bit or 8-bit precision to reduce memory overhead and
enable deployment on modest hardware. These models are further adapted to the task domain
using LoRA-based fine-tuning, which allows for task-specific training with minimal
updates to the model's core parameters. This dual strategy of quantization and PEFT
balances the trade-off between efficiency and performance, particularly in low-resource
environments.

To ensure alignment with RPG-specific narrative structures, the input data is curated
from a domain-relevant corpus of classic RPGs. The dataset is preprocessed into an XML-like
schema to reinforce consistent structure during generation and facilitate prompt-based
conditioning. This schema allows the model to learn not only the linguistic form
of quest descriptions but also their functional components, such as goals, rewards, and
preconditions.

The quest generation task is framed as a single-instance process, wherein each quest
is produced in isolation, i.e., without maintaining dialogue history or tracking long-term
narrative dependencies. This design choice streamlines the modeling requirements while
offering a stable and reproducible setting for model-to-model comparison.

The evaluation framework adopts a hybrid approach, combining automatic text generation
metrics with human judgment. Quantitative assessment relies on widely used NLG
metrics, including BLEU, METEOR, BERTScore, and model perplexity, which collectively
evaluate lexical overlap, semantic fidelity, and fluency. These are complemented by
qualitative human evaluations focused on narrative coherence, goal relevance, and stylistic
diversity of the generated quests.

A summary of the methodological components is provided in Table~\ref{table:scope}, while Table~\ref{table:limitations}
outlines the known constraints of the implementation. Together, these illustrate both the
practicality and the boundaries of the proposed system.

Ultimately, this solution offers a replicable and scalable framework for evaluating the
narrative capabilities of LLMs in a focused gaming context. By combining lightweight
adaptation techniques with structured data and robust evaluation, the system contributes
a baseline for future research in quest generation and narrative modeling.

\newpage
