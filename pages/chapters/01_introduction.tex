\clearpage

\chapter{Introduction}

In this chapter, we present an overview of the foundational concepts, motivations, and
methodological approaches underlying the development of a procedural quest generation
(PQG) system for role-playing games (RPGs) using large language models (LLMs). The
field of procedural content generation (PCG) has been a long-standing area of interest in
game design, offering a scalable and adaptive means of creating game elements such as
maps, levels, and narratives.

The process of PCG involves algorithmically creating game content—such as environments,
items, characters, and narratives—through predefined computational methods,
rather than relying solely on manual authoring by human designers. The central aim of
PCG is to automate aspects of game content creation in a way that reduces development
time and labor while increasing variability, replayability, and scalability.

Procedural generation techniques are typically rule-based or stochastic in nature, incorporating
elements of randomness, heuristics, grammar systems, or machine learning to
ensure content diversity and structural coherence. Depending on the design goals, PCG
systems can be fully autonomous or require designer-guided parameters to maintain thematic
and gameplay consistency. These systems are particularly useful in large-scale or
open-world games, where the manual creation of unique assets for every player experience
is impractical.

The history of PCG spans both digital and analog gaming traditions. Early digital
games such as \textit{Rogue} (1980)~\cite{rogue1980} pioneered procedurally generated dungeons, laying the
foundation for the "roguelike" genre and demonstrating how algorithmic systems can create
replayable gameplay experiences. On the analog side, table-top game systems like
\textit{Dungeons \& Dragons}~\cite{dungeonsanddragons} introduced rule-driven generation of encounters and storylines,
influencing digital design philosophies. Modern examples such as \textit{Minecraft}~\cite{minecraft} employ
PCG to generate endless, explorable terrain, while \textit{AI Dungeon}~\cite{ai-dungeon} uses language models
to dynamically create interactive narrative experiences. These systems highlight the
versatility of PCG in generating spatial, mechanical, and narrative content across genres.

Game content generated through procedural methods covers a broad spectrum, including
spatial elements like terrain, maps, and dungeons (as seen in \textit{Rogue}~\cite{rogue1980} and
\textit{Minecraft}~\cite{minecraft}), mechanical components such as enemy behaviors and item properties, and
narrative structures involving dialogue trees, quests, and branching storylines. Early PCG
systems often produced repetitive or structurally simple outputs focused primarily on level
or map generation. However, recent advances in computational techniques—particularly
the integration of artificial intelligence (AI) and natural language processing (NLP)—have
greatly expanded the expressive power of PCG. Modern methods now enable the generation
of complex, human-like storytelling and adaptive game experiences that respond
dynamically to player interactions, thereby enhancing immersion and replayability.

However, quest generation, in general, presents a unique challenge due to the structured
yet narrative-rich nature of quests that must align with both gameplay mechanics
and storytelling goals.

Traditional approaches to quest generation have relied heavily on handcrafted templates,
which often lack the linguistic fluency and variability required for immersive player
experiences. Recent advances in NLP, particularly with transformer-based architectures
such as GPT and LLaMA, have demonstrated exceptional capabilities in generative tasks,
including dialogue modeling, summarization, and storytelling~\cite{vaswani2017attention,brown2020language}. These models are pre-trained
on large corpora and can be fine-tuned with domain-specific datasets to generate
coherent and context-aware narrative content.

To address the specific task of quest generation, this project proposes a hybrid methodology
combining quantized language models with parameter-efficient fine-tuning (PEFT)
via LoRA adapters~\cite{peft,hu2022lora,dettmers2023qlora}. By leveraging dialogue and quest corpora extracted from well-established
RPGs such as the \textit{Fallout} series~\cite{fallout1,fallout2}, the \textit{Baldur's Gate} series~\cite{baldursgate,baldursgate2shadowsofamn}, and
the \textit{Elder Scrolls} series~\cite{theelderscrollsivoblivion,theelderscrollsvskyrim}, the system aims to generate semantically valid, context-sensitive
quests using a structured prompt format. The resulting system is designed to be
both memory-efficient and scalable for integration in real-time or offline game pipelines.

\section{Background}

RPGs have long relied on meticulously authored content to deliver immersive narratives
and interactive experiences. Traditionally, the creation of such content—especially quests
and dialogue—has required extensive manual effort from writers and designers. This
manual process limits scalability and often constrains replayability. To address these
issues, PCG techniques have been explored to automate the creation of game elements.
While PCG has been successfully used for generating maps, characters, terrains, and
items, narrative generation remains a relatively underdeveloped area due to its dependence
on linguistic coherence, contextual consistency, and player agency~\cite{togelius2013procedural}.

The integration of natural language generation (NLG) into PCG pipelines has opened
new possibilities for automating quest creation that balances narrative richness with structural
logic. Early procedural narrative systems often relied on handcrafted templates or
symbolic AI techniques, such as story grammars~\cite{black1979evaluation} and planning-based approaches~\cite{riedl2013interactive}.
Some systems also model quests as directed graphs or trees, encoding dependencies and
branching logic to support replayable and nonlinear narratives~\cite{hendrikx2013procedural}. However, these methods
struggled to produce diverse, semantically rich, or context-aware quests, often resulting
in repetitive or mechanically constrained narratives.

Recent advances in large-scale transformer-based language models—such as GPT~\cite{brown2020language}
and LLaMA~\cite{touvron2023llama}—have significantly altered this landscape. These models, pre-trained
on massive text corpora, are capable of generating fluent, coherent, and contextually
relevant textual outputs that approximate human writing styles across diverse domains.
This enables the procedural generation of branching quests, dialogue trees, and interactive
storytelling sequences with greater variability and linguistic naturalness. For example, \textit{AI Dungeon}~\cite{ai-dungeon}
leverages GPT-based models to create open-ended, player-driven narratives
that adapt dynamically to input prompts.

To effectively apply these models in games, structured datasets that include annotated
dialogue and quest sequences are essential. Extracted corpora from games such as
\textit{Fallout}~\cite{fallout1,fallout2}, \textit{Baldur's Gate}~\cite{baldursgate,baldursgate2shadowsofamn}, and \textit{The Elder Scrolls}~\cite{theelderscrollsivoblivion,theelderscrollsvskyrim} provide valuable
resources for training these systems. These datasets allow the models to learn not only
the language used in quests but also their structure, tone, and logical flow.

Additionally, the ability to generate semantically relevant and structurally sound
quests can significantly enhance dynamic storytelling and replayability, thus paving the
way for adaptive and personalized game experiences.

\section{Research Gaps}

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[
    node distance=1cm,
    main/.style={
      circle, draw, thick, fill=gray!30, minimum size=1cm,
      font=\scriptsize\bfseries, align=center
    },
    cluster/.style={
      rectangle, draw, rounded corners, fill=#1!40, minimum width=3cm, minimum height=1cm,
      font=\scriptsize, text width=2.5cm, align=center, inner sep=6pt, drop shadow
    },
    subgap/.style={
      rectangle, draw, rounded corners, fill=#1!20, minimum width=3cm, minimum height=1cm,
      font=\scriptsize, text width=2.5cm, align=center, inner sep=6pt, drop shadow
    },
    arrow/.style={-Stealth, thick}
  ]
    \node[main] (root) {Research Gaps in \\Procedural Quest \\Generation};
    \node[cluster=blue, above right=0.5cm and 1cm of root] (narrative) {Narrative \& Control};
    \node[cluster=green, above left=0.5cm and 1cm of root] (data) {Data \& Training};
    \node[cluster=orange, below left=0.5cm and 1cm of root] (evaluation) {Evaluation};
    \node[cluster=purple, below right=0.5cm and 1cm of root] (integration) {Integration \& Adaptivity};

    \node[subgap=blue, above right=1cm and -1cm of narrative] (planning) {Lack of structured narrative planning causing incoherent quest flows};
    \node[subgap=blue, above right=1cm and -5cm of narrative] (semantic) {Semantic inconsistencies from missing world state tracking};

    \node[subgap=green, above left=1cm and -1cm of data] (dataset) {Scarcity of domain-specific, annotated RPG quest datasets};
    \node[subgap=green, above left=1cm and -5cm of data] (scalability) {Challenges in scaling and efficient fine-tuning of large models};

    \node[subgap=orange, below left=1cm and -2cm of evaluation] (metrics) {Lack of robust, domain-specific narrative evaluation metrics};

    \node[subgap=purple, below right=1cm and -5cm of integration] (realtime) {Difficulties in real-time integration with dynamic game states};
    \node[subgap=purple, below right=1cm and -1cm of integration] (personalization) {Limited research on adaptive, personalized quest generation};

    \draw[arrow] (root) -- (narrative);
    \draw[arrow] (root) -- (data);
    \draw[arrow] (root) -- (evaluation);
    \draw[arrow] (root) -- (integration);

    \draw[arrow] (narrative) -- (planning);
    \draw[arrow] (narrative) -- (semantic);

    \draw[arrow] (data) -- (dataset);
    \draw[arrow] (data) -- (scalability);

    \draw[arrow] (evaluation) -- (metrics);

    \draw[arrow] (integration) -- (realtime);
    \draw[arrow] (integration) -- (personalization);
  \end{tikzpicture}
  \caption{Radial diagram showing key thematic research gaps in procedural quest generation with LLMs, highlighting challenges in narrative control, datasets, coherence, evaluation, integration, scalability, and personalization.}
  \label{fig:research-gaps}
\end{figure}

While the use of LLMs for PQG presents exciting possibilities, a number of research
gaps hinder the development of reliable, scalable, and context-sensitive systems. These
challenges are rooted in the intersection of natural language generation, narrative theory,
and real-time game design, and must be addressed to advance the field toward more
adaptive and immersive game experiences (see Figure~\ref{fig:research-gaps}).

One of the most fundamental limitations is the lack of structured narrative control in
existing LLMs. Although models like GPT and LLaMA demonstrate strong surface-level
fluency, they are inherently autoregressive and lack high-level narrative planning mechanisms~\cite{bommasani2021opportunities}.
This leads to inconsistencies in quest structure—such as missing subgoals,
conflicting motivations, or incoherent resolution paths. Unlike manually authored quests,
which follow well-defined narrative arcs (e.g., setup\textrightarrow complication\textrightarrow resolution), LLM outputs
often lose track of long-range dependencies across quest stages. Some hybrid systems
have introduced graph-based planning or symbolic scaffolds (e.g., plot graphs, quest
trees), but few have been effectively integrated with LLMs in a seamless and generalizable
way~\cite{ammanabrolu2019toward,ashby2023personalized}.

A second challenge involves the limited availability of domain-specific fine-tuning
resources. While general-purpose language models are trained on massive, heterogeneous
corpora, they lack detailed exposure to the linguistic patterns, structural tropes, and character
archetypes specific to RPGs. The adopted hypothesis from related works that the
dataset used to pretrain GPT-2 does not have enough number of quest examples to generate
high-quality quest results without further fine-tuning with specialized datasets~\cite{vartinen2022generating,van2021fine}.
Fine-tuning on curated corpora from various games can help models learn quest-specific
syntax and semantic flow. However, such datasets are rarely available in clean,
annotated formats that are aligned with narrative objectives or quest semantics. The
absence of open-source, standardized RPG quest datasets remains a major bottleneck for
reproducible research.

Even after fine-tuning, certain LLMs often exhibit semantic inconsistencies and logical
drift~\cite{luo2023empirical,huang2025survey}. These issues commonly manifest as contradictions between non-player
character (NPC) dialogue and quest objectives, incoherent character behaviors, or violations
of the game world's internal logic. Such breakdowns largely arise from the absence
of explicit state tracking or grounding mechanisms in these models. Since LLMs lack an
inherent understanding of world state, quest dependencies, and previously generated content,
they are prone to disrupting narrative causality and coherence. Although integrating
LLMs with structured memory systems, finite state machines, or symbolic representations
of world models holds promise, such hybrid approaches remain relatively underexplored
within the domain of PCG, particularly for narrative tasks.

Another significant research gap exists in the evaluation of procedurally generated
narratives. Unlike spatial or mechanical content—such as levels or terrains—that can be
assessed using objective metrics (e.g., pathfinding solvability or object density), narrative
content is inherently subjective and difficult to quantify. Current approaches often rely
on human evaluations, expert annotations, or engagement proxies, which are costly, non-scalable,
and lack standardization. Conventional text generation metrics like BLEU~\cite{papineni02bleu},
and METEOR~\cite{banarjee2005} offer limited utility in this domain, as they primarily measure lexical
similarity rather than deeper narrative attributes such as coherence, causality, or character
consistency~\cite{celikyilmaz2020evaluation}.

Recent advances in natural language generation propose more nuanced evaluation
methods, including structure-aware metrics, discourse modeling, and embedding-based
similarity measures such as BERTScore~\cite{zhang2019bertscore}. However, these techniques remain underutilized
in the context of interactive storytelling and quest generation. Key narrative
dimensions—such as world-state alignment, progression logic, and character intent—are
often ignored by current metrics. This highlights the pressing need for domain-specific
evaluation frameworks that can robustly assess both the semantic clarity and narrative
integrity of procedurally generated quests.

The integration of quest generation systems into real-time, interactive game environments
presents a range of practical and technical challenges. Most LLM-based quest
generation experiments are conducted offline or in isolated testing environments, without
the constraints of live player interaction. For games with dynamic states, generated quests
must work in tandem with the ever-evolving game state that encompasses various factors
such as available inventory, completed objectives, and player positioning. This signifies
the need for a bidirectional interface between the generative model and the underlying
game engine. The development of such tightly coupled systems poses ongoing challenges
in both computational design and real-time narrative integration.

The issue of scalability and performance is equally important. Large-scale models
are computationally expensive and often unsuitable for real-time generation on consumer
hardware. While techniques like model quantization and PEFT (e.g., LoRA adapters)
offer promising trade-offs between performance and quality~\cite{hu2022lora}, their use in long-form,
context-sensitive narrative generation has not been extensively benchmarked. The question
regarding the fidelity and controllability of such compressed models in creative applications
remains to be answered.

Finally, limited research has been conducted on personalization and adaptive quest
generation. Existing systems typically produce static quest content that fails to account
for a player's prior actions, decisions, or preferences. Developing adaptive systems capable
of tailoring quests to individual player profiles holds potential for enhancing engagement
and narrative richness. However, building such a system would require robust user modeling
and dynamic content adaptation mechanisms.

\section{Proposed Solution}

\begin{table}[t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.3}
  \begin{tabularx}{0.95\textwidth}{
    >{\raggedright\arraybackslash}p{5cm}
    >{\raggedright\arraybackslash}X
  }
    \toprule
    \textbf{Scope Aspect} & \textbf{Description} \\
    \midrule
    Dataset Construction
      & Focused on structuring quest and dialogue data extracted from a select set of classic RPGs using prompt-aligned, XML-like annotations. \\
    Modeling Approach
      & Applies quantized causal LLMs with parameter-efficient fine-tuning (LoRA) to enable low-resource adaptation to the domain. \\
    Task Objective
      & Targets single-instance procedural quest generation with coherent tasks and minimal dependencies across sessions. \\
    Evaluation Strategy
      & Combines automated metrics (BLEU, METEOR, BERTScore, perplexity) with human-assessed attributes (goal alignment, coherence, diversity). \\
    Application Domain
      & Tailored toward fantasy-themed role-playing games with linear or semi-branching quest structures. \\
    \bottomrule
  \end{tabularx}
  \caption{Scope of the PQG system, including key features and constraints}
  \label{table:scope}
\end{table}

\begin{table}[t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.3}
  \begin{tabularx}{0.95\textwidth}{
    >{\raggedright\arraybackslash}p{5cm}
    >{\raggedright\arraybackslash}X
  }
    \toprule
    \textbf{Limitation} & \textbf{Description} \\
    \midrule
    Dataset Scope
      & The dataset is restricted to a small number of RPGs, i.e., 8, which may limit genre diversity and model generalizability. \\
    Model Capacity
      & Fine-tuning is performed on quantized LLMs, potentially reducing expressiveness compared to full-precision, high-parameter models. \\
    Dialog Context Management
      & The system does not track persistent dialogue state, leading to isolated quest generations without long-term context awareness. \\
    Narrative Depth Evaluation
      & Evaluation focuses on surface-level lexical similarity and short-range coherence, omitting in-depth narrative or plot arc consistency. \\
    Genre Generalization
      & The model and prompt format are optimized for fantasy RPGs and require more adaptation for sci-fi, modern, or non-narrative games. \\
    \bottomrule
  \end{tabularx}
  \caption{Summary of key limitations observed in the proposed PQG approach}
  \label{table:limitations}
\end{table}

This study proposes a modular and resource-conscious system for PQG using quantized
LLMs augmented with PEFT techniques. The solution is designed to function within
the constraints of limited computational resources while ensuring semantic coherence and
domain relevance in the generated quest descriptions.

At its core, the system employs causal LLMs (e.g., GPT- or LLaMA-based architectures)
that have been quantized to 4-bit or 8-bit precision to reduce memory overhead and
enable deployment on modest hardware. These models are further adapted to the task domain
using LoRA-based fine-tuning, which allows for task-specific training with minimal
updates to the model's core parameters. This dual strategy of quantization and PEFT
balances the trade-off between efficiency and performance, particularly in low-resource
environments.

To ensure alignment with RPG-specific narrative structures, the input data is curated
from a domain-relevant corpus of classic RPGs. The dataset is preprocessed into an XML-like
schema to reinforce consistent structure during generation and facilitate prompt-based
conditioning. This schema allows the model to learn not only the linguistic form
of quest descriptions but also their functional components, such as goals, rewards, and
preconditions.

The quest generation task is framed as a single-instance process, wherein each quest
is produced in isolation, i.e., without maintaining dialogue history or tracking long-term
narrative dependencies. This design choice streamlines the modeling requirements while
offering a stable and reproducible setting for model-to-model comparison.

The evaluation framework adopts a hybrid approach, combining automatic text generation
metrics with human judgment. Quantitative assessment relies on widely used NLG
metrics, including BLEU, METEOR, BERTScore, and model perplexity, which collectively
evaluate lexical overlap, semantic fidelity, and fluency. These are complemented by
qualitative human evaluations focused on narrative coherence, goal relevance, and stylistic
diversity of the generated quests.

A summary of the methodological components is provided in Table~\ref{table:scope}, while Table~\ref{table:limitations}
outlines the known constraints of the implementation. Together, these illustrate both the
practicality and the boundaries of the proposed system.

Ultimately, this solution offers a replicable and scalable framework for evaluating the
narrative capabilities of LLMs in a focused gaming context. By combining lightweight
adaptation techniques with structured data and robust evaluation, the system contributes
a baseline for future research in quest generation and narrative modeling.

\section{Problem Statement}

The design and delivery of compelling quest content in RPGs pose both narrative and
technical challenges. Manually authored quests, while effective in delivering structured
and immersive storytelling, are inherently limited by scalability, production overhead,
and a lack of variability across playthroughs. PCG offers an avenue for addressing these
limitations, however, current systems often struggle to produce quests that are simultaneously
narratively coherent, mechanically relevant, and context-aware. LLMs, including
GPT and LLaMA, have demonstrated impressive capabilities in general NLG. In particular,
the relationship between model size, computational efficiency, and narrative quality
in real-time game environments remains underexplored, presenting a major obstacle to
the wider adoption of LLM-based systems in game development pipelines.

Thus, this study aims to bridge that gap by systematically evaluating the effectiveness
of LLMs in generating semantically rich and narratively coherent quest descriptions.
Unlike prior research that often relies on anecdotal evidence or game-specific testing, this
work adopts a more rigorous evaluation framework using established NLG metrics, such
as BLEU, METEOR, BERTScore, and perplexity, to assess multiple dimensions of language
quality. Additionally, this research addresses the practical challenge of deploying
LLMs in hardware-constrained environments common in game development. It investigates
the use of PEFT methods like LoRA and model quantization to reduce memory
usage and computational demands without significantly compromising output quality.
By integrating these optimization techniques into a low-resource generation pipeline, this
study contributes both methodological insights and practical tools for scalable, real-time
PQG using LLMs.

\section{Objectives \& Contributions}

The overarching goal of this study is to investigate the viability of LLMs for PQG in RPGs, particularly under constraints of data availability and computational resources. To this end, the research defines five key objectives, each targeting a specific challenge in the domain of narrative generation using LLMs:

\begin{enumerate}
  \item {Curate a structured dataset from classic RPGs, annotated with quest tasks, character
        roles, and narrative dependencies. This supports prompt-based LLM training
        by exposing models to rich, domain-specific narrative patterns.}

  \item {Analyze the performance, memory efficiency, and narrative generation quality of
        quantized LLMs to assess their suitability for real-time, low-resource game environments.}

  \item {Implement PEFT using LoRA adapters to adapt base models to the RPG domain
        while minimizing training overhead and hardware requirements.}

  \item {Design a comprehensive evaluation strategy combining automatic metrics (BLEU,
        METEOR, BERTScore, perplexity) with human-centric qualitative assessments to
        capture both surface-level and narrative-level coherence.}

  \item {Construct and benchmark an end-to-end PQG pipeline integrating optimized and
        tagged datasets, quantized models, and LoRA-based fine-tuning to demonstrate
        high-quality quest generation in constrained environments.}
\end{enumerate}

Collectively, the successful fulfillment of the research objectives results in several novel
contributions to PQG using LLMs. These include the creation of a prompt-aligned RPG
quest dataset tailored for narrative learning, an empirical evaluation of quantized LLMs,
and the application of LoRA-based PEFT to enhance model adaptability with minimal
computational overhead. Additionally, a hybrid evaluation framework combining lexical
and narrative-level metrics is introduced to enable more nuanced assessments of generation
quality. Finally, the study presents a low-resource pipeline capable of generating
semantically valid and narratively coherent quests, demonstrating the feasibility of integrating
LLM-based systems into real-time, resource-constrained game environments.

\section{Rest of the Report}

The remainder of this report provides a structured walkthrough of the technical background,
methodology, evaluation, and outcomes of the study. It introduces the core
concepts behind large language models and their adaptation for narrative generation, followed
by the system's design and training workflow. The results are then analyzed using
a combination of quantitative and qualitative metrics. The structure of the report is as
follows:

\begin{itemize}
  \item {"Chapter 2: Technical Background" provides the theoretical foundation for the
        study. It introduces LLMs used in this work (GPT-2, LLaMA 3.2, TinyLLaMA),
        and explains the principles of PEFT (using LoRA), model quantization techniques,
        transformer-based text generation, and the design logic behind PQG in RPGs.}
  \item {"Chapter 3: Related Work" surveys existing literature in PCG, quest generation
        using NLP techniques, and evaluation strategies for generated narrative content.}
  \item {"Chapter 4: Methodology" details the end-to-end workflow for procedural PQG,
        covering dataset construction, model setup, quantization, LoRA integration, and
        the evaluation framework. It describes how the system was designed to balance
        efficiency with generation quality in constrained settings.}
  \item {"Chapter 5: Results and Discussion" presents the empirical findings from model
        training and evaluation. It includes quantitative results (e.g., BLEU, METEOR,
        BERTScore, perplexity), qualitative insights, and a comparative analysis with previous
        studies.}
  \item {"Chapter 6: Conclusion and Future Work" summarizes the contributions of the
        thesis, outlines limitations, and suggests directions for future research in scalable,
        adaptive quest generation.}
  \item {Appendices A and B provide additional details about the quest dataset format and
        experimental setup, including hardware, software, and hyperparameter configurations.}
\end{itemize}

This structure provides a clean and logical progression from foundational knowledge
to applied experimentation techniques. Thus, it yields insights that inform both academic
research and practical implementations in game development workflows.

\newpage
