\clearpage

\chapter{Related Work}

In this chapter, we examine foundational and recent advances in PCG within digital
games, with a specific focus on quest generation. This includes traditional rule-based
and planning approaches, search-based PCG, and the integration of deep learning and
LLMs. We draw from key contributions by Togelius et al.~\cite{togelius2011search} and Shaker et al.~\cite{shaker2016procedural},
who formalized PCG taxonomies and techniques, as well as Liu et al.~\cite{liu2021deep}, who surveyed
the deep learning landscape for PCG. We also explore works by V{\"a}rtinen et al.~\cite{vartinen2022generating} and
Ammanabrolu et al.~\cite{ammanabrolu2019toward}, which apply LLMs to generate quest narratives.

Historical systems such as Player-Specific Stories via Automatically Generated Events
(PaSSAGE)~\cite{thue2007interactive} and Thespian~\cite{si2005thespian} focused on emergent, personalized storytelling through
player modeling and modifiable agents. Meanwhile, approaches like REwriting Graphs
for Enhanced Narratives (ReGEN)~\cite{kerr2009supporting} and Creation of Novel Adventure Narratives (CONAN)~\cite{breault2021let}
applied graph rewriting and grammars to produce narrative coherence in quest
design. However, many of these techniques suffer from issues of repetitiveness and low
generalizability.

Modern research increasingly leverages transformer-based LLMs, offering improved
diversity and coherence. For instance, van Stegeren and My{\'s}liwiec~\cite{van2021fine} demonstrated
GPT-2's capabilities on a World of Warcraft~\cite{worldofwarcraft} dataset, while V{\"a}rtinen et al.~\cite{vartinen2022generating} proposed
a broader dataset spanning six RPGs. Their findings suggest that LLM-generated
quests can sometimes surpass human-written content in perceived quality, though sample
sizes and model inputs often limit generalizability.

\section{Procedural Content Generation in Games}

PCG refers to the algorithmic creation of game content with limited or indirect user
input. It has seen widespread adoption in game development for generating levels, items,
characters, quests, and more. The primary motivation behind PCG is to reduce the
manual effort of content creation while enabling rich, dynamic, and replayable experiences.

Search-based PCG was introduced prominently by Togelius et al.~\cite{togelius2011search}. This approach
treats content generation as an optimization problem, using algorithms like evolutionary
strategies, genetic algorithms, and Monte Carlo tree search (MCTS) to search a space of
potential content. The fitness function in such cases evaluates how “playable” or “fun”
a piece of content is. Rule-based and agent-based methods offer deterministic content
generation but often lack variety. For instance, cellular automata and wave function
collapse are used for map generation and tiling~\cite{shaker2016procedural}.

Deep learning-based PCG techniques have expanded the scope of generative models
by enabling learning from existing content distributions. CNNs (convolutional neural
networks) and LSTMs (long short-term memory) have been used for visual and sequential
content respectively~\cite{liu2021deep}. Guzdial et al.\cite{guzdial2017visual} used CNNs for visual level blending, while
Liang et al.\cite{liang2019procedural} used LSTMs for beatmap generation. GANs (generative adversarial
networks) and autoencoders have enabled unsupervised learning and blending of complex
structures. Sarkar et al.~\cite{sarkar2020controllable} demonstrated controllable level blending using autoencoders.

\section{NLP Techniques for Quest Generation}

\begin{table}[t]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.3}
  \begin{tabularx}{0.95\textwidth}{
    >{\raggedright\arraybackslash}p{3.5cm}
    >{\raggedright\arraybackslash}X
    >{\raggedright\arraybackslash}X
    >{\raggedright\arraybackslash}X
    >{\centering\arraybackslash}p{1cm}
  }
    \toprule
    \textbf{Author(s)} & \textbf{Dataset(s)} & \textbf{Model(s)} & \textbf{Evaluation(s)} & \textbf{Year} \\
    \midrule
    Ammanabrolu et al.~\cite{ammanabrolu2019toward} & TextWorld custom environment + generated knowledge graphs & Hybrid: Knowledge Graph + Markov + LSTM RNN & Human evaluation, success rate in quest completion & 2019 \\
    V{\"a}rtinen et al.~\cite{vartinen2022generating} & Quest dataset from 6 major commercial RPGs (e.g., Skyrim) & Fine-tuned GPT-2 (Quest-GPT-2) & BLEU, METEOR, human preference ratings & 2022 \\
    Ashby et al.~\cite{ashby2023personalized} & Custom RPG dataset, dialogue and quest templates & Knowledge Graph + GPT-3.5 (personalization) & BLEU, ROUGE, user preference for personalized content & 2023 \\
    Current LLM-based works (instruction-based) & V{\"a}rtinen's dataset, custom templates, or synthetic quest data & GPT-3.5, GPT-4, Llama-2, Llama-3 (finetuned) & ROUGE, BLEU, quest structure validity, human alignment tests & 2024-25 \\
    \bottomrule
  \end{tabularx}
  \caption{Summary of works focused on structured quest generation techniques}
  \label{table:key-works}
\end{table}

NLP has become integral to procedural content generation in games, especially for generating
narrative structures such as quests. Among various approaches, structured quest
generation using LLMs and hybrid systems has emerged as a dominant paradigm. A
notable early contribution by Ammanabrolu et al.~\cite{ammanabrolu2019toward} proposed a knowledge graph-based
architecture for generating quests in text-adventure games. Their framework effectively
combined symbolic planning, rule-based logic, Markov chains, and neural models to create
coherent, goal-oriented storylines. This hybrid approach demonstrated the potential of
integrating structured world representations with data-driven generation techniques.

Later, V{\"a}rtinen et al.~\cite{vartinen2022generating} introduced a comprehensive quest dataset collected from
six major commercial RPGs. They used this dataset to train a GPT-2 model—referred
to as Quest-GPT-2—for generating quests that reflected patterns and structures found in
professionally designed game content. This dataset has since served as a benchmark for
evaluating model performance in structured quest generation tasks.

Building on these foundations, recent work has shifted towards using fine-tuned or
instruction-tuned LLMs that accept partially structured inputs—such as templates, quest
metadata, or dialogue context—to generate diverse and coherent quest content. These
models are designed to align better with human-authored quest structures, enabling the
generation of narrative elements that maintain logical progression and thematic consistency.

Table~\ref{table:key-works} presents a comparison of representative systems for structured quest generation.
It outlines their dataset sources, modeling techniques, and evaluation strategies,
highlighting both the evolution of methodologies and the diversity of benchmarks used
across studies.

As evident from the table, earlier models relied on handcrafted environments or symbolic
planners, while recent approaches increasingly leverage large-scale pre-trained language
models. However, consistent evaluation remains a challenge, with most works
relying on task-specific metrics like BLEU~\cite{papineni02bleu} and ROUGE~\cite{lin2004looking}, which may not fully
capture narrative quality or coherence.

\section{Human Evaluation of Generated Content}

In addition to automated metrics, human evaluation plays a crucial role in assessing
the quality and effectiveness of generated quests. This method focuses on subjective
aspects that are often difficult to quantify, such as coherence, creativity, and how well the
generated content aligns with the game world's narrative and logic. Human evaluations
are commonly employed to capture the user experience and assess whether the generated
quests feel meaningful and engaging within the context of the game.

For instance, Ammanabrolu et al.~\cite{ammanabrolu2019toward} used human evaluators to rate the coherence
and creativity of quests generated by their hybrid knowledge graph (KG) and neural
model approach. They focused on understanding how well the generated quests fit into
the context of text-based adventure games and whether they maintained narrative flow.
Similarly, van Stegeren et al.~\cite{van2021fine} employed participant ratings to evaluate quest quality
in their study on automated quest generation for RPGs. These evaluations often include
ratings on creativity, narrative alignment, and the overall immersion that the generated
quests provide.

Värtinen et al.~\cite{vartinen2022generating} also conducted a survey-based evaluation as part of their Quest-GPT-2
study. Participants were asked to compare generated quests with human-authored
counterparts based on multiple dimensions, including structure, creativity, and plausibility
within a game world. The results showed a preference for human-authored quests but
highlighted the potential of fine-tuned language models to approximate professional design
standards when guided by structured data.

Recent research in human-computer interaction (HCI) also emphasizes the importance
of player-centered design when evaluating procedurally generated narratives. For example,
Smith et al.~\cite{smith2011pcg} argue that evaluation frameworks for PCG must consider interactivity,
engagement, and player agency as factors that are often overlooked in standard NLP
evaluations. HCI perspectives advocate for iterative testing with diverse user groups
and the integration of usability metrics to ensure that generated quests support player
satisfaction and narrative immersion across different play styles and preferences.

Despite the importance of human evaluations, there are certain limitations to this
approach. One common issue is the small sample size of participants, which can lead to
variability in results and limit the robustness of the conclusions. Additionally, the specific
context or domain of the quests evaluated may not always generalize well to other types
of games or genres. For example, a system trained and evaluated on quests for a medieval
fantasy RPG may not perform as well when applied to a sci-fi or historical setting.
Furthermore, the subjective nature of human evaluations can introduce biases based on
individual preferences, leading to inconsistency in ratings across different evaluators.

While human evaluations remain a vital tool in quest generation research, these limitations
underscore the need for a combination of quantitative metrics, HCI-driven evaluation
frameworks, and subjective assessments to better understand and improve the quality of
generated content.

\newpage
